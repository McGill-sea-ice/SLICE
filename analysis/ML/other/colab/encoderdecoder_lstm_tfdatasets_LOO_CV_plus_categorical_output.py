# -*- coding: utf-8 -*-
"""EncoderDecoder_LSTM_TFDatasets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Dc3S0Zp_3ZazEEGBBPmOuVy48HqPPsqp
"""
#%%
local_path = '/storage/amelie/'
# local_path = '/Volumes/SeagateUSB/McGill/Postdoc/'

use_GPU = True

#%%
import tensorflow as tf

print("Num CPUs Available: ", len(tf.config.list_physical_devices('CPU')))
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

if use_GPU:
    if len(tf.config.list_physical_devices('GPU')) > 0:
        device_name = '/GPU:0'
    else:
        print('No GPU available... Defaulting to using CPU.')
        device_name = '/CPU:0'
else:
    device_name = '/CPU:0'

#%%
import sys
import os
FCT_DIR = os.path.dirname(os.path.abspath(local_path +'slice/prog/'+'/prog/'))
if not FCT_DIR in sys.path:
    sys.path.append(FCT_DIR)

import tensorflow.keras.backend as K
from tensorflow import keras
from tensorflow.keras.utils import plot_model
from keras.layers import Lambda
import sklearn.metrics as metrics

import numpy as np
import pandas as pd
import datetime as dt

from matplotlib import pyplot as plt
from matplotlib import dates as mdates
import cmocean

from functions import rolling_climo
from functions_ML import regression_metrics, plot_sample, plot_prediction_timeseries

#%%
# To see on which device the operation are done, uncomment below:
# tf.debugging.set_log_device_placement(True)

# Set random seed:
# tf.keras.utils.set_random_seed(422)

#%%
def fit_scaler(df_train_in,norm_type='MinMax'):
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.preprocessing import StandardScaler

    if norm_type =='MinMax': scaler = MinMaxScaler()
    if norm_type =='Standard': scaler = StandardScaler()

    return scaler.fit(df_train_in)




def normalize_df(df_in,scaler):
    df = df_in.copy()
    scaled_values = scaler.transform(df.values)

    for ip, p in enumerate(df.columns):
        df.iloc[:,ip] = scaled_values[:,ip]

    return df




def get_predictor_clim(df_tr,df_va,df_te,
                       ind_tr,ind_va,ind_te,
                       time_tr,tr_years,time_in,
                       nw=1,
                       verbose = True):

    df_tr_clim = df_tr.copy()
    df_va_clim = df_va.copy()
    df_te_clim = df_te.copy()

    for ip, p in enumerate(df_tr.columns):
        if verbose: print(p, df_tr.iloc[:,ip].name)
        p_clim_mean, p_clim_std, _ = rolling_climo(nw, df_tr.iloc[:,ip].values,'other',time_tr,tr_years,time_other=time_in)
        df_tr_clim[p] = p_clim_mean[ind_tr]
        df_va_clim[p] = p_clim_mean[ind_va]
        df_te_clim[p] = p_clim_mean[ind_te]

    return df_tr_clim, df_va_clim, df_te_clim




def replace_nan_with_clim(df_tr,df_va,df_te,
                          df_tr_clim,df_va_clim,df_te_clim,
                          verbose = True):

    for ip, p in enumerate(df_tr.columns):
        df_tr.iloc[np.where(np.isnan(df_tr.iloc[:,ip]))[0],ip] = df_tr_clim.iloc[np.where(np.isnan(df_tr.iloc[:,ip]))[0],ip]
        df_va.iloc[np.where(np.isnan(df_va.iloc[:,ip]))[0],ip] = df_va_clim.iloc[np.where(np.isnan(df_va.iloc[:,ip]))[0],ip]
        df_te.iloc[np.where(np.isnan(df_te.iloc[:,ip]))[0],ip] = df_te_clim.iloc[np.where(np.isnan(df_te.iloc[:,ip]))[0],ip]
        # If there are still NaNs in the climatology because of missing data, we will just put a zero.
        # !!! THIS IS A QUICK FIX AND COULD BE IMPROVED !!! But it only happens for certain climate indices.
        # !!! CHECK THIS FOR MASKING THE NANS DURING MODEL TRAINING/TESTING: https://keras.io/api/layers/core_layers/masking/
        df_tr.iloc[np.where(np.isnan(df_tr.iloc[:,ip]))[0],ip] = 0
        df_va.iloc[np.where(np.isnan(df_va.iloc[:,ip]))[0],ip] = 0
        df_te.iloc[np.where(np.isnan(df_te.iloc[:,ip]))[0],ip] = 0

    if verbose:
        # Check if there are remaining nan values.
        print('Nans in train set?' , np.any(np.sum(np.isnan(df_tr[df_tr.columns[:]])) > 0 ))
        print('Nans in valid set?' ,np.any(np.sum(np.isnan(df_va[df_va.columns[:]])) > 0 ))
        print('Nans in test set?' ,np.any(np.sum(np.isnan(df_te[df_te.columns[:]])) > 0 ))

    return df_tr,df_va,df_te




def create_dataset(df, df_clim, time_in,
                   n_forecasts,
                   window_size, forecast_size,
                   batch_size,
                   shuffle=False):
    """
    SEE WEB EXAMPLE:
    (https://www.angioi.com/time-series-encoder-decoder-tensorflow/)
    """

    # Total size of window is given by the number of steps to be considered
    # before prediction time + steps that we want to forecast
    total_size = window_size + forecast_size

    # Selecting windows
    data = tf.data.Dataset.from_tensor_slices(df.values)
    data = data.window(total_size, shift=1, drop_remainder=True)
    data = data.flat_map(lambda k: k.batch(total_size))

    data_clim = tf.data.Dataset.from_tensor_slices(df_clim.values)
    data_clim = data_clim.window(total_size, shift=1, drop_remainder=True)
    data_clim = data_clim.flat_map(lambda k: k.batch(total_size))

    time = tf.data.Dataset.from_tensor_slices(time_in)
    time = time.window(total_size, shift=1, drop_remainder=True)
    time = time.flat_map(lambda k: k.batch(total_size))

    # Zip all datasets together so that we can filter out the samples
    # that are discontinuous in time due to cross-validation splits.
    all_ds = tf.data.Dataset.zip((data, data_clim, time))
    all_ds_filtered =  all_ds.filter(lambda d,dc,t: tf.math.equal(t[-1]-t[0]+1,total_size))

    # Then extract the separate data sets
    data_filtered = all_ds_filtered.map(lambda d,dc,t: d)
    data_clim_filtered =  all_ds_filtered.map(lambda d,dc,t: dc)
    time_filtered =  all_ds_filtered.map(lambda d,dc,t: t)

    # Shuffling data
    # !!!!! NOT SURE HOW TO DEAL WITH SHUFFLE AND RECONSTRUCT THE SHUFFLED TIME SERIES...
    # so we keep shuffle to False for now...
    shuffle = False
    if shuffle:
        shuffle_buffer_size = len(data_filtered) # This number can be changed
    #     data = data.shuffle(shuffle_buffer_size, seed=42)
        data_filtered = data_filtered.shuffle(shuffle_buffer_size, seed=42)
        data_clim_filtered =  data_clim_filtered.shuffle(shuffle_buffer_size, seed=42)
        time_filtered =  time_filtered.shuffle(shuffle_buffer_size, seed=42)

    # Extracting (past features, forecasts, decoder initial recurrent input) + targets
    # NOTE : the initial decoder input is set as the last value of the target.
    if n_forecasts > 0:
        data_filtered = data_filtered.map(lambda k: ((k[:-forecast_size,1:-n_forecasts], # Past predictors samples
                                                      k[-forecast_size:, -n_forecasts:], # Future forecasts samples
                                                      k[-forecast_size-1:-forecast_size,0:1] # Decoder input: last time step of target before prediction time starts
                                                     ),
                                                     (k[-forecast_size:, 0:1],# Target samples during prediction time
                                                      k[-forecast_size:-forecast_size+1, -n_forecasts-1:-n_forecasts] # FU occurence during prediction time
                                                     )
                                                    )) 

        data_clim_filtered = data_clim_filtered.map(lambda k: (k[:-forecast_size,1:-n_forecasts], # Past predictor climatology samples
                                                               k[-forecast_size:,0:1])) # Target climatology samples during prediction time



    else:
        data_filtered = data_filtered.map(lambda k: ((k[:-forecast_size,1:],  # Past predictors samples
                                                      k[-forecast_size-1:-forecast_size,0:1] # Decoder input: last time step of target before prediction time starts
                                                     ),
                                                     (k[-forecast_size:, 0:1],# Target samples during prediction time
                                                      k[-forecast_size:-forecast_size+1, -n_forecasts-1] # FU occurence during prediction time
                                                     )
                                                     )) 

        data_clim_filtered = data_clim_filtered.map(lambda k: (k[:-forecast_size,1:], # Past predictor climatology samples
                                                               k[-forecast_size:,0:1])) # Target climatology samples during prediction time

    time_filtered = time_filtered.map(lambda k: (k[:-forecast_size], # Time for past predictors samples
                                                 k[-forecast_size:]))    # Time for prediction samples



    return data_filtered.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE), data_clim_filtered.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE), time_filtered.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)




def reconstruct_ysamples(model,ds_w,ds_clim_w,time_w,scaler,sample_len,n_frcst_vars,is_anomaly):
    # Get targets and predictions for all samples. These are the scaled
    # values of the samples that we got after the scaler.
    y_scaled = np.concatenate([y[0] for x, y in ds_w], axis=0)
    FU_scaled = np.concatenate([y[1] for x, y in ds_w], axis=0)
    y_pred_scaled,FU_pred_scaled = model.predict(ds_w)
    time_y = np.concatenate([t_tar for t_pred, t_tar in time_w], axis=0)

    # Get past predictors and forecasts as well because
    # they will be needed to reverse the scaler.
    X = np.concatenate([x[0] for x, y in ds_w], axis=0)
    if n_frcst_vars > 0:
        F = np.concatenate([x[1] for x, y in ds_w], axis=0)

    # All data must be retransformed back using the scaler.
    # The format of the data that was passed to the scaler was
    #     (n_timesteps, 1 (target)+ n_predictors columns+ n_forecast_vars):

    # We will reconstruct the target samples as:
    #    (nsamples, pred_len, 1)
    y_pred = np.zeros(y_pred_scaled.shape)
    y = np.zeros(y_scaled.shape)

    for i in range(sample_len):
        if n_frcst_vars > 0:
            y_pred[:,i,0] = scaler.inverse_transform(np.concatenate((y_pred_scaled[:,i,:], X[:,i,:], F[:,i,:]), axis=1))[:,0] # Here, 0 selects the target column.
            y[:,i,0] = scaler.inverse_transform(np.concatenate((y_scaled[:,i,:], X[:,i,:], F[:,i,:]), axis=1))[:,0]
        else:
            y_pred[:,i,0] = scaler.inverse_transform(np.concatenate((y_pred_scaled[:,i,:], X[:,i,:]), axis=1))[:,0] # Here, 0 selects the target column.
            y[:,i,0] = scaler.inverse_transform(np.concatenate((y_scaled[:,i,:], X[:,i,:]), axis=1))[:,0]

    # The climatolgy was not scaled with the scaler, so we can
    # directly use the values
    y_clim = np.concatenate([tar_clim for pred_clim, tar_clim in ds_clim_w], axis=0)

    if is_anomaly:
        # Add the climatology back to the anomaly samples:
        y += y_clim
        y_pred += y_clim

    return y, y_pred, y_clim, time_y



def encoder_decoder_recursive(input_len,pred_len,npredictors,nfuturevars,latent_dim,inp_dropout,rec_dropout,dense_act_func):

    # ENCODER:
    past_inputs = keras.layers.Input(shape=(input_len, npredictors), name='past_inputs')
    encoder = keras.layers.LSTM(latent_dim, return_state=True,
                                dropout = inp_dropout,
                                recurrent_dropout = rec_dropout, name='encoder')
    encoder_outputs, encoder_state_h, encoder_state_c = encoder(past_inputs)
    # Discard encoder outputs and only keep the cell states and hidden states.
    encoder_states = [encoder_state_h, encoder_state_c]


    # DECODER: Process only one step at a time, reinjecting the output at step t as input to step t+1
    decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True,
                                     return_state=True,
                                     dropout = inp_dropout,
                                     recurrent_dropout = rec_dropout,
                                     input_shape=[None,1,nfuturevars+1], name='recursive_decoder')

    # The output of the dense layer is fixed at one to return only the predicted water temperature.
    # The sigmoid activation function ensures that the predicted values are positive and scaled between 0 and 1.
    decoder_dense = keras.layers.Dense(1,activation=dense_act_func,name='Dense')

    # Set the initial state of the decoder to be the ouput state of the encoder
    states = encoder_states

    # Initalize the recursive and forecast inputs
    first_recursive_input = keras.layers.Input(shape=(1,1), name='first_recursive_input')

    if nfuturevars > 0 :
        it = -1
        future_inputs = keras.layers.Input(shape=(pred_len, nfuturevars), name='future_inputs')
        first_future_input = tf.keras.layers.Cropping1D(cropping=(0,pred_len-1), name='future_input_'+str(it+1))(future_inputs)
        inputs = tf.keras.layers.Concatenate(name='concat_input_'+str(it+1))([first_future_input, first_recursive_input])
    else:
        inputs = first_recursive_input

    all_outputs = []

    for it in range(pred_len):
        # Run the decoder on one timestep
        outputs, state_h, state_c = decoder_lstm(inputs,initial_state=states)
        outputs = decoder_dense(outputs)

        # Store the current prediction (we will concatenate all predictions later)
        all_outputs.append(outputs)

        # Reinject the outputs as inputs for the next loop iteration and concatenate
        # with the forecast for the next time step
        # + update the states
        if nfuturevars > 0 :
            if it < pred_len - 1:
              inputs = tf.keras.layers.Concatenate(name='concat_input_'+str(it+1))([tf.keras.layers.Cropping1D(cropping=(it+1,pred_len-(it+2)),name='future_input_'+str(it+1))(future_inputs), outputs])
        else:
            inputs = outputs

        states = [state_h, state_c]

    # Concatenate all predictions
    decoder_outputs = Lambda(lambda x: K.concatenate(x, axis=1), name='concat_outputs')(all_outputs)
    
    decoder_outputs = tf.reshape(decoder_outputs,[tf.shape(decoder_outputs)[0],pred_len])
    densetw = keras.layers.Dense(pred_len,activation=dense_act_func,input_shape=[None,pred_len],name='Dense_Tw')
    Tw_outputs = tf.expand_dims(densetw(decoder_outputs),axis=2,name='Tw_out')

    densefu = keras.layers.Dense(1,activation='sigmoid',input_shape=[None,pred_len],name='Dense_FU')
    FU_output = tf.expand_dims(densefu(decoder_outputs),axis=2,name='FU_out')

    # DEFINE MODEL:
    # if nfuturevars > 0 :
    #     model = tf.keras.models.Model(inputs=[past_inputs, future_inputs, first_recursive_input], outputs=[decoder_outputs,FU_output])
    # else:
    #     model = tf.keras.models.Model(inputs=[past_inputs, first_recursive_input], outputs=[decoder_outputs,FU_output])
    if nfuturevars > 0 :
        model = tf.keras.models.Model(inputs=[past_inputs, future_inputs, first_recursive_input], outputs=[Tw_outputs,FU_output])
    else:
        model = tf.keras.models.Model(inputs=[past_inputs, first_recursive_input], outputs=[Tw_outputs,FU_output])

    return model






def execute_fold(df_train_in, df_valid_in, df_test_in,
                 ind_train_in, ind_valid_in, ind_test_in,
                 train_years,time,
                 time_train,time_valid,time_test,
                 time_train_plot,time_valid_plot,time_test_plot,
                 latent_dim,inp_dropout,rec_dropout,dense_act_func,
                 n_pred_vars,n_frcst_vars,
                 input_len,pred_len,
                 norm_type,anomaly_target,anomaly_past,anomaly_frcst,
                 batch_size,lr_in,loss,n_epochs,
                 plot_loss=True,plot_predictions=True,
                 plot_targets=False,show_modelgraph=False,show_weights=False):


    # GET TRAINING CLIMATOLOGIES:
    df_train_clim, df_valid_clim, df_test_clim = get_predictor_clim(df_train_in,df_valid_in,df_test_in,
                                                                    ind_train_in,ind_valid_in,ind_test_in,
                                                                    time_train,train_years,time,nw=1,verbose = True)
    # REPLACE NAN WITH CLIMATOLOGICAL VALUES:
    df_train_in, df_valid_in, df_test_in = replace_nan_with_clim(df_train_in,df_valid_in,df_test_in,
                                                        df_train_clim,df_valid_clim,df_test_clim,
                                                        verbose = True)


    # REMOVE CLIMATOLOGY TO GET ANOMALIES:
    if anomaly_target:
        df_train_in.iloc[:,0:1] = df_train_in.iloc[:,0:1]-df_train_clim.iloc[:,0:1]
        df_valid_in.iloc[:,0:1] = df_valid_in.iloc[:,0:1]-df_valid_clim.iloc[:,0:1]
        df_test_in.iloc[:,0:1] = df_test_in.iloc[:,0:1]-df_test_clim.iloc[:,0:1]

    if anomaly_past:
        if n_frcst_vars > 0:
            df_train_in.iloc[:,1:-n_frcst_vars] = df_train_in.iloc[:,1:-n_frcst_vars]-df_train_clim.iloc[:,1:-n_frcst_vars]
            df_valid_in.iloc[:,1:-n_frcst_vars] = df_valid_in.iloc[:,1:-n_frcst_vars]-df_valid_clim.iloc[:,1:-n_frcst_vars]
            df_test_in.iloc[:,1:-n_frcst_vars] = df_test_in.iloc[:,1:-n_frcst_vars]-df_test_clim.iloc[:,1:-n_frcst_vars]
        else:
            df_train_in.iloc[:,1:] = df_train_in.iloc[:,1:]-df_train_clim.iloc[:,1:]
            df_valid_in.iloc[:,1:] = df_valid_in.iloc[:,1:]-df_valid_clim.iloc[:,1:]
            df_test_in.iloc[:,1:] = df_test_in.iloc[:,1:]-df_test_clim.iloc[:,1:]

    if anomaly_frcst:
        if n_frcst_vars > 0:
            # !!!! WHEN USING REAL FORECAST, THIS IS WHERE I CAN COMPUTE THE FORECAST ANOMALY.
            # OR I SIMPLY KEEP THIS AS FALSE AND PASS IN THE FORECAST VARIABLES AS ANOMALIES ALREADY...
            df_train_in.iloc[:,-n_frcst_vars:] = df_train_in.iloc[:,-n_frcst_vars:]-df_train_clim.iloc[:,-n_frcst_vars:]
            df_valid_in.iloc[:,-n_frcst_vars:] = df_valid_in.iloc[:,-n_frcst_vars:]-df_valid_clim.iloc[:,-n_frcst_vars:]
            df_test_in.iloc[:,-n_frcst_vars:] = df_test_in.iloc[:,-n_frcst_vars:]-df_test_clim.iloc[:,-n_frcst_vars:]

    # plot_targets = True
    if plot_targets:
        plt.figure()
        plt.plot(time_train_plot,df_train_in.iloc[:,0:1], color='blue')
        plt.plot(time_valid_plot,df_valid_in.iloc[:,0:1],color='green')
        plt.plot(time_test_plot, df_test_in.iloc[:,0:1],color='red')
        if not anomaly_target:
            plt.plot(time_train_plot, df_train_clim.iloc[:,0:1], ':',color='cyan')
            plt.plot(time_valid_plot, df_valid_clim.iloc[:,0:1], ':',color='brown')
            plt.plot(time_test_plot, df_test_clim.iloc[:,0:1], ':',color='orange')


    # DATA NORMALIZATION
    # Normalize all predictors, forecasts, and targets using only the training data
    scaler = fit_scaler(df_train_in,norm_type=norm_type)
    df_train_scaled = normalize_df(df_train_in,scaler)
    df_valid_scaled = normalize_df(df_valid_in,scaler)
    df_test_scaled = normalize_df(df_test_in,scaler)

    target_scaler = fit_scaler(df_train_in.iloc[:,0:1],norm_type=norm_type)
    FU_threshold = np.ones((len(df_train_in),1))*0.75
    # FU_threshold = np.ones((len(df_train_in),1))*0.5
    scaled_FU_threshold = target_scaler.transform(FU_threshold)[0,0]
    # print(scaled_FU_threshold)


    # GET WINDOWED DATA SETS
    # Now we get training, validation, and test as tf.data.Dataset objects
    # The 'create_dataset' function returns batched datasets ('batch_size')
    # using a rolling window shifted by 1-day.
    train_windowed, train_clim_windowed, time_train_windowed = create_dataset(df_train_scaled, df_train_clim, time_train,
                                                                              n_frcst_vars,
                                                                              input_len, pred_len,
                                                                              batch_size,
                                                                              shuffle = False)

    valid_windowed, valid_clim_windowed, time_valid_windowed  = create_dataset(df_valid_scaled, df_valid_clim, time_valid,
                                                                               n_frcst_vars,
                                                                               input_len, pred_len,
                                                                               batch_size,
                                                                               shuffle = False)

    test_windowed, test_clim_windowed, time_test_windowed  = create_dataset(df_test_scaled, df_test_clim, time_test,
                                                                            n_frcst_vars,
                                                                            input_len, pred_len,
                                                                            batch_size=1,
                                                                            shuffle = False)

    # BUILD MODEL:
    model = encoder_decoder_recursive(input_len,pred_len,
                                      n_pred_vars,
                                      n_frcst_vars,
                                      latent_dim,
                                      inp_dropout,rec_dropout,
                                      dense_act_func)

    optimizer = keras.optimizers.Adam(learning_rate=lr_in)

    # DEFINE CALLBACKS FOR TRAINING:
    early_stop = tf.keras.callbacks.EarlyStopping(
                                                monitor="val_loss",
                                                # patience=5, # Set to 5 when using forecasts.
                                                # patience=8,# Set to 8 or 10 when not using forecasts.
                                                patience=28,# Set to 8 or 10 when not using forecasts.
                                                min_delta=0,
                                                verbose=1
                                            )
    lr_plateau =  tf.keras.callbacks.ReduceLROnPlateau(
                                                        monitor="val_loss",
                                                        factor=0.5,
                                                        patience=5,
                                                        verbose=1,
                                                        min_delta=0.0001,
                                                        min_lr=0.001
                                                    )
    training_callbacks = [lr_plateau,early_stop]
    # training_callbacks = []

    if loss == 'custom_loss':
        # !!!!! THIS WILL ONLY WORK IF anomaly_target = False
        # NEED TO ADD THIS AS A CONDITION FOR THE CUSTOM LOSS USE.
        def loss_wraper(scaled_FU_threshold):

            # def custom_loss(y_true,y_pred):
            #     # Put 100 times more weight on time steps where y_true is below threshold
            #     mse = tf.keras.losses.MeanSquaredError()
            #     w = (1.+100.*tf.cast(tf.reduce_any(tf.math.less_equal(y_true, scaled_FU_threshold),axis=2),'float'))
            #     return mse(y_true,y_pred,sample_weight=w)


            # def custom_loss(y_true,y_pred):
            #     # Add MSE loss for value of Tw,
            #     # and MSE loss for dTw/dt:
            #     mse = tf.keras.losses.MeanSquaredError()
            #     dy_true = y_true[:,1:,:]-y_true[:,:-1,:]
            #     dy_pred = y_pred[:,1:,:]-y_pred[:,:-1,:]
            #     w = (1.+100.*tf.cast(tf.reduce_any(tf.math.less_equal(y_true, scaled_FU_threshold),axis=2),'float'))
            #     bce = tf.keras.losses.BinaryCrossentropy()
 
            #     return mse(y_true,y_pred,w)+mse(dy_true,dy_pred,w[:,:-1])


            def custom_loss(y_true,y_pred):

                # penalize_FN = True
                penalize_FN = False

                log_loss = False
                # log_loss = True

                penalize_recall = True
                # penalize_recall = False

                # use_exp_decay_loss = True
                use_exp_decay_loss = False
                tau = 1/30.

                mse = tf.keras.losses.MeanSquaredError()

                if penalize_FN:
                    # We compute the weight for how the sample loss will
                    # contribute to the overall batch loss, to penalize cases of
                    # False Negative (FN) freeze-up events (weight times 4),
                    # False Positive (FP) freeze-up events (weight times 2),
                    # while True Positive (TP) and True Negative (TN) are
                    # weighted as usual (weight = 1)

                    # Check if Tw reaches below the freeze-up threshold in each sample.
                    # This is 1 if there is a freeze-up in the target or prediction,
                    #      or 0 if there is no freeze-up.
                    # (The freeze-up is defined as Tw < scaled_FU_threshold)
                    obs_FU_batch = tf.cast(tf.reduce_any(tf.math.less(y_true,scaled_FU_threshold),axis=1),y_true.dtype)
                    pred_FU_batch = tf.cast(tf.reduce_any(tf.math.less(y_pred,scaled_FU_threshold),axis=1),y_true.dtype)

                    # Compute the weight to assign to that sample.
                    #     # w = 1 for true positive and true negative (i.e. freeze-up or no freeze-up is detected in both the prediction and observations)
                    #     # w = 2 for false positive (i.e. freeze-up is detected in the prediction, but not in observations)
                    #     # w = 4 for false negative (i.e. no freeze-up is detected in the prediction, but there was a freeze-up in the observations)
                    w_batch = tf.fill(tf.shape(obs_FU_batch),1.0) + 2*tf.math.squared_difference(obs_FU_batch,pred_FU_batch) + tf.math.subtract(obs_FU_batch,pred_FU_batch)
                    w_batch = tf.squeeze(w_batch)

                    if use_exp_decay_loss:
                        # Forcing an exponential weight decay per time step in
                        # forecast horizon, i.e. put more weight on the error
                        # of the first forecast steps.
                        wtmp = tf.cast(tf.fill(tf.shape(tf.squeeze(y_true)),1.0),y_true.dtype)
                        exp_tmp = 1/tf.exp(tf.range(1,pred_len+1,dtype=y_true.dtype)*tau)
                        exp_w = tf.multiply(wtmp,exp_tmp)

                        w = tf.multiply(exp_w,tf.expand_dims(w_batch,axis=-1))
                    else:
                        w = w_batch

                    return mse(y_true,y_pred,sample_weight=w)

                # elif penalize_recall:

                elif log_loss:

                    return tf.reduce_sum(tf.math.log(tf.reduce_mean(tf.math.square(y_true-y_pred),axis=0)),axis=0)

                elif penalize_recall:
                    mask_threshold = tf.cast(tf.math.less_equal(y_true, scaled_FU_threshold), 'float')
                    mask_threshold_pred = tf.cast(tf.math.less_equal(y_pred, scaled_FU_threshold), 'float')

                    tp = tf.reduce_sum(tf.cast(tf.math.multiply(mask_threshold,mask_threshold_pred), 'float'), axis=1)
                    tn = tf.reduce_sum(tf.cast(tf.math.multiply(1-mask_threshold,1-mask_threshold_pred), 'float'), axis=1)
                    fp = tf.reduce_sum(tf.cast(tf.math.multiply(1-mask_threshold,mask_threshold_pred), 'float'), axis=1)
                    fn = tf.reduce_sum(tf.cast(tf.math.multiply(mask_threshold,1-mask_threshold_pred), 'float'), axis=1)

                    precision = tp / (tp + fp + K.epsilon())
                    recall = tp / (tp + fn + K.epsilon())
                    FNR = fn / (tp + fn + K.epsilon())
                    
                    bce = tf.keras.losses.BinaryCrossentropy()
                    
                    dy_true = y_true[:,1:,:]-y_true[:,:-1,:]
                    dy_pred = y_pred[:,1:,:]-y_pred[:,:-1,:]
                    #     w = (1.+100.*tf.cast(tf.reduce_any(tf.math.less_equal(y_true, scaled_FU_threshold),axis=2),'float'))

                    #     return mse(y_true,y_pred,w)+mse(dy_true,dy_pred,w[:,:-1])
                    
                    return mse(y_true,y_pred)+ mse(dy_true,dy_pred)#+bce(mask_threshold,mask_threshold_pred,sample_weight=1+fn)
                    # return mse(y_true,y_pred,1+fn) + bce(mask_threshold,mask_threshold_pred,1+fn)
                    # return  mse(y_true,y_pred,1+fn)

                
                else:
                    if use_exp_decay_loss:
                        # Forcing an exponential weight decay per time step in
                        # forecast horizon, i.e. put more weight on the error
                        # of the first forecast steps.
                        wtmp = tf.cast(tf.fill(tf.shape(tf.squeeze(y_true)),1.0),y_true.dtype)
                        exp_tmp = 1/tf.exp(tf.range(1,pred_len+1,dtype=y_true.dtype)*tau)
                        exp_w = tf.multiply(wtmp,exp_tmp)

                        w = exp_w
                    else:
                        w = 1

                    return mse(y_true,y_pred,sample_weight=w)


            return custom_loss

        # COMPILE MODEL WITH CUSTOM LOSS FUNCTION:
        model.compile(optimizer=optimizer, loss=[loss_wraper(scaled_FU_threshold),tf.keras.losses.BinaryCrossentropy()],
                        metrics=["mae"]
                        )
    else:
        # COMPILE MODEL:
        model.compile(optimizer=optimizer, loss=[loss,tf.keras.losses.BinaryCrossentropy()],
                        metrics=["mae"]
                        )


    # FIT/TRAIN MODEL:
    h = model.fit(train_windowed,
                    epochs = n_epochs,
                    validation_data = valid_windowed,
                    verbose = 2,
                    callbacks = training_callbacks)


    # SHOW TRAINING AND VALIDATION LOSS:
    # Compare the training and validation loss to diagnose overfitting
    # plot_loss = True
    if plot_loss:
        fig, ax = plt.subplots(figsize=[12, 6])
        ax.plot(h.history['loss'], 'o-')
        ax.plot(h.history['val_loss'], 'o-')
        plt.grid(True)
        plt.legend(['Loss', 'Val loss'])
        plt.xlabel('Number of epochs')
        plt.ylabel('MSE')

    # SHOW MODEL STRUCTURE:
    # show_modelgraph = False
    if show_modelgraph:
        plot_model(
            model,
            to_file='model_plot.png',
            # show_shapes=True,
            show_layer_names=True
            )

    # SHOW MODEL WEIGHTS:
    # show_weights = False
    if show_weights:
        model.summary()



    # EVALUATE THE MODEL PERFORMANCE
    # The model performance is evaluated using typical regression metrics (i.e. MAE, RMSE, R$^2$)

    # Reconstrcut predictions and targets, by scaling them back to their
    # original scales, and in the format (nsamples, pred_len):
    y_train,y_pred_train,y_clim_train,target_time_train = reconstruct_ysamples(model,train_windowed,train_clim_windowed,time_train_windowed,scaler,pred_len,n_frcst_vars,anomaly_target)
    y_valid,y_pred_valid,y_clim_valid,target_time_valid = reconstruct_ysamples(model,valid_windowed,valid_clim_windowed,time_valid_windowed,scaler,pred_len,n_frcst_vars,anomaly_target)
    y_test,y_pred_test,y_clim_test,target_time_test = reconstruct_ysamples(model,test_windowed,test_clim_windowed,time_test_windowed,scaler,pred_len,n_frcst_vars,anomaly_target)

    # Get the regression metrics:
    y_train = np.squeeze(y_train); y_clim_train = np.squeeze(y_clim_train)
    y_valid = np.squeeze(y_valid); y_clim_valid = np.squeeze(y_clim_valid)
    y_test = np.squeeze(y_test); y_clim_test = np.squeeze(y_clim_test)

    y_pred_train = np.squeeze(y_pred_train)
    y_pred_valid = np.squeeze(y_pred_valid)
    y_pred_test = np.squeeze(y_pred_test)

    rsqr_train, mae_train, rmse_train =  regression_metrics(y_train,y_pred_train)
    rsqr_valid, mae_valid, rmse_valid =  regression_metrics(y_valid,y_pred_valid)
    rsqr_test, mae_test, rmse_test =  regression_metrics(y_test,y_pred_test)

    print('TRAINING ---')
    print('Rsqr = '+ str(np.round(rsqr_train, 2)))
    print('MAE = '+ str(np.round(mae_train, 2)))
    print('RMSE = '+ str(np.round(rmse_train, 2)))
    print(' ')
    print('VALIDATION ---')
    print('Rsqr = '+ str(np.round(rsqr_valid, 2)))
    print('MAE = '+ str(np.round(mae_valid, 2)))
    print('RMSE = '+ str(np.round(rmse_valid, 2)))
    print(' ')
    print('TEST ---')
    print('Rsqr = '+ str(np.round(rsqr_test, 2)))
    print('MAE = '+ str(np.round(mae_test, 2)))
    print('RMSE = '+ str(np.round(rmse_test, 2)))
    print(' ')
    print('============================================')
    print(' ')
    print('TRAINING ---')
    print('Rsqr = '+ str(np.round(np.mean(rsqr_train), 4)))
    print('MAE = '+ str(np.round(np.mean(mae_train), 4)))
    print('RMSE = '+ str(np.round(np.mean(rmse_train), 4)))
    print(' ')
    print('VALIDATION ---')
    print('Rsqr = '+ str(np.round(np.mean(rsqr_valid), 4)))
    print('MAE = '+ str(np.round(np.mean(mae_valid), 4)))
    print('RMSE = '+ str(np.round(np.mean(rmse_valid), 4)))
    print(' ')
    print('TEST ---')
    print('Rsqr = '+ str(np.round(np.mean(rsqr_test), 4)))
    print('MAE = '+ str(np.round(np.mean(mae_test), 4)))
    print('RMSE = '+ str(np.round(np.mean(rmse_test), 4)))

    # plot_predictions = True
    if plot_predictions:
        # Plot predictions - TRAINING
        plot_prediction_timeseries(y_pred_train,y_train,y_clim_train,target_time_train, pred_type = 'training', lead=0, nyrs_plot= 2)
        plot_prediction_timeseries(y_pred_train,y_train,y_clim_train,target_time_train, pred_type = 'training', lead=50, nyrs_plot= 2)

        # Plot predictions - TESTING
        plot_prediction_timeseries(y_pred_test,y_test,y_clim_test,target_time_test,pred_type='testing', lead=0, nyrs_plot= 2)
        plot_prediction_timeseries(y_pred_test,y_test,y_clim_test,target_time_test,pred_type='testing', lead=50, nyrs_plot= 2)

    # return model,target_time_train,target_time_valid,target_time_test,y_pred_train,y_pred_valid,y_pred_test,y_train,y_valid,y_test,y_clim_train,y_clim_valid,y_clim_test
    return model,test_windowed,train_windowed,df_test_in,target_time_train,target_time_valid,target_time_test,y_pred_train,y_pred_valid,y_pred_test,y_train,y_valid,y_test,y_clim_train,y_clim_valid,y_clim_test

































#%%
# CHOOSE VALIDATION SCHEME:

# If valid_scheme == 'LOOk', need to specify how many folds to use for validation:
valid_scheme = 'LOOk'
nfolds = 5

# If valid_scheme == 'standard', need to specify bounds for train/valid/test sets:
valid_scheme = 'standard'
train_yr_start = 1992 # Training dataset: 1992 - 2010
valid_yr_start = 2011 # Validation dataset: 2011 - 2015
test_yr_start = 2016 # Testing dataset: 2016 - 2021



# SET MODEL AND TRAINING HYPER-PARAMETERS:

# Choose batch size:
# batch_size = 32
# batch_size = 64
# batch_size = 128
# batch_size = 256
batch_size = 512 # <--- This is the max size for running on bjerknes' GPU
# batch_size = 1024
# batch_size = 2048
# batch_size = 4096
# batch_size = 8192

# Choose learning rate: (Note: Optimizer is Adam)
# lr = 0.001
# lr = 0.002
# lr = 0.004
# lr = 0.008
lr = 0.016
# lr = 0.032
# lr = 0.064
# lr = 0.128
# lr = 0.256


# Choose loss function:
# loss = tf.keras.losses.MeanSquaredError()
# loss = tf.keras.losses.Huber()
# loss = tf.keras.losses.MeanAbsoluteError()
loss = 'custom_loss'

# Set max. number of epochs
n_epochs = 150
# n_epochs = 1

# Number of hidden neurons in Encoder and Decoder
latent_dim = 20

# Choose Dense layer activation function and data normalization type:
dense_act_func = 'sigmoid'
norm_type='MinMax'

# dense_act_func = None
# norm_type='Standard'



# Choose Dropout rate:
inp_dropout = 0
rec_dropout = 0

# Prediction window length, in days
pred_len = 60

# Input window length, in days
input_len = 128

# Select variables to use:
predictor_vars = ['Avg. Ta_max',
                  'Avg. Ta_min',
                  'Tot. snowfall',
                  'NAO',
                  'Avg. Twater',
                  'FUD-0060'
                  ]
# predictor_vars = ['Avg. Ta_max']

# forecast_vars = ['Avg. Ta_mean',
#                 'Tot. snowfall'
#                 ]
forecast_vars = ['Avg. Ta_mean']
# forecast_vars = []

target_var = ['Avg. Twater']

# Choose if using anomaly timeseries:
anomaly_target = False
anomaly_past = False
anomaly_frcst = False





























#%%
with tf.device(device_name):

    # LOADING THE DATA AND PRE-PROCESSING
    """
    The dataset is composed of the following variables:

    *   Dates (in days since 1900-01-01)
    *   Daily water temperature (in °C - from the Longueuil water filtration plant)
    *   ERA5 weather daily variables (*see belo*w)
    *   Daily discharge (in m$^3$/s) and level (in m) for the St-Lawrence River (at Lasalle and Pointe-Claire, respectively)
    *   Daily level (in m) for the Ottawa River (at Ste-Anne-de-Bellevue)
    *   Daily time series of climate indices (*see below*)
    *   Daily time series of monthly and seasonal forecasts from CanSIPS (*see below*)

    """

    # The dataset has been copied into an Excel spreadsheet for this example
    filepath = 'slice/data/colab/'
    df = pd.read_excel(local_path+filepath+'predictor_data_daily_timeseries.xlsx')

    # The dates column is not needed
    time = df['Days since 1900-01-01'].values
    df.drop(columns='Days since 1900-01-01', inplace=True)

    # Keep only data for 1992-2020.
    yr_start = 1992
    yr_end = 2020
    date_ref = dt.date(1900,1,1)
    it_start = np.where(time == (dt.date(yr_start,1,1)-date_ref).days)[0][0]
    it_end = np.where(time == (dt.date(yr_end+1,1,1)-date_ref).days)[0][0]

    df = df.iloc[it_start:it_end,:]
    time = time[it_start:it_end]

    # There is a missing data in the water temperature time series...
    # This gap occurs during the winter (in 2018), so we will fill it with zeros.
    df['Avg. Twater'][9886:9946] = 0

    # We also cap all negative water temperature to zero degrees.
    df['Avg. Twater'][df['Avg. Twater'] < 0] = 0

    # And replace nan values in FDD and TDD by zeros
    df['Tot. FDD'][np.isnan(df['Tot. FDD'])] = 0
    df['Tot. TDD'][np.isnan(df['Tot. TDD'])] = 0

    # Other nan values due to missing values are replaced by the
    # training climatology below...

    # Create the time vector for plotting purposes:
    first_day = (date_ref+dt.timedelta(days=int(time[0]))).toordinal()
    last_day = (date_ref+dt.timedelta(days=int(time[-1]))).toordinal()
    time_plot = np.arange(first_day, last_day + 1)

    plot_target = False
    if plot_target:
        # Plot the observed water temperature time series, which will be the target variable
        fig, ax = plt.subplots(figsize=[12, 6])
        ax.plot(time_plot[:], df['Avg. Twater'][:], color='C0', label='T$_w$')
        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))
        ax.set_xlabel('Time')
        ax.set_ylabel('Water temperature $[^{\circ}C]$')
        ax.legend(loc='best')
        ax.grid(True)


    # PREDICTOR SELECTION
    """
    The data frame will contain (in order):
      [the target variable] x 1 column
      [the past predictors] x n_predictors columns
      [the forecasts] x n_forecasts columns
    """
    df = df[target_var+predictor_vars+forecast_vars]


    # SPLITTING THE DATASET + FILLING NANS + DATA SCALING + CREATE SAMPLES
    """
    The data is first separated in train-valid-test sets according to the
    chosen CV scheme.

    Then a daily climatology is computed for each predictor by using a moving
    average window and using only values from the training set.
    Nan values in the predictors and the target time series are replaced by
    their daily climatological values.

    The data is then normalized (MinMaxScaler or StandardScaler) to help
    the learning process. The scaler uses only the training dataset for
    calibration.

    Finally, the samples are prepared using a rolling window with
    tf.data.Dataset objects that are batched and pre-fetched.
    """

    #---------------------------
    if valid_scheme == 'LOOk':

        from sklearn.model_selection import KFold

        valid_years = np.nan
        test_years = np.arange(yr_start,yr_end)

        y_test_all = np.zeros((len(test_years),365*28,pred_len))*np.nan
        y_valid_all = np.zeros((len(test_years),nfolds,365*28,pred_len))*np.nan
        y_train_all = np.zeros((len(test_years),365*28,pred_len))*np.nan

        y_pred_test_all = np.zeros((len(test_years),365*28,pred_len))*np.nan
        y_pred_valid_all = np.zeros((len(test_years),nfolds,365*28,pred_len))*np.nan
        y_pred_train_all = np.zeros((len(test_years),365*28,pred_len))*np.nan

        y_clim_test_all = np.zeros((len(test_years),365*28,pred_len))*np.nan
        y_clim_valid_all = np.zeros((len(test_years),nfolds,365*28,pred_len))*np.nan
        y_clim_train_all = np.zeros((len(test_years),365*28,pred_len))*np.nan

        target_time_test_all = np.zeros((len(test_years),365*28,pred_len))*np.nan
        target_time_valid_all = np.zeros((len(test_years),nfolds,365*28,pred_len))*np.nan
        target_time_train_all = np.zeros((len(test_years),365*28,pred_len))*np.nan


        kf = KFold(n_splits=nfolds)

        for iyr_test,yr_test in enumerate(test_years[10:12]):

            df_tmp = df.copy()
            istart_yr_test = np.where(time_plot == dt.date(yr_test, 4, 1).toordinal())[0][0]
            iend_yr_test = np.where(time_plot == dt.date(yr_test+1, 4, 1).toordinal())[0][0]

            is_test = np.zeros(df.shape[0], dtype=bool)
            is_test[istart_yr_test:iend_yr_test] = True

            # Mask test year from data set:
            df_tmp[is_test] = np.nan

            df_kfold = df[~is_test].copy()
            ind_kfold = df[~is_test].index

            df_test_fold = df[is_test].copy()
            ind_test_fold = df[is_test].index  # NOTE: THESE ARE THE SAME INDICES AS WHERE is_test IS TRUE
            test_dates = [dt.date.fromordinal(d) for d in time_plot[ind_test_fold]]
            time_test = time[ind_test_fold]
            time_test_plot = time_plot[ind_test_fold]

            # Get cross-validation folds (determined using all years other than test year)
            for ifold,[train_index, valid_index] in enumerate(kf.split(df_kfold.index)):
                if (ifold == 2): # REMOVE THIS AFTER!!!!!!!!!!!1

                    # GET TRAINING AND VALIDATION DATA SETS FOR THIS FOLD:
                    df_train_fold, df_valid_fold = df_kfold.iloc[train_index], df_kfold.iloc[valid_index]
                    ind_train_fold, ind_valid_fold = ind_kfold[train_index], ind_kfold[valid_index]

                    train_dates = [dt.date.fromordinal(d) for d in time_plot[ind_train_fold]]
                    valid_dates = [dt.date.fromordinal(d) for d in time_plot[ind_valid_fold]]

                    time_train = time[ind_train_fold]
                    time_valid = time[ind_valid_fold]
                    time_train_plot = time_plot[ind_train_fold]
                    time_valid_plot = time_plot[ind_valid_fold]

                    train_years = np.unique([dt.date.fromordinal(d).year for d in time_plot[ind_train_fold]])
                    valid_years = np.unique([dt.date.fromordinal(d).year for d in time_plot[ind_valid_fold]])

                    df_train = df_train_fold
                    df_valid = df_valid_fold
                    df_test = df_test_fold

                    ind_train = ind_train_fold
                    ind_valid = ind_valid_fold
                    ind_test = ind_test_fold

                    [model_out,
                    target_time_train,target_time_valid,target_time_test,
                    y_pred_train,y_pred_valid,y_pred_test,
                    y_train,y_valid,y_test,
                    y_clim_train,y_clim_valid,y_clim_test] = execute_fold(df_train, df_valid, df_test,
                                                                            ind_train, ind_valid, ind_test,
                                                                            train_years,time,
                                                                            time_train,time_valid,time_test,
                                                                            time_train_plot,time_valid_plot,time_test_plot,
                                                                            latent_dim,inp_dropout,rec_dropout,dense_act_func,
                                                                            len(predictor_vars),len(forecast_vars),
                                                                            input_len,pred_len,
                                                                            norm_type,anomaly_target,anomaly_past,anomaly_frcst,
                                                                            batch_size,lr,loss,n_epochs,
                                                                            plot_loss=True,
                                                                            plot_predictions=False,
                                                                            plot_targets=False,
                                                                            show_modelgraph=False,
                                                                            show_weights=False)

                    # SAVE VALIDATION PREDICTIONS AND METRICS
                    # FOR ALL FOLDS OF THAT TEST YEAR.
                    y_valid_all[iyr_test,ifold,0:y_valid.shape[0],:] = y_valid
                    y_pred_valid_all[iyr_test,ifold,0:y_pred_valid.shape[0],:] = y_pred_valid
                    y_clim_valid_all[iyr_test,ifold,0:y_clim_valid.shape[0],:] = y_clim_valid
                    target_time_valid_all[iyr_test,ifold,0:target_time_valid.shape[0],:] = target_time_valid



            # !!!!!!!!! PROBLEM HERE!!!!!!!!!!!
            # THE MODEL NEEDS TO BE RE-COMPILED BEFORE RE-TRAINING WITH ALL YEARS
            # OTHER THAN TEST YEAR, BECAUSE NOW IT TAKES THE ALREADY TRAINED MODEL
            # AND CONTINUES THE TRAINING BUT WITH DIFFERENT DATA....

            # Then, train with all other years to predict test year:
            df_train = df[~is_test].copy()
            df_valid = df[is_test].copy() # This is the same as test data, just as a placeholder
            df_test = df[is_test].copy()

            ind_train = df[~is_test].index
            ind_valid = df[is_test].index
            ind_test = df[is_test].index

            test_dates = [dt.date.fromordinal(d) for d in time_plot[ind_test]]

            time_train = time[ind_train]
            time_valid = time[ind_valid]
            time_test = time[ind_test]
            time_train_plot = time_plot[ind_train]
            time_valid_plot = time_plot[ind_valid]
            time_test_plot = time_plot[ind_test]

            train_years = np.unique([dt.date.fromordinal(d).year for d in time_plot[ind_train]])
            valid_years = np.unique([dt.date.fromordinal(d).year for d in time_plot[ind_valid]])

            [model_out_all,
            target_time_train,target_time_valid,target_time_test,
            y_pred_train,y_pred_valid,y_pred_test,
            y_train,y_valid,y_test,
            y_clim_train,y_clim_valid,y_clim_test] = execute_fold(df_train, df_valid, df_test,
                                                                    ind_train, ind_valid, ind_test,
                                                                    train_years,time,
                                                                    time_train,time_valid,time_test,
                                                                    time_train_plot,time_valid_plot,time_test_plot,
                                                                    latent_dim,inp_dropout,rec_dropout,dense_act_func,
                                                                    len(predictor_vars),len(forecast_vars),
                                                                    input_len,pred_len,
                                                                    norm_type,anomaly_target,anomaly_past,anomaly_frcst,
                                                                    batch_size,lr,loss,n_epochs,
                                                                    plot_loss=True,
                                                                    plot_predictions=False,
                                                                    plot_targets=False,
                                                                    show_modelgraph=False,
                                                                    show_weights=False)

            # SAVE TRAINING & TEST PREDICTIONS AND METRICS
            # FOR THAT TEST YEAR.
            y_train_all[iyr_test,0:y_train.shape[0],:] = y_train
            y_pred_train_all[iyr_test,0:y_pred_train.shape[0],:] = y_pred_train
            y_clim_train_all[iyr_test,0:y_clim_train.shape[0],:] = y_clim_train
            target_time_train_all[iyr_test,0:target_time_train.shape[0],:] = target_time_train

            y_test_all[iyr_test,0:y_test.shape[0],:] = y_test
            y_pred_test_all[iyr_test,0:y_pred_test.shape[0],:] = y_pred_test
            y_clim_test_all[iyr_test,0:y_clim_test.shape[0],:] = y_clim_test
            target_time_test_all[iyr_test,0:target_time_test.shape[0],:] = target_time_test







    #-----------------------------
    if valid_scheme == 'standard':

        # GET TRAINING AND VALIDATION DATA SETS
        train_years = np.arange(train_yr_start,valid_yr_start)
        valid_years = np.arange(valid_yr_start,test_yr_start)
        test_years = np.arange(test_yr_start,yr_end+1)

        istart_train = np.where(time_plot == dt.date(train_yr_start, 4, 1).toordinal())[0][0]
        istart_valid = np.where(time_plot == dt.date(valid_yr_start, 4, 1).toordinal())[0][0]
        istart_test = np.where(time_plot == dt.date(test_yr_start, 4, 1).toordinal())[0][0]
        ind_train = np.arange(istart_train,istart_valid)
        ind_valid = np.arange(istart_valid,istart_test)
        ind_test = np.arange(istart_test,len(time))

        time_train = time[ind_train]
        time_valid = time[ind_valid]
        time_test = time[ind_test]
        time_train_plot = time_plot[ind_train]
        time_valid_plot = time_plot[ind_valid]
        time_test_plot = time_plot[ind_test]

        df_train = df.iloc[ind_train]
        df_valid = df.iloc[ind_valid]
        df_test = df.iloc[ind_test]


        [model_out,
        test_windowed,train_windowed,df_test_in,
        target_time_train_all,target_time_valid_all,target_time_test_all,
        y_pred_train_all,y_pred_valid_all,y_pred_test_all,
        y_train_all,y_valid_all,y_test_all,
        y_clim_train_all,y_clim_valid_all,y_clim_test_all] = execute_fold(df_train, df_valid, df_test,
                                                                                     ind_train, ind_valid, ind_test,
                                                                                     train_years,time,
                                                                                     time_train,time_valid,time_test,
                                                                                     time_train_plot,time_valid_plot,time_test_plot,
                                                                                     latent_dim,inp_dropout,rec_dropout,dense_act_func,
                                                                                     len(predictor_vars),len(forecast_vars),
                                                                                     input_len,pred_len,
                                                                                     norm_type,anomaly_target,anomaly_past,anomaly_frcst,
                                                                                     batch_size,lr,loss,n_epochs,
                                                                                     plot_loss=True,
                                                                                     plot_predictions=False,
                                                                                     plot_targets=False,
                                                                                     show_modelgraph=False,
                                                                                     show_weights=False)

                                                                          
                                                                          
                                                                          
                                                                          
                                                                          
                                                                          
                                                                          
                                                                          
                                                                          
                                               
                                                                          
                                                                          
                                                                          
                                                                          
                                                                          
                                                                          
                                                                          
                                                                          
                                                                          
                                                                          


#%%
# plot_sample(y_pred_test,y_test,y_clim_test,target_time_test,it=210, pred_type = 'testing')
# plot_sample(y_pred_test,y_test,y_clim_test,target_time_test,it=220, pred_type = 'testing')
# plot_sample(y_pred_test,y_test,y_clim_test,target_time_test,it=240, pred_type = 'testing')

# plot_sample(y_pred_test,y_test,y_clim_test,target_time_test,it=200+365, pred_type = 'testing')
# plot_sample(y_pred_test,y_test,y_clim_test,target_time_test,it=210+365, pred_type = 'testing')
# plot_sample(y_pred_test,y_test,y_clim_test,target_time_test,it=220+365, pred_type = 'testing')

#%%
if valid_scheme == 'standard':
    plot_prediction_timeseries(y_pred_test_all,y_test_all,y_clim_test_all,target_time_test_all,pred_type='testing', lead=0, nyrs_plot= 28)
    plot_prediction_timeseries(y_pred_test_all,y_test_all,y_clim_test_all,target_time_test_all,pred_type='testing', lead=50, nyrs_plot= 28)
    # plot_prediction_timeseries(y_pred_train_all,y_train_all,y_clim_train_all,target_time_train_all,pred_type='training', lead=0, nyrs_plot= 28)
    # plot_prediction_timeseries(y_pred_valid_all,y_valid_all,y_clim_valid_all,target_time_valid_all,pred_type='valid', lead=0, nyrs_plot= 28)
    # plot_prediction_timeseries(y_pred_valid_all,y_valid_all,y_clim_valid_all,target_time_valid_all,pred_type='valid', lead=50, nyrs_plot= 28)

if valid_scheme == 'LOOk':
    # y_pred_in = np.reshape(y_pred_test_all,[28*365*28,pred_len])
    # y_pred_in = y_pred_in[np.where(np.all(~np.isnan(y_pred_in),axis=1))[0]]
    # y_in = np.reshape(y_test_all,[28*365*28,pred_len])
    # y_in = y_in[np.where(np.all(~np.isnan(y_in),axis=1))[0]]
    # y_clim_in = np.reshape(y_clim_test_all,[28*365*28,pred_len])
    # y_clim_in = y_clim_in[np.where(np.all(~np.isnan(y_clim_in),axis=1))[0]]
    # time_in = np.reshape(target_time_test_all,[28*365*28,pred_len])
    # time_in = time_in[np.where(np.all(~np.isnan(time_in),axis=1))[0]]
    # plot_prediction_timeseries(y_pred_in,y_in,y_clim_in,time_in,pred_type='testing', lead=0, nyrs_plot= 28)
    
    y_pred_in = np.reshape(y_pred_train_all,[28*365*28,pred_len])
    y_pred_in = y_pred_in[np.where(np.all(~np.isnan(y_pred_in),axis=1))[0]]
    y_in = np.reshape(y_train_all,[28*365*28,pred_len])
    y_in = y_in[np.where(np.all(~np.isnan(y_in),axis=1))[0]]
    y_clim_in = np.reshape(y_clim_train_all,[28*365*28,pred_len])
    y_clim_in = y_clim_in[np.where(np.all(~np.isnan(y_clim_in),axis=1))[0]]
    time_in = np.reshape(target_time_train_all,[28*365*28,pred_len])
    time_in = time_in[np.where(np.all(~np.isnan(time_in),axis=1))[0]]
    plot_prediction_timeseries(y_pred_in,y_in,y_clim_in,time_in,pred_type='training', lead=0, nyrs_plot= 56)
    
    # y_pred_in = np.reshape(y_pred_valid_all,[28*365*28*nfolds,pred_len])
    # y_pred_in = y_pred_in[np.where(np.all(~np.isnan(y_pred_in),axis=1))[0]]
    # y_in = np.reshape(y_valid_all,[28*365*28*nfolds,pred_len])
    # y_in = y_in[np.where(np.all(~np.isnan(y_in),axis=1))[0]]
    # y_clim_in = np.reshape(y_clim_valid_all,[28*365*28*nfolds,pred_len])
    # y_clim_in = y_clim_in[np.where(np.all(~np.isnan(y_clim_in),axis=1))[0]]
    # time_in = np.reshape(target_time_valid_all,[28*365*28*nfolds,pred_len])
    # time_in = time_in[np.where(np.all(~np.isnan(time_in),axis=1))[0]]
    # plot_prediction_timeseries(y_pred_in,y_in,y_clim_in,time_in,pred_type='valid', lead=0, nyrs_plot= 6)



#%%
# EVALUATE TWATER FORECASTS:

def evaluate_Tw_forecasts(y_pred_in,y_in,yclim_in,time_in,mname):

    def plot_Tw_metric(plot_Tw_metric,plot_Tw_clim_metric,metric_name,mname):
        vmin = 0; vmax = 2
        # cmap = cmocean.cm.tempo
        # cmap = cmocean.cm.dense
        # cmap = cmocean.cm.deep
        # cmap = cmocean.cm.thermal
        # cmap = plt.get_cmap('cividis')
        # cmap = plt.get_cmap('viridis')
        cmap = plt.get_cmap('magma')

        fig, axs = plt.subplots(1, 1, figsize=(6,4))
        mappable = axs.pcolormesh(np.flipud(plot_Tw_metric), cmap=cmap, vmin=vmin, vmax=vmax)
        axs.set_title('$T_{w}$ '+metric_name+' (deg. C) - ' + mname)
        fig.colorbar(mappable, ax=[axs], location='left')
        for imonth in range(12):
            axs.text(62,11.35-imonth,str(np.nanmean(plot_Tw_metric[imonth,:])))

        fig_clim, axs_clim = plt.subplots(1, 1, figsize=(6,4))
        mappable = axs_clim.pcolormesh(np.flipud(plot_Tw_clim_metric), cmap=cmap, vmin=vmin, vmax=vmax)
        axs_clim.set_title('$T_{w}$ '+metric_name+' (deg. C) - Climatology')
        fig_clim.colorbar(mappable, ax=[axs_clim], location='left')

        vmin = -0.95; vmax = 0.95
        cmap = cmocean.cm.balance
        fig_diff, axs_diff = plt.subplots(1, 1, figsize=(6,4))
        mappable = axs_diff.pcolormesh(np.flipud(plot_Tw_metric)-np.flipud(plot_Tw_clim_metric), cmap=cmap, vmin=vmin, vmax=vmax)
        axs_diff.set_title('$T_{w}$ '+metric_name+' diff. (deg. C)\n ' + mname+' - Climatology')
        fig_diff.colorbar(mappable, ax=[axs_diff], location='left')

    rsqr_Tw = np.zeros((12,pred_len))*np.nan; rsqr_Tw_clim = np.zeros((12,pred_len))*np.nan
    mae_Tw = np.zeros((12,pred_len))*np.nan; mae_Tw_clim = np.zeros((12,pred_len))*np.nan
    rmse_Tw = np.zeros((12,pred_len))*np.nan; rmse_Tw_clim = np.zeros((12,pred_len))*np.nan

    # for h in range(pred_len):
    for imonth in range(12):
        month = imonth+1
        samples_Tw = y_pred_in[np.where(np.array([(date_ref+dt.timedelta(days=int(time_in[s,0]))).month for s in range(time_in.shape[0])]) == month )[0]]
        targets_Tw = y_in[np.where(np.array([(date_ref+dt.timedelta(days=int(time_in[s,0]))).month for s in range(time_in.shape[0])]) == month )[0]]
        clim_Tw = yclim_in[np.where(np.array([(date_ref+dt.timedelta(days=int(time_in[s,0]))).month for s in range(time_in.shape[0])]) == month )[0]]

        if samples_Tw.shape[0] > 0:
            rsqr_Tw[imonth,:],mae_Tw[imonth,:],rmse_Tw[imonth,:] = regression_metrics(targets_Tw,samples_Tw)
            rsqr_Tw_clim[imonth,:], mae_Tw_clim[imonth,:], rmse_Tw_clim[imonth,:] =  regression_metrics(targets_Tw,clim_Tw)

            if month == 11:
                rsqr_nov,mae_nov,rmse_nov = regression_metrics(targets_Tw,samples_Tw,output_opt='uniform_average')
                print('Nov. Tw. MAE (days): '+ str(mae_nov) )
                # print(np.nanmean(mae_Tw[imonth,:]))
            if month == 12:
                rsqr_dec,mae_dec,rmse_dec = regression_metrics(targets_Tw,samples_Tw,output_opt='uniform_average')
                print('Dec. Tw. MAE (days): '+ str(mae_dec) )
                # print(np.nanmean(mae_Tw[imonth,:]))
                
    # Plot Tw forecast metrics
    # plot_Tw_metric(rsqr_Tw,rsqr_Tw_clim,'R$^2$',mname)
    plot_Tw_metric(mae_Tw,mae_Tw_clim,'MAE',mname)
    # plot_Tw_metric(rmse_Tw,rmse_Tw_clim,'RMSE',mname)


if valid_scheme == 'LOOk':
    model_name = 'Encoder-Decoder LSTM - TEST'
    y_pred_in = np.reshape(y_pred_test_all,[28*365*28,pred_len])
    y_pred_in = y_pred_in[np.where(np.all(~np.isnan(y_pred_in),axis=1))[0]]
    y_in = np.reshape(y_test_all,[28*365*28,pred_len])
    y_in = y_in[np.where(np.all(~np.isnan(y_in),axis=1))[0]]
    y_clim_in = np.reshape(y_clim_test_all,[28*365*28,pred_len])
    y_clim_in = y_clim_in[np.where(np.all(~np.isnan(y_clim_in),axis=1))[0]]
    time_in = np.reshape(target_time_test_all,[28*365*28,pred_len])
    time_in = time_in[np.where(np.all(~np.isnan(time_in),axis=1))[0]]
    evaluate_Tw_forecasts(y_pred_in,y_in,y_clim_in,time_in,model_name)

    model_name = 'Encoder-Decoder LSTM - VALID'
    y_pred_in = np.reshape(y_pred_valid_all,[28*365*28*nfolds,pred_len])
    y_pred_in = y_pred_in[np.where(np.all(~np.isnan(y_pred_in),axis=1))[0]]
    y_in = np.reshape(y_valid_all,[28*365*28*nfolds,pred_len])
    y_in = y_in[np.where(np.all(~np.isnan(y_in),axis=1))[0]]
    y_clim_in = np.reshape(y_clim_valid_all,[28*365*28*nfolds,pred_len])
    y_clim_in = y_clim_in[np.where(np.all(~np.isnan(y_clim_in),axis=1))[0]]
    time_in = np.reshape(target_time_valid_all,[28*365*28*nfolds,pred_len])
    time_in = time_in[np.where(np.all(~np.isnan(time_in),axis=1))[0]]
    evaluate_Tw_forecasts(y_pred_in,y_in,y_clim_in,time_in,model_name)

    model_name = 'Encoder-Decoder LSTM - TRAINING'
    y_pred_in = np.reshape(y_pred_train_all,[28*365*28,pred_len])
    y_pred_in = y_pred_in[np.where(np.all(~np.isnan(y_pred_in),axis=1))[0]]
    y_in = np.reshape(y_train_all,[28*365*28,pred_len])
    y_in = y_in[np.where(np.all(~np.isnan(y_in),axis=1))[0]]
    y_clim_in = np.reshape(y_clim_train_all,[28*365*28,pred_len])
    y_clim_in = y_clim_in[np.where(np.all(~np.isnan(y_clim_in),axis=1))[0]]
    time_in = np.reshape(target_time_train_all,[28*365*28,pred_len])
    time_in = time_in[np.where(np.all(~np.isnan(time_in),axis=1))[0]]
    evaluate_Tw_forecasts(y_pred_in,y_in,y_clim_in,time_in,model_name)

else:
    # model_name = 'Encoder-Decoder LSTM - TEST'
    # y_clim_test_all = np.squeeze(y_clim_test_all)
    # evaluate_Tw_forecasts(y_pred_test_all,y_test_all,y_clim_test_all,target_time_test_all,model_name)

    model_name = 'Encoder-Decoder LSTM - VALID'
    y_clim_valid_all = np.squeeze(y_clim_valid_all)
    evaluate_Tw_forecasts(y_pred_valid_all,y_valid_all,y_clim_valid_all,target_time_valid_all,model_name)

    # model_name = 'Encoder-Decoder LSTM - TRAINING'
    # y_clim_train_all = np.squeeze(y_clim_train_all)
    # evaluate_Tw_forecasts(y_pred_train_all,y_train_all,y_clim_train_all,target_time_train_all,model_name)


#%%


def find_freezeup_Tw_threshold(def_opt,Twater_in,time,thresh_T=2.0,ndays=7,date_ref=dt.date(1900,1,1)):

    def record_event(istart,time,year,Twater_in,freezeup_date,mask_freezeup,date_ref):
        # Temperature has been lower than thresh_T
        # for more than (or equal to) ndays.
        # Define freezeup date as first date of group

        date_start = date_ref+dt.timedelta(days=int(time[istart]))
        doy_start = (date_start - dt.date(int(date_start.year),1,1)).days+1

        if ((date_start.year > 1992) | ((date_start.year == 1992) & (date_start.month > 10)) ):
            if ( (date_start.year == year) & (doy_start > 319) ) | ((date_start.year == year+1) & (doy_start < 46)):
                    freezeup_date[0] = date_start.year
                    freezeup_date[1] = date_start.month
                    freezeup_date[2] = date_start.day
                    freezeup_Tw = Twater_in[istart]
                    mask_freezeup[istart] = True
            else:
                freezeup_date[0] = np.nan
                freezeup_date[1] = np.nan
                freezeup_date[2] = np.nan
                freezeup_Tw = np.nan
                mask_freezeup[istart] = False
        else:
        # I think this condition exists because the Tw time series
        # starts in January 1992, so it is already frozen, but we
        # do not want to detect this as freezeup for 1992, so we
        # have to wait until at least October 1992 before recording
        # any FUD events.
            freezeup_date[0] = np.nan
            freezeup_date[1] = np.nan
            freezeup_date[2] = np.nan
            freezeup_Tw = np.nan
            mask_freezeup[istart] = False

        return freezeup_date, freezeup_Tw, mask_freezeup

    date_start = dt.timedelta(days=int(time[0])) + date_ref
    if date_start.month < 3:
        year = date_start.year-1
    else:
        year = date_start.year

    mask_tmp = Twater_in <= thresh_T
    mask_freezeup = mask_tmp.copy()
    mask_freezeup[:] = False

    freezeup_Tw = np.nan
    freezeup_date=np.zeros((3))*np.nan

    # Loop on sample time steps
    for im in range(mask_freezeup.size):

        if (im == 0): # First time step cannot be detected as freeze-up
            sum_m = 0
            istart = -1 # This ensures that a freeze-up is not detected if the time series started already below the freezing temp.
        else:
            if (np.sum(mask_freezeup) == 0): # Only continue while no prior freeze-up was detected for the sequence
                if (~mask_tmp[im-1]):
                    sum_m = 0
                    if ~mask_tmp[im]:
                        sum_m = 0
                    else:
                        # start new group
                        sum_m +=1
                        istart = im
                        # Below will only occur if ndays is set to 1, e.g. first day of freezing temp.
                        if (sum_m >= ndays):
                            freezeup_date,freezeup_Tw,mask_freezeup = record_event(istart,time,year,Twater_in,freezeup_date,mask_freezeup,date_ref)
                else:
                    if (mask_tmp[im]) & (istart > 0):
                        sum_m += 1
                        if (sum_m >= ndays):
                            freezeup_date,freezeup_Tw,mask_freezeup = record_event(istart,time,year,Twater_in,freezeup_date,mask_freezeup,date_ref)

    return freezeup_date, mask_freezeup


#-----------------
freezeup_opt = 1
if freezeup_opt == 1:
    def_opt = 1
    smooth_T =False; N_smooth = 3; mean_type='centered'
    round_T = False; round_type= 'half_unit'
    Gauss_filter = False
    sig_dog = 3.5
    T_thresh = 0.75
    dTdt_thresh = 0.25
    d2Tdt2_thresh = 0.25
    nd = 1
    no_negTw = False

start_doy_arr =[307,        314,         321,         328,         335        ]
istart_label = ['Nov. 3rd', 'Nov. 10th', 'Nov. 17th', 'Nov. 24th', 'Dec. 1st' ]
month_istart = [11,11,11,11,12]
day_istart   = [ 3,10,17,24, 1]
date_ref = dt.date(1900,1,1)
years_eval = np.arange(1992,2020)

plot_samples = False
plot_samples = True

plot_FUD_ts = True

years = years_eval

y_true = y_test_all
y_pred = y_pred_test_all
y_clim = y_clim_test_all
time_y = target_time_test_all
model_name = 'Encoder-Decoder LSTM - TEST'

y_true = y_train_all
y_pred = y_pred_train_all
y_clim = y_clim_train_all
time_y = target_time_train_all
model_name = 'Encoder-Decoder LSTM - TRAINING'

y_true = y_valid_all
y_pred = y_pred_valid_all
y_clim = y_clim_valid_all
time_y = target_time_valid_all
model_name = 'Encoder-Decoder LSTM - VALID'
#-----------------


from functions import running_nanmean
import scipy
import scipy.ndimage
import calendar
import statsmodels.api as sm

# FIND FREEZE-UP FROM SIMULATED SAMPLES
nsamples_per_doy = np.zeros(len(start_doy_arr))*np.nan

for istart in range(len(start_doy_arr)):
    month = month_istart[istart]
    day = day_istart[istart]
    month_ind = np.where((np.array([(date_ref+dt.timedelta(days=int(time_y[s,0]))).month for s in range(time_y.shape[0])]) == month))[0]
    day_ind = np.where((np.array([(date_ref+dt.timedelta(days=int(time_y[s,0]))).day for s in range(time_y.shape[0])]) == day))[0]
    istart_ind = np.sort(list( set(month_ind).intersection(day_ind) ))
    nsamples_per_doy[istart] = len(istart_ind)

freezeup_dates_sample = np.zeros((len(start_doy_arr),int(np.nanmax(nsamples_per_doy)),4))*np.nan
freezeup_dates_sample_doy = np.zeros((len(start_doy_arr),int(np.nanmax(nsamples_per_doy))))*np.nan

freezeup_dates_target = np.zeros((len(start_doy_arr),int(np.nanmax(nsamples_per_doy)),4))*np.nan
freezeup_dates_target_doy = np.zeros((len(start_doy_arr),int(np.nanmax(nsamples_per_doy))))*np.nan

freezeup_dates_clim_target = np.zeros((len(start_doy_arr),int(np.nanmax(nsamples_per_doy)),4))*np.nan
freezeup_dates_clim_target_doy = np.zeros((len(start_doy_arr),int(np.nanmax(nsamples_per_doy))))*np.nan

for istart,start_doy in enumerate(start_doy_arr):

    # Find sample starting on the day & month of start date:
    month = month_istart[istart]
    day = day_istart[istart]
    month_ind = np.where((np.array([(date_ref+dt.timedelta(days=int(time_y[s,0]))).month for s in range(time_y.shape[0])]) == month))[0]
    day_ind = np.where((np.array([(date_ref+dt.timedelta(days=int(time_y[s,0]))).day for s in range(time_y.shape[0])]) == day))[0]
    istart_ind = np.sort(list( set(month_ind).intersection(day_ind) ))

    samples = y_pred[istart_ind]
    targets = y_true[istart_ind]
    clim_targets = y_clim[istart_ind]
    time_st = time_y[istart_ind]

    for s in range(samples.shape[0]):

        time = time_st[s,:]

        # FIND DTDt, D2Tdt2,etc. - SAMPLE
        Twater_sample = samples[s]

        Twater_tmp = Twater_sample.copy()
        Twater_dTdt_sample = np.zeros(Twater_sample.shape)*np.nan
        Twater_d2Tdt2_sample = np.zeros(Twater_sample.shape)*np.nan
        Twater_DoG1_sample = np.zeros(Twater_sample.shape)*np.nan
        Twater_DoG2_sample = np.zeros(Twater_sample.shape)*np.nan

        if round_T:
            if round_type == 'unit':
                Twater_tmp = np.round(Twater_tmp.copy())
            if round_type == 'half_unit':
                Twater_tmp = np.round(Twater_tmp.copy()* 2) / 2.
        if smooth_T:
            Twater_tmp = running_nanmean(Twater_tmp.copy(),N_smooth,mean_type=mean_type)

        dTdt_tmp = np.zeros((Twater_tmp.shape[0],3))*np.nan

        dTdt_tmp[0:-1,0]= Twater_tmp[1:]- Twater_tmp[0:-1] # Forwards
        dTdt_tmp[1:,1] = Twater_tmp[1:] - Twater_tmp[0:-1] # Backwards
        dTdt_tmp[0:-1,2]= Twater_tmp[0:-1]-Twater_tmp[1:]  # -1*Forwards

        Twater_dTdt_sample= np.nanmean(dTdt_tmp[:,0:2],axis=1)
        Twater_d2Tdt2_sample = -1*np.nanmean(dTdt_tmp[:,1:3],axis=1)

        if Gauss_filter:
            Twater_dTdt_sample = scipy.ndimage.gaussian_filter1d(Twater_tmp.copy(),sigma=sig_dog,order=1)
            Twater_d2Tdt2_sample = scipy.ndimage.gaussian_filter1d(Twater_tmp.copy(),sigma=sig_dog,order=2)

        # FIND DTDt, D2Tdt2,etc. - TARGET
        Twater_target = targets[s]
        Twater_dTdt_target = np.zeros(Twater_target.shape)*np.nan
        Twater_d2Tdt2_target = np.zeros(Twater_target.shape)*np.nan
        Twater_DoG1_target = np.zeros(Twater_target.shape)*np.nan
        Twater_DoG2_target = np.zeros(Twater_target.shape)*np.nan

        Twater_tmp_target = Twater_target.copy()
        if round_T:
            if round_type == 'unit':
                Twater_tmp_target = np.round(Twater_tmp_target.copy())
            if round_type == 'half_unit':
                Twater_tmp_target = np.round(Twater_tmp_target.copy()* 2) / 2.
        if smooth_T:
            Twater_tmp_target = running_nanmean(Twater_tmp_target.copy(),N_smooth,mean_type=mean_type)

        dTdt_tmp = np.zeros((Twater_tmp_target.shape[0],3))*np.nan

        dTdt_tmp[0:-1,0]= Twater_tmp_target[1:]- Twater_tmp_target[0:-1] # Forwards
        dTdt_tmp[1:,1] = Twater_tmp_target[1:] - Twater_tmp_target[0:-1] # Backwards
        dTdt_tmp[0:-1,2]= Twater_tmp_target[0:-1]-Twater_tmp_target[1:]  # -1*Forwards

        Twater_dTdt_target= np.nanmean(dTdt_tmp[:,0:2],axis=1)
        Twater_d2Tdt2_target = -1*np.nanmean(dTdt_tmp[:,1:3],axis=1)

        if Gauss_filter:
            Twater_dTdt_target = scipy.ndimage.gaussian_filter1d(Twater_tmp_target.copy(),sigma=sig_dog,order=1)
            Twater_d2Tdt2_target = scipy.ndimage.gaussian_filter1d(Twater_tmp_target.copy(),sigma=sig_dog,order=2)

        # FIND DTDt, D2Tdt2,etc. - CLIM TARGET
        Twater_clim_target = clim_targets[s]
        Twater_dTdt_clim_target = np.zeros(Twater_clim_target.shape)*np.nan
        Twater_d2Tdt2_clim_target = np.zeros(Twater_clim_target.shape)*np.nan
        Twater_DoG1_clim_target = np.zeros(Twater_clim_target.shape)*np.nan
        Twater_DoG2_clim_target = np.zeros(Twater_clim_target.shape)*np.nan

        Twater_tmp_clim_target = Twater_clim_target.copy()
        if round_T:
            if round_type == 'unit':
                Twater_tmp_clim_target = np.round(Twater_tmp_clim_target.copy())
            if round_type == 'half_unit':
                Twater_tmp_clim_target = np.round(Twater_tmp_clim_target.copy()* 2) / 2.
        if smooth_T:
            Twater_tmp_clim_target = running_nanmean(Twater_tmp_clim_target.copy(),N_smooth,mean_type=mean_type)

        dTdt_tmp = np.zeros((Twater_tmp_clim_target.shape[0],3))*np.nan

        dTdt_tmp[0:-1,0]= Twater_tmp_clim_target[1:]- Twater_tmp_clim_target[0:-1] # Forwards
        dTdt_tmp[1:,1] = Twater_tmp_clim_target[1:] - Twater_tmp_clim_target[0:-1] # Backwards
        dTdt_tmp[0:-1,2]= Twater_tmp_clim_target[0:-1]-Twater_tmp_clim_target[1:]  # -1*Forwards

        Twater_dTdt_clim_target= np.nanmean(dTdt_tmp[:,0:2],axis=1)
        Twater_d2Tdt2_clim_target = -1*np.nanmean(dTdt_tmp[:,1:3],axis=1)

        if Gauss_filter:
            Twater_dTdt_clim_target = scipy.ndimage.gaussian_filter1d(Twater_tmp_clim_target.copy(),sigma=sig_dog,order=1)
            Twater_d2Tdt2_clim_target = scipy.ndimage.gaussian_filter1d(Twater_tmp_clim_target.copy(),sigma=sig_dog,order=2)


        if (istart == 4):
            if plot_samples:
                plt.figure()
                plt.title(istart_label[istart]+', '+str((dt.timedelta(days=int(time[0])) + date_ref).year))
                plt.plot(Twater_target, color='black',label='Observed')
                plt.plot(Twater_clim_target,label='Climatology')
                plt.plot(Twater_sample,label='Forecast')
                plt.legend()


        # FIND FREEZE-UP FOR BOTH SAMPLE AND TARGET
        date_start = dt.timedelta(days=int(time[0])) + date_ref
        year = date_start.year

        if year >= years[0]:
            iyr = np.where(years == year)[0][0]
            fd_sample, mask_freeze_sample = find_freezeup_Tw_threshold(def_opt,Twater_tmp,time,thresh_T = T_thresh,ndays = nd)
            fd_target, mask_freeze_target = find_freezeup_Tw_threshold(def_opt,Twater_tmp_target,time,thresh_T = T_thresh,ndays = nd)
            fd_clim_target, mask_freeze_clim_target = find_freezeup_Tw_threshold(def_opt,Twater_tmp_clim_target,time,thresh_T = T_thresh,ndays = nd)

            if (np.sum(mask_freeze_sample) > 0): # A freeze-up was detected in sample
                if fd_sample[0] == year:
                    if calendar.isleap(years[iyr]):
                        freezeup_dates_sample[istart,s,0] = iyr
                        freezeup_dates_sample[istart,s,1:4] = fd_sample
                        freezeup_dates_sample_doy[istart,s]= (dt.date(int(fd_sample[0]),int(fd_sample[1]),int(fd_sample[2]))-dt.date(int(fd_sample[0]),1,1)).days
                    else:
                        freezeup_dates_sample[istart,s,0] = iyr
                        freezeup_dates_sample[istart,s,1:4] = fd_sample
                        freezeup_dates_sample_doy[istart,s]= (dt.date(int(fd_sample[0]),int(fd_sample[1]),int(fd_sample[2]))-dt.date(int(fd_sample[0]),1,1)).days+1
                else:
                    if calendar.isleap(years[iyr]):
                        freezeup_dates_sample[istart,s,0] = iyr
                        freezeup_dates_sample[istart,s,1:4] = fd_sample
                        freezeup_dates_sample_doy[istart,s]= fd_sample[2]+365
                    else:
                        freezeup_dates_sample[istart,s,0] = iyr
                        freezeup_dates_sample[istart,s,1:4] = fd_sample
                        freezeup_dates_sample_doy[istart,s]= fd_sample[2]+365


            if (np.sum(mask_freeze_target) > 0): # A freeze-up was detected in target
                if fd_target[0] == year:
                    if calendar.isleap(years[iyr]):
                        freezeup_dates_target[istart,s,0] = iyr
                        freezeup_dates_target[istart,s,1:4] = fd_target
                        freezeup_dates_target_doy[istart,s]= (dt.date(int(fd_target[0]),int(fd_target[1]),int(fd_target[2]))-dt.date(int(fd_target[0]),1,1)).days
                    else:
                        freezeup_dates_target[istart,s,0] = iyr
                        freezeup_dates_target[istart,s,1:4] = fd_target
                        freezeup_dates_target_doy[istart,s]= (dt.date(int(fd_target[0]),int(fd_target[1]),int(fd_target[2]))-dt.date(int(fd_target[0]),1,1)).days+1
                else:
                    if calendar.isleap(years[iyr]):
                        freezeup_dates_target[istart,s,0] = iyr
                        freezeup_dates_target[istart,s,1:4] = fd_target
                        freezeup_dates_target_doy[istart,s]= fd_target[2]+365
                    else:
                        freezeup_dates_target[istart,s,0] = iyr
                        freezeup_dates_target[istart,s,1:4] = fd_target
                        freezeup_dates_target_doy[istart,s]= fd_target[2]+365

            if (np.sum(mask_freeze_clim_target) > 0): # A freeze-up was detected in climatology
                if fd_clim_target[0] == year:
                    freezeup_dates_clim_target[istart,s,0] = iyr
                    if calendar.isleap(years[iyr]):
                        freezeup_dates_clim_target[istart,s,1] = fd_clim_target[0]
                        freezeup_dates_clim_target[istart,s,2] = fd_clim_target[1]
                        freezeup_dates_clim_target[istart,s,3] = fd_clim_target[2]+1
                        freezeup_dates_clim_target_doy[istart,s]= (dt.date(int(fd_clim_target[0]),int(fd_clim_target[1]),int(fd_clim_target[2]))-dt.date(int(fd_clim_target[0]),1,1)).days+1
                    else:
                        freezeup_dates_clim_target[istart,s,1:4] = fd_clim_target
                        freezeup_dates_clim_target_doy[istart,s]= (dt.date(int(fd_clim_target[0]),int(fd_clim_target[1]),int(fd_clim_target[2]))-dt.date(int(fd_clim_target[0]),1,1)).days+1


# FIND MEAN FUD FROM Tw CLIM:
mean_clim_FUD = np.nanmean(freezeup_dates_clim_target_doy)
# if recalibrate:
#     if offset_type == 'mean_clim':
#         offset_forecasts = mean_clim_FUD
#     freezeup_dates_sample_doy = freezeup_dates_sample_doy - offset_forecasts + mean_FUD_Longueuil_train



# EVALUATE FREEZE-UP FORECAST ACCORDING TO SELECTED METRIC:
fu_rmse = np.zeros((len(start_doy_arr),len(years)))*np.nan
fu_acc = np.zeros((len(start_doy_arr),len(years)))*np.nan
fu_mae = np.zeros((len(start_doy_arr),len(years)))*np.nan

MAE_arr = np.zeros((len(start_doy_arr)))*np.nan
RMSE_arr = np.zeros((len(start_doy_arr)))*np.nan
Rsqr_arr = np.zeros((len(start_doy_arr)))*np.nan
Rsqradj_arr = np.zeros((len(start_doy_arr)))*np.nan
Acc_arr = np.zeros((len(start_doy_arr)))*np.nan

for istart in range(len(start_doy_arr)):
    n = 0

    # The FUD is not detectabble because the forecast
    # length doesn't reach the FUD (or because there
    # were no samples for that period, e.g. because
    # of the presence of nans in the predictors time
    # series)
    
    # years_impossible =[]
    # tmp_arr = years.copy().astype('float')
    # for iyr in range(len(years)):
    #     if len(np.where(freezeup_dates_target[istart,:,0]==iyr)[0])>0:
    #         i_s = np.where(freezeup_dates_target[istart,:,0]==iyr)[0]
    #         if ~np.isnan(freezeup_dates_target[istart,i_s,0]):
    #             tmp_arr[iyr] = np.nan
    # years_impossible = years_impossible + tmp_arr[~np.isnan(tmp_arr)].tolist()

    # print(years_impossible)
    # years_impossible = np.array(years_impossible).astype('int')
    sample_freezeup_doy = freezeup_dates_sample_doy[istart,:]
    clim_target_freezeup_doy = freezeup_dates_clim_target_doy[istart,:]
    target_freezeup_doy = freezeup_dates_target_doy[istart,:]
    ts_doy = np.zeros(len(years))*np.nan
    ts_doy_obs = np.zeros(len(years))*np.nan

    # Evaluate the performance only on detectable FUDs
    if np.sum(~np.isnan(target_freezeup_doy)) > 0:

        for iyr in range(len(years)):
            if len(np.where(freezeup_dates_target[istart,:,0]==iyr)[0])>0:
                i_st = np.where(freezeup_dates_target[istart,:,0]==iyr)[0]
                fo_doy = target_freezeup_doy[i_st]
                ts_doy_obs[iyr] = fo_doy
                
                if (~np.isnan(target_freezeup_doy[i_st])):
                    n += 1
                    if len(np.where(freezeup_dates_sample[istart,:,0]==iyr)[0])>0:
                        i_ss = np.where(freezeup_dates_sample[istart,:,0]==iyr)[0][0]
                        fs_doy = sample_freezeup_doy[i_ss]
                        ts_doy[iyr] = fs_doy
                        fc_doy = mean_clim_FUD
    
                        # obs_cat = Longueuil_FUD_period_cat[iyr]
                        # if ~np.isnan(fs_doy):
                        #     if fs_doy <= tercile1_FUD_Longueuil_train:
                        #         sample_cat = -1
                        #     elif fs_doy > tercile2_FUD_Longueuil_train:
                        #         sample_cat = 1
                        #     else:
                        #         sample_cat = 0
                        #     if (sample_cat == obs_cat):
                        #         fu_acc[istart,iyr] = 1
                        #     else:
                        #         fu_acc[istart,iyr] = 0
                        # else:
                        #     fu_acc[istart,iyr] = np.nan
    
                        fu_rmse[istart,iyr] = (fs_doy-fo_doy)**2.
                        fu_mae[istart,iyr] = np.abs(fs_doy-fo_doy)
                        # print(istart,iyr,n,fo_doy,fs_doy,fc_doy,obs_cat,sample_cat,fu_acc[istart,iyr])
    
                    else:
                        ts_doy[iyr] = np.nan
                        fu_rmse[istart,iyr] = np.nan
                        fu_mae[istart,iyr] = np.nan
                        # fu_acc[istart,iyr] = np.nan
    
                        # print(istart,iyr,n,Longueuil_FUD_period[iyr],np.nan,mean_clim_FUD,Longueuil_FUD_period_cat[iyr],np.nan,fu_acc[istart,iyr])

    # Here we don't use nanmean, but we nansum and divide by "n", where
    # "n" is the number of detectable FUDs for the given start date.
    # If an FUD was not forecasted/detected but it could have been,
    # then n will be larger than the available number of ML forecasts so the
    # performance is penalized.
    # print(istart,n)
    if np.all(np.isnan(fu_mae[istart,:])):
        MAE_arr[istart] = np.nan
    else:
        MAE_arr[istart] = np.nansum(fu_mae[istart,:])/n

    if np.all(np.isnan(fu_rmse[istart,:])):
        RMSE_arr[istart] = np.nan
    else:
        RMSE_arr[istart] = np.sqrt(np.nansum(fu_rmse[istart,:])/n)

    # if  np.all(np.isnan(fu_acc[istart,:])):
    #     Acc_arr[istart] = np.nan
    # else:
    #     Acc_arr[istart] = np.nansum(fu_acc[istart,:])/n

    # if (np.all(np.isnan(ts_doy))) | (np.all(np.isnan(Longueuil_FUD_period))) :
    #     Rsqr_arr[istart,il,iw,ih] = np.nan
    #     Rsqradj_arr[istart,il,iw,ih] = np.nan
    # else:
    #     model = sm.OLS(Longueuil_FUD_period, sm.add_constant(ts_doy,has_constant='skip'), missing='drop').fit()
    #     Rsqr_arr[istart,il,iw,ih] = model.rsquared
    #     Rsqradj_arr[istart,il,iw,ih] = model.rsquared_adj


if plot_FUD_ts:
    # PLOT FUD TIME SERIES
    fig, ax = plt.subplots()
    ax.plot(years,np.ones(len(years))*(mean_clim_FUD),color=plt.get_cmap('tab20c')(2))
    # ax.plot(years,np.ones(len(years))*(mean_FUD_Longueuil_train),color=[0.7,0.7,0.7])
    ax.plot(years,ts_doy_obs,'o-',color='black')
    
    for ic,istart in enumerate([0,1,2,3,4]):

        figi, axi = plt.subplots()
        axi.plot(years,np.ones(len(years))*(mean_clim_FUD),color=plt.get_cmap('tab20c')(2))
        axi.plot(years,ts_doy_obs,'o-',color='black')
        
        fd_ML_forcast = np.zeros((len(years)))*np.nan
        for iyr in range(len(years)):
            select_yr_fud = freezeup_dates_sample[istart,np.where(freezeup_dates_sample[istart,:,0] == iyr)[0]]
            if select_yr_fud.shape[0] > 0:
                fd_ML_forcast[iyr] = freezeup_dates_sample_doy[istart,np.where(freezeup_dates_sample[istart,:,0] == iyr)[0]]
        # print(fd_ML_forcast)
        ax.plot(years,fd_ML_forcast,'o:', color=plt.get_cmap('tab20c')(7-ic), label= model_name + ' - '+istart_label[istart])
        axi.plot(years,fd_ML_forcast,'o:', color=plt.get_cmap('tab20c')(7-ic), label= model_name + ' - '+istart_label[istart])
        
        if ~np.all(np.isnan(fd_ML_forcast)):
            model = sm.OLS(ts_doy_obs, sm.add_constant(fd_ML_forcast,has_constant='skip'), missing='drop').fit()
            print('-----------------------------')
            print('START DATE: ' + istart_label[istart])
            print('Rsqr: ',model.rsquared, model.rsquared_adj)
            print('MAE: ', MAE_arr[istart])
            print('RMSE: ',RMSE_arr[istart])
            print('Acc.: ',Acc_arr[istart])

        # if recalibrate:
        #     if offset_type == 'mean_clim':
        #         plt.title('Recalibrated Forecasts - Mean clim\n'+'nlayers:'+str(nlayers)+', input window: '+str(inpw))
        # else:
        #     plt.title('Raw Forecasts\n'+'nlayers:'+str(nlayers)+', input window: '+str(inpw))
        plt.title('Raw Forecasts')

    ax.legend()



istart_plot = [0,1,2,3,4]
fig_mae,ax_mae = plt.subplots(nrows = 1, ncols = len(istart_plot),figsize=(14,3))
letter = ['a)','b)','c)','d)','e)','f)']
for i,istart in enumerate(istart_plot):
    p_mae=ax_mae[i].imshow(np.expand_dims(np.expand_dims(MAE_arr[istart],axis=0),axis=0),vmin=3,vmax=9)
    # p_mae=ax_mae[i].imshow(np.squeeze(MAE_arr[istart]),vmin=3,vmax=9)
    # ax_mae[0].set_ylabel('Nb. Layers')
    # ax_mae[1].set_xlabel('Context Window (days)')
    # ax_mae[i].set_yticks(np.arange(len(nb_layers_list)))
    # ax_mae[i].set_yticklabels([str(nb_layers_list[k]) for k in range(len(nb_layers_list))])
    # ax_mae[i].set_xticks(np.arange(len(input_window_list)))
    # ax_mae[i].set_xticklabels([str(input_window_list[k]) for k in range(len(input_window_list))])
    ax_mae[i].set_title(letter[i]+' '+istart_label[istart])

fig_mae.subplots_adjust(left=0.2)
cbar_mae_ax = fig_mae.add_axes([0.07, 0.16, 0.02, 0.62])
fig_mae.colorbar(p_mae, cax=cbar_mae_ax)
cbar_mae_ax.text(-0.95,4.75,'MAE (days)',fontsize=14, rotation = 90)










#%% BELOW IS FOR WORKING ON TRANSFERRING FUD DETECTION TO TF.TENSORS

# y_pred = np.expand_dims(y_pred_test[200:200+512,:],axis=-1)
# y_true = np.expand_dims(y_test[200:200+512,:],axis=-1)
y_pred = np.expand_dims(y_pred_train_all[1200:1200+512,:],axis=-1)
y_true = np.expand_dims(y_train_all[1200:1200+512,:],axis=-1)

def_opt = 1
Twater_in = y_true
time = np.squeeze(target_time_test_all[200+365,:])
thresh_T=0.75
ndays=1
date_ref=dt.date(1900,1,1)

mask_threshold = tf.math.less_equal(y_true, thresh_T)
mask_threshold_pred = tf.math.less_equal(y_pred, thresh_T)

n_same = tf.equal(mask_threshold,mask_threshold_pred)

n_thresh = tf.reduce_sum(tf.cast(mask_threshold,'int32'),axis=1)
n_thresh_pred = tf.reduce_sum(tf.cast(mask_threshold_pred,'int32'),axis=1)
#%%
# tf.where(tf.equal(mask_threshold[0],mask_threshold_pred[0]),
#           y_true[0],tf.constant(100,shape=tf.shape(y_true[0]))
#           )

# tf.equal(mask_threshold[0],mask_threshold_pred[0])

# metrics.confusion_matrix(y_true[0],y_pred[0])
thresh_T=0.75
mask_threshold = tf.cast(tf.math.less_equal(y_true, thresh_T), 'float')
mask_threshold_pred = tf.cast(tf.math.less_equal(y_pred, thresh_T), 'float')



tp = tf.reduce_sum(tf.cast(tf.math.multiply(mask_threshold,mask_threshold_pred), 'float'), axis=1)
tn = tf.reduce_sum(tf.cast(tf.math.multiply(1-mask_threshold,1-mask_threshold_pred), 'float'), axis=1)
fp = tf.reduce_sum(tf.cast(tf.math.multiply(1-mask_threshold,mask_threshold_pred), 'float'), axis=1)
fn = tf.reduce_sum(tf.cast(tf.math.multiply(mask_threshold,1-mask_threshold_pred), 'float'), axis=1)

precision = tp / (tp + fp + K.epsilon())
recall = tp / (tp + fn + K.epsilon())

# f1 = 2*precision*recall / (precision+recall+K.epsilon())
# f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)
#%%
# # [batchsize,pred_len,1]
# y_pred = np.expand_dims(y_pred_train_all[1200:1200+512,:],axis=-1)
# y_true = np.expand_dims(y_train_all[1200:1200+512,:],axis=-1)

# mse = tf.keras.losses.MeanSquaredError()
# dy_true = y_true[:,1:,:]-y_true[:,:-1,:]
# dy_pred = y_pred[:,1:,:]-y_pred[:,:-1,:]

# # tmp = tf.concat(dy_true,tf.constant(1.0,shape=[tf.shape(y_true)[0],1,1]))

# l = 0.8*mse(y_true,y_pred)+0.2*mse(dy_true,dy_pred)

# wtmp = (tf.cast(tf.math.less_equal(y_true, 0.75),'float'))

# w = (1.+100.*tf.cast(tf.reduce_any(tf.math.less_equal(y_true, 0.75),axis=2),'float'))
  



#%%
# def record_event_tf(istart,time,year,Twater_in,freezeup_date,mask_freezeup,date_ref):
#     # Temperature has been lower than thresh_T
#     # for more than (or equal to) ndays.
#     # Define freezeup date as first date of group

#     date_start = date_ref+dt.timedelta(days=int(time[istart]))
#     doy_start = (date_start - dt.date(int(date_start.year),1,1)).days+1

#     if ((date_start.year > 1992) | ((date_start.year == 1992) & (date_start.month > 10)) ):
#         if ( (date_start.year == year) & (doy_start > 319) ) | ((date_start.year == year+1) & (doy_start < 46)):
#                 freezeup_date[0] = date_start.year
#                 freezeup_date[1] = date_start.month
#                 freezeup_date[2] = date_start.day
#                 freezeup_Tw = Twater_in[istart]
#                 mask_freezeup[istart] = True
#         else:
#             freezeup_date[0] = np.nan
#             freezeup_date[1] = np.nan
#             freezeup_date[2] = np.nan
#             freezeup_Tw = np.nan
#             mask_freezeup[istart] = False
#     else:
#     # I think this condition exists because the Tw time series
#     # starts in January 1992, so it is already frozen, but we
#     # do not want to detect this as freezeup for 1992, so we
#     # have to wait until at least October 1992 before recording
#     # any FUD events.
#         freezeup_date[0] = np.nan
#         freezeup_date[1] = np.nan
#         freezeup_date[2] = np.nan
#         freezeup_Tw = np.nan
#         mask_freezeup[istart] = False

#     return freezeup_date, freezeup_Tw, mask_freezeup


# date_start = dt.timedelta(days=int(time[0])) + date_ref
# if date_start.month < 3:
#     year = date_start.year-1
# else:
#     year = date_start.year


# mask_threshold = tf.math.less_equal(Twater_in, thresh_T)
# mask_freezeup = tf.constant(False, dtype=bool,shape=tf.shape(mask_threshold))

# # tf.where(mask_threshold)

# #%%

# freezeup_date=np.zeros((3))*np.nan
# isample = 0
# # Loop on sample time steps
# for im in range(mask_freezeup.shape[1]):

#     if (im == 0): # First time step cannot be detected as freeze-up
#         sum_m = 0
#         istart = -1 # This ensures that a freeze-up is not detected if the time series started already below the freezing temp.
#     else:
#         if (tf.equal(tf.reduce_sum(tf.cast(mask_freezeup,'int32'), axis=1)[isample],0)):
#         # if (np.sum(mask_freezeup) == 0): # Only continue while no prior freeze-up was detected for the sequence
#             if (~mask_threshold[im-1]):
#                 sum_m = 0
#                 if ~mask_threshold[im]:
#                     sum_m = 0
#                 else:
#                     # start new group
#                     sum_m +=1
#                     istart = im
#                     # Below will only occur if ndays is set to 1, e.g. first day of freezing temp.
#                     if (sum_m >= ndays):
#                         freezeup_date,freezeup_Tw,mask_freezeup = record_event_tf(istart,time,year,Twater_in,freezeup_date,mask_freezeup,date_ref)
#             else:
#                 if (mask_threshold[im]) & (istart > 0):
#                     sum_m += 1
#                     if (sum_m >= ndays):
#                         freezeup_date,freezeup_Tw,mask_freezeup = record_event_tf(istart,time,year,Twater_in,freezeup_date,mask_freezeup,date_ref)










# #%%
# # BELOW IS JUST A SPACE TO TEST CUSTOM LOSS IMPLEMENTATION
# mse = tf.keras.losses.MeanSquaredError()
# # mse2 = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)

# # [batchsize,pred_len,1]
# y_pred = np.expand_dims(y_pred_test[0:512,:],axis=-1)
# y_true = np.expand_dims(y_test[0:512,:],axis=-1)

# # penalize_FN = True
# penalize_FN = False

# # use_exp_decay_loss = True
# use_exp_decay_loss = False

# if penalize_FN:
#     # Check for each sample in targets and predictions if Tw reaches below the freeze-up threshold
#     obs_FU_batch = tf.cast(tf.reduce_any(tf.math.less(y_true,0.75),axis=1),y_true.dtype)
#     pred_FU_batch = tf.cast(tf.reduce_any(tf.math.less(y_pred,0.75),axis=1),y_true.dtype)

#     # One weight per sample, depending if sample is a false positive,
#     # false negative, true positive or true negative for detected freeze-up
#     w_batch = tf.constant(1,y_true.dtype,shape=obs_FU_batch.shape) + 2*tf.math.squared_difference(obs_FU_batch,pred_FU_batch) + tf.math.subtract(obs_FU_batch,pred_FU_batch)
#     w_batch = tf.squeeze(w_batch)

#     if use_exp_decay_loss:
#         # Forcing an exponential weight decay per time step in
#         # forecast horizon, i.e. put more weight on the error
#         # of the first forecast steps.
#         obs_FU_all_steps = tf.cast(tf.reduce_any(tf.math.less(y_true,0.75),axis=-1),y_true.dtype)
#         pred_FU_all_steps = tf.cast(tf.reduce_any(tf.math.less(y_pred,0.75),axis=-1),y_true.dtype)

#         wtmp = tf.cast(tf.fill(tf.shape(obs_FU_all_steps),1.0),y_true.dtype)
#         exp_tmp = 1/tf.exp(tf.range(1,pred_len+1,dtype=y_true.dtype))
#         exp_w = tf.multiply(wtmp,exp_tmp)

#         w = tf.multiply(exp_w,tf.expand_dims(w_batch,axis=-1))
#     else:
#         w = w_batch

# else:
#     if use_exp_decay_loss:
#         # Forcing an exponential weight decay per time step in
#         # forecast horizon, i.e. put more weight on the error
#         # of the first forecast steps.
#         obs_FU_all_steps = tf.cast(tf.reduce_any(tf.math.less(y_true,0.75),axis=-1),y_true.dtype)
#         pred_FU_all_steps = tf.cast(tf.reduce_any(tf.math.less(y_pred,0.75),axis=-1),y_true.dtype)

#         wtmp = tf.cast(tf.fill(tf.shape(tf.squeeze(y_true)),1.0),y_true.dtype)
#         exp_tmp = 1/tf.exp(tf.range(1,pred_len+1,dtype=y_true.dtype))
#         exp_w = tf.multiply(wtmp,exp_tmp)

#         w = exp_w
#     else:
#         w = 1

# out = mse(y_true,y_pred,sample_weight=w)


# exp_tmp = 1/tf.exp(tf.range(1,pred_len+1,dtype=y_true.dtype))
# tmp = tf.reduce_sum(tf.math.log(tf.reduce_mean(tf.math.square(y_true-y_pred),axis=0)),axis=0)

# tmp = tf.math.log(tf.reduce_mean(tf.math.square(y_true-y_pred),axis=0))



# #%%

# # Epoch 1/50
# # 14/14 - 589s - loss: 5.7569 - mae: 0.4655 - val_loss: 1.3554 - val_mae: 0.3147 - lr: 0.0160 - 589s/epoch - 42s/step


# # TRAINING ---
# # Rsqr = 0.9857
# # MAE = 0.6915
# # RMSE = 0.9985

# # VALIDATION ---
# # Rsqr = 0.9852
# # MAE = 0.7562
# # RMSE = 1.0602

# # TEST ---
# # Rsqr = 0.9824
# # MAE = 0.8597
# # RMSE = 1.1815