# -*- coding: utf-8 -*-
"""EncoderDecoder_LSTM_TFDatasets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Dc3S0Zp_3ZazEEGBBPmOuVy48HqPPsqp
"""
#%%
local_path = '/storage/amelie/'
# local_path = '/Volumes/SeagateUSB/McGill/Postdoc/'

#%%
import tensorflow as tf

import sys
import os
FCT_DIR = os.path.dirname(os.path.abspath(local_path +'slice/prog/'+'/prog/'))
if not FCT_DIR in sys.path:
    sys.path.append(FCT_DIR)

import tensorflow.keras.backend as K
from tensorflow import keras
from tensorflow.keras.utils import plot_model
from keras.layers import Lambda
import sklearn.metrics as metrics

import numpy as np
import pandas as pd
import datetime as dt
import time as timer

from matplotlib import pyplot as plt
from matplotlib import dates as mdates
import cmocean

import gc

from functions import rolling_climo
from functions_ML import regression_metrics, plot_sample, plot_prediction_timeseries

from functions_encoderdecoder import fit_scaler, normalize_df, get_predictor_clim, replace_nan_with_clim, create_dataset, reconstruct_ysamples, encoder_decoder_recursive, execute_fold

#%%
# CHOOSE RUN SETTINGS:

use_GPU = False

print("Num CPUs Available: ", len(tf.config.list_physical_devices('CPU')))
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

if use_GPU:
    if len(tf.config.list_physical_devices('GPU')) > 0:
        device_name = '/GPU:0'
    else:
        print('No GPU available... Defaulting to using CPU.')
        device_name = '/CPU:0'
else:
    device_name = '/CPU:0'


# To see on which device the operation are done, uncomment below:
# tf.debugging.set_log_device_placement(True)

# Set random seed:
fixed_seed = True
fixed_seed = False

seed = 422
if fixed_seed:
    tf.keras.utils.set_random_seed(seed)

#%%
# CHOOSE MODEL SETTINGS:

start_time = timer.time()

# If valid_scheme == 'LOOk', need to specify how many folds to use for validation:
valid_scheme = 'LOOk'
nfolds = 5

# If valid_scheme == 'standard', need to specify bounds for train/valid/test sets:
# valid_scheme = 'standard'
train_yr_start = 1992 # Training dataset: 1992 - 2010
valid_yr_start = 2011 # Validation dataset: 2011 - 2015
test_yr_start = 2016 # Testing dataset: 2016 - 2021


# SET MODEL AND TRAINING HYPER-PARAMETERS:

# Choose batch size:
# batch_size = 32
# batch_size = 64
# batch_size = 128
# batch_size = 256
batch_size = 512 # <--- This is the max size for running on bjerknes' GPU
# batch_size = 1024
# batch_size = 2048
# batch_size = 4096
# batch_size = 8192

# Choose learning rate: (Note: Optimizer is Adam)
optimizer_name = 'Adam'
# lr = 0.001
# lr = 0.002
# lr = 0.004
# lr = 0.008
lr = 0.016
# lr = 0.032
# lr = 0.064
# lr = 0.128
# lr = 0.256


# Choose loss function:
loss = 'MSETw'
# loss = 'MSETw_MSEdTwdt'
# loss = 'MSETw_penalize_FN'
# loss = 'MSETw_penalize_recall'
# loss = 'log_loss'

use_exp_decay_loss = False
tau = 30

weights_on_timesteps = False
added_weight = 1.
Tw_thresh = 0.75

loss_name = (loss
             +weights_on_timesteps*('_with_weights'+str(added_weight).rsplit('.')[0]+'_on_thresh'+str(Tw_thresh).rsplit('.')[0]+'_'+str(Tw_thresh).rsplit('.')[1])
             +use_exp_decay_loss*('_exp_decay_tau'+str(tau))
             )


# Set max. number of epochs
n_epochs = 50
# n_epochs = 12

# Number of hidden neurons in Encoder and Decoder
latent_dim = 50
nb_layers = 1

# Choose Dense layer activation function and data normalization type:
# dense_act_func = 'sigmoid'
# norm_type='MinMax'

dense_act_func = None
norm_type='Standard'


# Choose Dropout rate:
inp_dropout = 0
rec_dropout = 0

# Prediction window length, in days
pred_len = 60
# pred_len = 4

# Input window length, in days
input_len = 128

# Select variables to use:
predictor_vars = [#'Avg. Ta_mean',
                  'Avg. Ta_min',
                  'Avg. Ta_max',
                  'Avg. cloud cover',
                  'Tot. snowfall',
                  'NAO',
                  'Avg. Twater',
                  'Avg. discharge St-L. River',
                  #'Avg. SH (sfc)',
                  #'Avg. SW down (sfc)',
                  ]
# predictor_vars = ['Avg. Ta_max']

forecast_vars = ['Avg. Ta_mean',
                'Tot. snowfall',
                # 'Avg. cloud cover',
                # 'Avg. discharge St-L. River',
                # 'NAO',
                # 'Avg. SH (sfc)',
                # 'Avg. SW down (sfc)',
#                 'Avg. SLP'
                ]
# forecast_vars = []

target_var = ['Avg. Twater']

# Choose if using anomaly timeseries:
# anomaly_target = True
# anomaly_past = True
# anomaly_frcst = True
anomaly_target = False
anomaly_past = False
anomaly_frcst = False

save_model_outputs = True
# save_model_outputs = False

# suffix = '_perfectexp_Ta_mean_cloud_cover_snowfall'
suffix = '_perfectexp_Ta_mean_snowfall'
# suffix = '_perfectexp_Ta_mean'
# suffix = '_perfectexp'
# suffix = '_no_forecast_vars'

#%% RUN MODEL
with tf.device(device_name):

    # LOADING THE DATA AND PRE-PROCESSING
    """
    The dataset is composed of the following variables:

    *   Dates (in days since 1900-01-01)
    *   Daily water temperature (in Â°C - from the Longueuil water filtration plant)
    *   ERA5 weather daily variables (*see belo*w)
    *   Daily discharge (in m$^3$/s) and level (in m) for the St-Lawrence River (at Lasalle and Pointe-Claire, respectively)
    *   Daily level (in m) for the Ottawa River (at Ste-Anne-de-Bellevue)
    *   Daily time series of climate indices (*see below*)
    *   Daily time series of monthly and seasonal forecasts from CanSIPS (*see below*)

    """

    # The dataset has been copied into an Excel spreadsheet for this example
    filepath = 'slice/data/colab/'
    df = pd.read_excel(local_path+filepath+'predictor_data_daily_timeseries.xlsx')

    # The dates column is not needed
    time = df['Days since 1900-01-01'].values
    df.drop(columns='Days since 1900-01-01', inplace=True)

    # Keep only data for 1992-2020.
    yr_start = 1992
    yr_end = 2020
    date_ref = dt.date(1900,1,1)
    it_start = np.where(time == (dt.date(yr_start,1,1)-date_ref).days)[0][0]
    it_end = np.where(time == (dt.date(yr_end+1,1,1)-date_ref).days)[0][0]

    df = df.iloc[it_start:it_end,:]
    time = time[it_start:it_end]

    # There is a missing data in the water temperature time series...
    # This gap occurs during the winter (in 2018), so we will fill it with zeros.
    df['Avg. Twater'][9886:9946] = 0

    # We also cap all negative water temperature to zero degrees.
    df['Avg. Twater'][df['Avg. Twater'] < 0] = 0

    # And replace nan values in FDD and TDD by zeros
    df['Tot. FDD'][np.isnan(df['Tot. FDD'])] = 0
    df['Tot. TDD'][np.isnan(df['Tot. TDD'])] = 0

    # Other nan values due to missing values are replaced by the
    # training climatology below...

    # Create the time vector for plotting purposes:
    first_day = (date_ref+dt.timedelta(days=int(time[0]))).toordinal()
    last_day = (date_ref+dt.timedelta(days=int(time[-1]))).toordinal()
    time_plot = np.arange(first_day, last_day + 1)

    plot_target = False
    if plot_target:
        # Plot the observed water temperature time series, which will be the target variable
        fig, ax = plt.subplots(figsize=[12, 6])
        ax.plot(time_plot[:], df['Avg. Twater'][:], color='C0', label='T$_w$')
        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))
        ax.set_xlabel('Time')
        ax.set_ylabel('Water temperature $[^{\circ}C]$')
        ax.legend(loc='best')
        ax.grid(True)


    # PREDICTOR SELECTION
    """
    The data frame will contain (in order):
      [the target variable] x 1 column
      [the past predictors] x n_predictors columns
      [the forecasts] x n_forecasts columns
    """
    df = df[target_var+predictor_vars+forecast_vars]


    # SPLITTING THE DATASET + FILLING NANS + DATA SCALING + CREATE SAMPLES
    """
    The data is first separated in train-valid-test sets according to the
    chosen CV scheme.

    Then a daily climatology is computed for each predictor by using a moving
    average window and using only values from the training set.
    Nan values in the predictors and the target time series are replaced by
    their daily climatological values.

    The data is then normalized (MinMaxScaler or StandardScaler) to help
    the learning process. The scaler uses only the training dataset for
    calibration.

    Finally, the samples are prepared using a rolling window with
    tf.data.Dataset objects that are batched and pre-fetched.
    """

    #---------------------------
    if valid_scheme == 'LOOk':

        from sklearn.model_selection import KFold

        valid_years = np.nan
        test_years = np.arange(yr_start,yr_end)

        kf = KFold(n_splits=nfolds)

        for iyr_test,yr_test in enumerate(test_years):
        # for iyr_test,yr_test in enumerate(test_years[10:12]):
            df_tmp = df.copy()
            istart_yr_test = np.where(time_plot == dt.date(yr_test, 4, 1).toordinal())[0][0]
            iend_yr_test = np.where(time_plot == dt.date(yr_test+1, 4, 1).toordinal())[0][0]

            is_test = np.zeros(df.shape[0], dtype=bool)
            is_test[istart_yr_test:iend_yr_test] = True

            # Mask test year from data set:
            df_tmp[is_test] = np.nan

            df_kfold = df[~is_test].copy()
            ind_kfold = df[~is_test].index

            df_test_fold = df[is_test].copy()
            ind_test_fold = df[is_test].index  # NOTE: THESE ARE THE SAME INDICES AS WHERE is_test IS TRUE
            test_dates = [dt.date.fromordinal(d) for d in time_plot[ind_test_fold]]
            time_test = time[ind_test_fold]
            time_test_plot = time_plot[ind_test_fold]

            y_valid_all = np.zeros((nfolds,365*28,pred_len))*np.nan
            y_pred_valid_all = np.zeros((nfolds,365*28,pred_len))*np.nan
            y_clim_valid_all = np.zeros((nfolds,365*28,pred_len))*np.nan
            target_time_valid_all = np.zeros((nfolds,365*28,pred_len))*np.nan
            history_valid_all = dict()
            history_test_all = dict()

            # Get cross-validation folds (determined using all years other than test year)
            for ifold,[train_index, valid_index] in enumerate(kf.split(df_kfold.index)):
                # if (ifold == 2) | (ifold == 3): # REMOVE THIS AFTER!!!!!!!!!!!1

                # GET TRAINING AND VALIDATION DATA SETS FOR THIS FOLD:
                df_train_fold, df_valid_fold = df_kfold.iloc[train_index], df_kfold.iloc[valid_index]
                ind_train_fold, ind_valid_fold = ind_kfold[train_index], ind_kfold[valid_index]

                train_dates = [dt.date.fromordinal(d) for d in time_plot[ind_train_fold]]
                valid_dates = [dt.date.fromordinal(d) for d in time_plot[ind_valid_fold]]

                time_train = time[ind_train_fold]
                time_valid = time[ind_valid_fold]
                time_train_plot = time_plot[ind_train_fold]
                time_valid_plot = time_plot[ind_valid_fold]

                train_years = np.unique([dt.date.fromordinal(d).year for d in time_plot[ind_train_fold]])
                valid_years = np.unique([dt.date.fromordinal(d).year for d in time_plot[ind_valid_fold]])

                [model_out,
                 history_valid_all['loss'+str(ifold)], history_valid_all['val_loss'+str(ifold)],
                _,target_time_valid,_,
                _,y_pred_valid,_,
                _,y_valid,_,
                _,y_clim_valid,_] = execute_fold(df_train_fold, df_valid_fold, df_test_fold,
                                                    ind_train_fold, ind_valid_fold, ind_test_fold,
                                                    train_years,time,
                                                    time_train,time_valid,time_test,
                                                    time_train_plot,time_valid_plot,time_test_plot,
                                                    latent_dim,nb_layers,inp_dropout,rec_dropout,dense_act_func,
                                                    len(predictor_vars),len(forecast_vars),
                                                    input_len,pred_len,
                                                    norm_type,anomaly_target,anomaly_past,anomaly_frcst,
                                                    batch_size,lr,
                                                    loss,use_exp_decay_loss,tau,weights_on_timesteps,added_weight,Tw_thresh,
                                                    n_epochs,
                                                    plot_loss=False,
                                                    plot_predictions=False,
                                                    plot_targets=False,
                                                    show_modelgraph=False,
                                                    show_weights=False)

                # SAVE VALIDATION PREDICTIONS AND METRICS
                # FOR ALL FOLDS OF THAT TEST YEAR.
                y_valid_all[ifold,0:y_valid.shape[0],:] = y_valid
                y_pred_valid_all[ifold,0:y_valid.shape[0],:] = y_pred_valid
                y_clim_valid_all[ifold,0:y_valid.shape[0],:] = y_clim_valid
                target_time_valid_all[ifold,0:y_valid.shape[0],:] = target_time_valid


                print('=====================================================')
                print(str(yr_test) + ' --- FOLD ' + str(ifold) + ' OF ' + str(nfolds)+ ' COMPLETE')
                print('=====================================================')
                del target_time_valid,y_pred_valid,y_valid,y_clim_valid,model_out
                gc.collect()

            # Remove other variables that are no longer needed
            del df_kfold, df_tmp, df_test_fold
            gc.collect()

            # Then, train with all other years to predict test year:
            df_train = df[~is_test].copy()
            df_valid = df[is_test].copy() # This is the same as test data, just as a placeholder
            df_test = df[is_test].copy()

            ind_train = df[~is_test].index
            ind_valid = df[is_test].index
            ind_test = df[is_test].index

            test_dates = [dt.date.fromordinal(d) for d in time_plot[ind_test]]

            time_train = time[ind_train]
            time_valid = time[ind_valid]
            time_test = time[ind_test]
            time_train_plot = time_plot[ind_train]
            time_valid_plot = time_plot[ind_valid]
            time_test_plot = time_plot[ind_test]

            train_years = np.unique([dt.date.fromordinal(d).year for d in time_plot[ind_train]])
            valid_years = np.unique([dt.date.fromordinal(d).year for d in time_plot[ind_valid]])

            [model_out_all,
            history_test_all['loss'], history_test_all['val_loss'],
            target_time_train,_,target_time_test,
            y_pred_train,_,y_pred_test,
            y_train,_,y_test,
            y_clim_train,_,y_clim_test] = execute_fold(df_train, df_valid, df_test,
                                                        ind_train, ind_valid, ind_test,
                                                        train_years,time,
                                                        time_train,time_valid,time_test,
                                                        time_train_plot,time_valid_plot,time_test_plot,
                                                        latent_dim,nb_layers,inp_dropout,rec_dropout,dense_act_func,
                                                        len(predictor_vars),len(forecast_vars),
                                                        input_len,pred_len,
                                                        norm_type,anomaly_target,anomaly_past,anomaly_frcst,
                                                        batch_size,lr,
                                                        loss,use_exp_decay_loss,tau,weights_on_timesteps,added_weight,Tw_thresh,
                                                        n_epochs,
                                                        plot_loss=False,
                                                        plot_predictions=False,
                                                        plot_targets=False,
                                                        show_modelgraph=False,
                                                        show_weights=False)

            # SAVE ALL VARIABLES FOR THAT TEST YEAR.
            if save_model_outputs:
                if dense_act_func is None: dense_act_func_name = 'None'
                np.savez('./output/encoderdecoder_horizon'+str(pred_len)+'_context'+str(input_len)+'_nneurons'+str(latent_dim)+'_nepochs'+str(n_epochs)+(nb_layers > 1)*('_'+str(nb_layers)+'layers')+'_'+norm_type+'_'+dense_act_func_name+'_'+loss_name+anomaly_target*'_anomaly_target'+suffix+'_'+str(yr_test),
                        target_time_train=target_time_train,
                        target_time_valid=target_time_valid_all,
                        target_time_test=target_time_test,
                        y_pred_train_all=y_pred_train,
                        y_pred_valid=y_pred_valid_all,
                        y_pred_test=y_pred_test,
                        y_train=y_train,
                        y_valid=y_valid_all,
                        y_test=y_test,
                        y_clim_train=y_clim_train,
                        y_clim_valid=y_clim_valid_all,
                        y_clim_test=y_clim_test,
                        predictor_vars = predictor_vars,
                        forecast_vars = forecast_vars,
                        target_var = target_var,
                        input_len = input_len,
                        pred_len = pred_len,
                        n_epochs = n_epochs,
                        date_ref = date_ref,
                        anomaly_target = anomaly_target,
                        anomaly_past =  anomaly_past,
                        anomaly_frcst = anomaly_frcst,
                        train_yr_start = train_yr_start,
                        valid_yr_start = valid_yr_start,
                        test_yr_start = test_yr_start,
                        valid_scheme = valid_scheme,
                        nfolds = nfolds,
                        norm_type = norm_type,
                        dense_act_func = dense_act_func,
                        latent_dim = latent_dim,
                        nb_layers = nb_layers,
                        batch_size = batch_size,
                        learning_rate = lr,
                        seed = seed,
                        fixed_seed = fixed_seed,
                        inp_dropout = inp_dropout,
                        rec_dropout = rec_dropout,
                        test_history = history_test_all,
                        valid_history = history_valid_all,
                        optimizer_name = optimizer_name,
                        loss_name = loss_name,
                        )





            print('=====================================================')
            print(str(yr_test) + ' --- END')
            print('=====================================================')
            del df_train, df_test, df_valid
            del target_time_train,target_time_valid_all,target_time_test
            del y_pred_train,y_pred_valid_all,y_pred_test,
            del y_train,y_valid_all,y_test,
            del y_clim_train,y_clim_valid_all,y_clim_test
            del history_test_all, history_valid_all
            gc.collect()


    #-----------------------------
    if valid_scheme == 'standard':

        # GET TRAINING AND VALIDATION DATA SETS
        train_years = np.arange(train_yr_start,valid_yr_start)
        valid_years = np.arange(valid_yr_start,test_yr_start)
        test_years = np.arange(test_yr_start,yr_end+1)

        istart_train = np.where(time_plot == dt.date(train_yr_start, 4, 1).toordinal())[0][0]
        istart_valid = np.where(time_plot == dt.date(valid_yr_start, 4, 1).toordinal())[0][0]
        istart_test = np.where(time_plot == dt.date(test_yr_start, 4, 1).toordinal())[0][0]
        ind_train = np.arange(istart_train,istart_valid)
        ind_valid = np.arange(istart_valid,istart_test)
        ind_test = np.arange(istart_test,len(time))

        time_train = time[ind_train]
        time_valid = time[ind_valid]
        time_test = time[ind_test]
        time_train_plot = time_plot[ind_train]
        time_valid_plot = time_plot[ind_valid]
        time_test_plot = time_plot[ind_test]

        df_train = df.iloc[ind_train]
        df_valid = df.iloc[ind_valid]
        df_test = df.iloc[ind_test]

        history = dict()

        [model_out,history['loss'], history['val_loss'],
        target_time_train,target_time_valid,target_time_test,
        y_pred_train,y_pred_valid,y_pred_test,
        y_train,y_valid,y_test,
        y_clim_train,y_clim_valid,y_clim_test] = execute_fold(df_train, df_valid, df_test,
                                                                ind_train, ind_valid, ind_test,
                                                                train_years,time,
                                                                time_train,time_valid,time_test,
                                                                time_train_plot,time_valid_plot,time_test_plot,
                                                                latent_dim,nb_layers,inp_dropout,rec_dropout,dense_act_func,
                                                                len(predictor_vars),len(forecast_vars),
                                                                input_len,pred_len,
                                                                norm_type,anomaly_target,anomaly_past,anomaly_frcst,
                                                                batch_size,lr,
                                                                loss,use_exp_decay_loss,tau,weights_on_timesteps,added_weight,Tw_thresh,
                                                                n_epochs,
                                                                plot_loss=True,
                                                                plot_predictions=False,
                                                                plot_targets=False,
                                                                show_modelgraph=False,
                                                                show_weights=False)

        suffix = '_perfectexp'
        # suffix = '_TEST'
        if save_model_outputs:
            if dense_act_func is None: dense_act_func_name = 'None'
            np.savez('./output/encoderdecoder_horizon'+str(pred_len)+'_context'+str(input_len)+'_nneurons'+str(latent_dim)+'_nepochs'+str(n_epochs)+(nb_layers > 1)*('_'+str(nb_layers)+'layers')+'_'+norm_type+'_'+dense_act_func_name+'_'+loss_name+anomaly_target*'_anomaly_target'+suffix,
            # np.savez('./output/TEST,
                    target_time_train=target_time_train,
                    target_time_valid=target_time_valid,
                    target_time_test=target_time_test,
                    y_pred_train_all=y_pred_train,
                    y_pred_valid=y_pred_valid,
                    y_pred_test=y_pred_test,
                    y_train=y_train,
                    y_valid=y_valid,
                    y_test=y_test,
                    y_clim_train=y_clim_train,
                    y_clim_valid=y_clim_valid,
                    y_clim_test=y_clim_test,
                    predictor_vars = predictor_vars,
                    forecast_vars = forecast_vars,
                    target_var = target_var,
                    input_len = input_len,
                    pred_len = pred_len,
                    n_epochs = n_epochs,
                    date_ref = date_ref,
                    anomaly_target = anomaly_target,
                    anomaly_past =  anomaly_past,
                    anomaly_frcst = anomaly_frcst,
                    train_yr_start = train_yr_start,
                    valid_yr_start = valid_yr_start,
                    test_yr_start = test_yr_start,
                    valid_scheme = valid_scheme,
                    nfolds = nfolds,
                    norm_type = norm_type,
                    dense_act_func = dense_act_func,
                    latent_dim = latent_dim,
                    nb_layers = nb_layers,
                    batch_size = batch_size,
                    learning_rate = lr,
                    seed = seed,
                    fixed_seed = fixed_seed,
                    inp_dropout = inp_dropout,
                    rec_dropout = rec_dropout,
                    test_history = history,
                    valid_history = history,
                    optimizer_name = optimizer_name,
                    loss_name = loss_name,
                    )


end_time = timer.time()
print('=====================================================')
print('END!')
print('Total time: ' + str(end_time - start_time) + ' seconds')







#%%

# from functions_ML import regression_metrics, plot_sample, plot_prediction_timeseries
# from compare_encoderdecoder_output import load_data_year
# from compare_encoderdecoder_output import plot_Tw_metric, evaluate_Tw_forecasts
# from compare_encoderdecoder_output import detect_FUD_from_Tw_samples, evaluate_FUD_forecasts


# pred_type = 'valid'
# # pred_type = 'train'
# if pred_type == 'valid':
#     y = y_valid
#     y_pred = y_pred_valid
#     y_clim = y_clim_valid
#     target_time = target_time_valid
# if pred_type == 'test':
#     y = y_test
#     y_pred = y_pred_test
#     y_clim = y_clim_test
#     target_time = target_time_test
# if pred_type == 'train':
#     y = y_train
#     y_pred = y_pred_train
#     y_clim = y_clim_train
#     target_time = target_time_train

# plot_prediction_timeseries(y_pred,y,y_clim,target_time,pred_type, lead=0, nyrs_plot= 28)
# plot_prediction_timeseries(y_pred,y,y_clim,target_time,pred_type, lead=50, nyrs_plot= 28)

# plot_sample(y_pred,y,y_clim,target_time,it=125,pred_type=pred_type,show_clim=True)


# TRAINING ---
# Rsqr = 0.9943
# MAE = 0.4762
# RMSE = 0.6301

# VALIDATION ---
# Rsqr = 0.9936
# MAE = 0.5161
# RMSE = 0.6808

# TEST ---
# Rsqr = 0.9929
# MAE = 0.5672
# RMSE = 0.7384

#%% BELOW IS FOR WORKING ON TRANSFERRING FUD DETECTION TO TF.TENSORS

# # y_pred = np.expand_dims(y_pred_test[200:200+512,:],axis=-1)
# # y_true = np.expand_dims(y_test[200:200+512,:],axis=-1)
# y_pred = np.expand_dims(y_pred_train_all[1200:1200+512,:],axis=-1)
# y_true = np.expand_dims(y_train_all[1200:1200+512,:],axis=-1)

# def_opt = 1
# Twater_in = y_true
# time = np.squeeze(target_time_test_all[200+365,:])
# thresh_T=0.75
# ndays=1
# date_ref=dt.date(1900,1,1)

# mask_threshold = tf.math.less_equal(y_true, thresh_T)
# mask_threshold_pred = tf.math.less_equal(y_pred, thresh_T)

# n_same = tf.equal(mask_threshold,mask_threshold_pred)

# n_thresh = tf.reduce_sum(tf.cast(mask_threshold,'int32'),axis=1)
# n_thresh_pred = tf.reduce_sum(tf.cast(mask_threshold_pred,'int32'),axis=1)
#%%
# tf.where(tf.equal(mask_threshold[0],mask_threshold_pred[0]),
#           y_true[0],tf.constant(100,shape=tf.shape(y_true[0]))
#           )

# tf.equal(mask_threshold[0],mask_threshold_pred[0])

# metrics.confusion_matrix(y_true[0],y_pred[0])
# thresh_T=0.75
# mask_threshold = tf.cast(tf.math.less_equal(y_true, thresh_T), 'float')
# mask_threshold_pred = tf.cast(tf.math.less_equal(y_pred, thresh_T), 'float')



# tp = tf.reduce_sum(tf.cast(tf.math.multiply(mask_threshold,mask_threshold_pred), 'float'), axis=1)
# tn = tf.reduce_sum(tf.cast(tf.math.multiply(1-mask_threshold,1-mask_threshold_pred), 'float'), axis=1)
# fp = tf.reduce_sum(tf.cast(tf.math.multiply(1-mask_threshold,mask_threshold_pred), 'float'), axis=1)
# fn = tf.reduce_sum(tf.cast(tf.math.multiply(mask_threshold,1-mask_threshold_pred), 'float'), axis=1)

# precision = tp / (tp + fp + K.epsilon())
# recall = tp / (tp + fn + K.epsilon())

# f1 = 2*precision*recall / (precision+recall+K.epsilon())
# f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)
#%%
# [batchsize,pred_len,1]
# y_pred = np.expand_dims(y_pred_train_all[0,1200:1200+512,:],axis=-1)
# y_true = np.expand_dims(y_train_all[0,1200:1200+512,:],axis=-1)

# mse = tf.keras.losses.MeanSquaredError()
# dy_true = y_true[:,1:,:]-y_true[:,:-1,:]
# dy_pred = y_pred[:,1:,:]-y_pred[:,:-1,:]

# # # tmp = tf.concat(dy_true,tf.constant(1.0,shape=[tf.shape(y_true)[0],1,1]))

# # l = 0.8*mse(y_true,y_pred)+0.2*mse(dy_true,dy_pred)

# # wtmp = (tf.cast(tf.math.less_equal(y_true, 0.75),'float'))

# w = (tf.cast(tf.reduce_any(tf.math.less_equal(y_true, 3.0),axis=2),'float'))




#%%
# def record_event_tf(istart,time,year,Twater_in,freezeup_date,mask_freezeup,date_ref):
#     # Temperature has been lower than thresh_T
#     # for more than (or equal to) ndays.
#     # Define freezeup date as first date of group

#     date_start = date_ref+dt.timedelta(days=int(time[istart]))
#     doy_start = (date_start - dt.date(int(date_start.year),1,1)).days+1

#     if ((date_start.year > 1992) | ((date_start.year == 1992) & (date_start.month > 10)) ):
#         if ( (date_start.year == year) & (doy_start > 319) ) | ((date_start.year == year+1) & (doy_start < 46)):
#                 freezeup_date[0] = date_start.year
#                 freezeup_date[1] = date_start.month
#                 freezeup_date[2] = date_start.day
#                 freezeup_Tw = Twater_in[istart]
#                 mask_freezeup[istart] = True
#         else:
#             freezeup_date[0] = np.nan
#             freezeup_date[1] = np.nan
#             freezeup_date[2] = np.nan
#             freezeup_Tw = np.nan
#             mask_freezeup[istart] = False
#     else:
#     # I think this condition exists because the Tw time series
#     # starts in January 1992, so it is already frozen, but we
#     # do not want to detect this as freezeup for 1992, so we
#     # have to wait until at least October 1992 before recording
#     # any FUD events.
#         freezeup_date[0] = np.nan
#         freezeup_date[1] = np.nan
#         freezeup_date[2] = np.nan
#         freezeup_Tw = np.nan
#         mask_freezeup[istart] = False

#     return freezeup_date, freezeup_Tw, mask_freezeup


# date_start = dt.timedelta(days=int(time[0])) + date_ref
# if date_start.month < 3:
#     year = date_start.year-1
# else:
#     year = date_start.year


# mask_threshold = tf.math.less_equal(Twater_in, thresh_T)
# mask_freezeup = tf.constant(False, dtype=bool,shape=tf.shape(mask_threshold))

# # tf.where(mask_threshold)

# #%%

# freezeup_date=np.zeros((3))*np.nan
# isample = 0
# # Loop on sample time steps
# for im in range(mask_freezeup.shape[1]):

#     if (im == 0): # First time step cannot be detected as freeze-up
#         sum_m = 0
#         istart = -1 # This ensures that a freeze-up is not detected if the time series started already below the freezing temp.
#     else:
#         if (tf.equal(tf.reduce_sum(tf.cast(mask_freezeup,'int32'), axis=1)[isample],0)):
#         # if (np.sum(mask_freezeup) == 0): # Only continue while no prior freeze-up was detected for the sequence
#             if (~mask_threshold[im-1]):
#                 sum_m = 0
#                 if ~mask_threshold[im]:
#                     sum_m = 0
#                 else:
#                     # start new group
#                     sum_m +=1
#                     istart = im
#                     # Below will only occur if ndays is set to 1, e.g. first day of freezing temp.
#                     if (sum_m >= ndays):
#                         freezeup_date,freezeup_Tw,mask_freezeup = record_event_tf(istart,time,year,Twater_in,freezeup_date,mask_freezeup,date_ref)
#             else:
#                 if (mask_threshold[im]) & (istart > 0):
#                     sum_m += 1
#                     if (sum_m >= ndays):
#                         freezeup_date,freezeup_Tw,mask_freezeup = record_event_tf(istart,time,year,Twater_in,freezeup_date,mask_freezeup,date_ref)










#%%
# # BELOW IS JUST A SPACE TO TEST CUSTOM LOSS IMPLEMENTATION
# mse = tf.keras.losses.MeanSquaredError()
# # mse2 = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)

# # [batchsize,pred_len,1]
# y_pred = np.expand_dims(y_pred_test[0:512,:],axis=-1)
# y_true = np.expand_dims(y_test[0:512,:],axis=-1)

# # penalize_FN = True
# penalize_FN = False

# # use_exp_decay_loss = True
# use_exp_decay_loss = False

# if penalize_FN:
#     # Check for each sample in targets and predictions if Tw reaches below the freeze-up threshold
#     obs_FU_batch = tf.cast(tf.reduce_any(tf.math.less(y_true,0.75),axis=1),y_true.dtype)
#     pred_FU_batch = tf.cast(tf.reduce_any(tf.math.less(y_pred,0.75),axis=1),y_true.dtype)

#     # One weight per sample, depending if sample is a false positive,
#     # false negative, true positive or true negative for detected freeze-up
#     w_batch = tf.constant(1,y_true.dtype,shape=obs_FU_batch.shape) + 2*tf.math.squared_difference(obs_FU_batch,pred_FU_batch) + tf.math.subtract(obs_FU_batch,pred_FU_batch)
#     w_batch = tf.squeeze(w_batch)

#     if use_exp_decay_loss:
#         # Forcing an exponential weight decay per time step in
#         # forecast horizon, i.e. put more weight on the error
#         # of the first forecast steps.
#         obs_FU_all_steps = tf.cast(tf.reduce_any(tf.math.less(y_true,0.75),axis=-1),y_true.dtype)
#         pred_FU_all_steps = tf.cast(tf.reduce_any(tf.math.less(y_pred,0.75),axis=-1),y_true.dtype)

#         wtmp = tf.cast(tf.fill(tf.shape(obs_FU_all_steps),1.0),y_true.dtype)
#         exp_tmp = 1/tf.exp(tf.range(1,pred_len+1,dtype=y_true.dtype))
#         exp_w = tf.multiply(wtmp,exp_tmp)

#         w = tf.multiply(exp_w,tf.expand_dims(w_batch,axis=-1))
#     else:
#         w = w_batch

# else:
#     if use_exp_decay_loss:
#         # Forcing an exponential weight decay per time step in
#         # forecast horizon, i.e. put more weight on the error
#         # of the first forecast steps.
#         obs_FU_all_steps = tf.cast(tf.reduce_any(tf.math.less(y_true,0.75),axis=-1),y_true.dtype)
#         pred_FU_all_steps = tf.cast(tf.reduce_any(tf.math.less(y_pred,0.75),axis=-1),y_true.dtype)

#         wtmp = tf.cast(tf.fill(tf.shape(tf.squeeze(y_true)),1.0),y_true.dtype)
#         exp_tmp = 1/tf.exp(tf.range(1,pred_len+1,dtype=y_true.dtype))
#         exp_w = tf.multiply(wtmp,exp_tmp)

#         w = exp_w
#     else:
#         w = 1

# out = mse(y_true,y_pred,sample_weight=w)


# exp_tmp = 1/tf.exp(tf.range(1,pred_len+1,dtype=y_true.dtype))
# tmp = tf.reduce_sum(tf.math.log(tf.reduce_mean(tf.math.square(y_true-y_pred),axis=0)),axis=0)

# tmp = tf.math.log(tf.reduce_mean(tf.math.square(y_true-y_pred),axis=0))
