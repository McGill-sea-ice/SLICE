# -*- coding: utf-8 -*-
"""EncoderDecoder_LSTM_TFDatasets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Dc3S0Zp_3ZazEEGBBPmOuVy48HqPPsqp
"""
#%%
# local_path = '/storage/amelie/'
local_path = '/Volumes/SeagateUSB/McGill/Postdoc/'

plot_target = False

#%%
import sys
import os
FCT_DIR = os.path.dirname(os.path.abspath(local_path +'slice/prog/'+'/prog/'))
if not FCT_DIR in sys.path:
    sys.path.append(FCT_DIR)

import tensorflow as tf
import tensorflow.keras.backend as K
from tensorflow import keras
from tensorflow.keras.utils import plot_model
from keras.layers import Lambda

import numpy as np
import pandas as pd
import datetime as dt

from matplotlib import pyplot as plt
from matplotlib import dates as mdates
from functions import rolling_climo
from functions_ML import regression_metrics, plot_sample, plot_prediction_timeseries

#%%
print("Num CPUs Available: ", len(tf.config.list_physical_devices('CPU')))
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

# To see on which device the operation are done, uncomment below:
# tf.debugging.set_log_device_placement(True)

# Set random seed:
tf.keras.utils.set_random_seed(55)

#%%
"""# Loading the dataset
The dataset is composed of the following variables:

*   Dates (in days since 1900-01-01)
*   Daily water temperature (in Â°C - from the Longueuil water filtration plant)
*   ERA5 weather daily variables (*see belo*w)
*   Daily discharge (in m$^3$/s) and level (in m) for the St-Lawrence River (at Lasalle and Pointe-Claire, respectively)
*   Daily level (in m) for the Ottawa River (at Ste-Anne-de-Bellevue)
*   Daily time series of climate indices (*see below*)
*   Daily time series of monthly and seasonal forecasts from CanSIPS (*see below*)

"""

# The dataset has been copied into an Excel spreadsheet for this example
usb_path = '/Volumes/SeagateUSB/McGill/Postdoc/'
# usb_path = '/storage/amelie/'
filepath = 'slice/data/colab/'
df = pd.read_excel(usb_path+filepath+'predictor_data_daily_timeseries.xlsx')

# The dates column is not needed
time = df['Days since 1900-01-01'].values
df.drop(columns='Days since 1900-01-01', inplace=True)

# Keep only data for 1992-2020.
yr_start = 1992
yr_end = 2020
date_ref = dt.date(1900,1,1)
it_start = np.where(time == (dt.date(yr_start,1,1)-date_ref).days)[0][0]
it_end = np.where(time == (dt.date(yr_end+1,1,1)-date_ref).days)[0][0]

df = df[df.columns][it_start:it_end]
time = time[it_start:it_end]

# There is a missing data in the water temperature time series...
# This gap occurs in winter 2018, so we will fill it with zeros.
df['Avg. Twater'][9886:9946] = 0

# We also cap all negative water temperature to zero degrees.
df['Avg. Twater'][df['Avg. Twater'] < 0] = 0

# And replace nan values in FDD and TDD by zeros
df['Tot. FDD'][np.isnan(df['Tot. FDD'])] = 0
df['Tot. TDD'][np.isnan(df['Tot. TDD'])] = 0

# Other nan values will be replaced by the training climatology below...

# Create the time vector for plotting purposes:
first_day = (date_ref+dt.timedelta(days=int(time[0]))).toordinal()
last_day = (date_ref+dt.timedelta(days=int(time[-1]))).toordinal()
time_plot = np.arange(first_day, last_day + 1)

if plot_target:
    # Plot the observed water temperature time series, which will be the target variable
    fig, ax = plt.subplots(figsize=[12, 6])
    ax.plot(time_plot[:], df['Avg. Twater'][:], color='C0', label='T$_w$')
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))
    ax.set_xlabel('Time')
    ax.set_ylabel('Water temperature $[^{\circ}C]$')
    ax.legend(loc='best')
    ax.grid(True)

#%%
"""# Splitting the dataset + Data scaling

The training will be done on the 1992-2010 period, validated on the 2011-2015 period and tested on the 2016-2021 period.

The training dataset will be used to compute the water temprature climatology.

Then, the data will be normalized between 0 and 1 using a MinMaxScaler to help the learning process.
"""

train_yr_start = 1992 # Training dataset: 1992 - 2010
valid_yr_start = 2011 # Validation dataset: 2011 - 2015
test_yr_start = 2016 # Testing dataset: 2016 - 2021

istart_train = np.where(time_plot == dt.date(train_yr_start, 1, 1).toordinal())[0][0]
istart_valid = np.where(time_plot == dt.date(valid_yr_start, 1, 1).toordinal())[0][0]
istart_test = np.where(time_plot == dt.date(test_yr_start, 1, 1).toordinal())[0][0]

# THIS COULD BE MOVED FURTHER DOWN AFTER ONLY THE DESIRED PREDICTORS ARE KEPT IN THE DATAFRAME
# Compute daily rolling predictor climatology using only training values and replace nan values with climatological values
nw = 1
train_years = np.arange(train_yr_start,valid_yr_start)
df_clim_mean = df.copy()

for p in df.columns:
  p_clim_mean, p_clim_std, _ = rolling_climo(nw, df[p],'all_time',time,train_years)
  df_clim_mean[p] = p_clim_mean
  df[p].iloc[np.where(np.isnan(df[p]))] = df_clim_mean[p].iloc[np.where(np.isnan(df[p]))]

  # If there are still NaNs in the climatology because of missing data, we will just put a zero.
  # !!! THIS IS A QUICK FIX AND COULD BE IMPROVED !!! But it only happens for certain climate indices.
  # !!! CHECK THIS FOR MASKING THE NANS DURING MODEL TRAINING/TESTING: https://keras.io/api/layers/core_layers/masking/
  df[p].iloc[np.where(np.isnan(df[p]))] = 0

# Check if there are remaining nan values.
print(np.any(np.sum(np.isnan(df[df.columns[:]])) > 0 ))
# print(np.sum(np.isnan(df[df.columns[0:19]])) )
# print(np.sum(np.isnan(df[df.columns[19:33]])) )
# print(np.sum(np.isnan(df[df.columns[33:]])) )

#%%
# Split dataset into training, validation, and test sets
df_train = df[istart_train:istart_valid]
df_valid = df[istart_valid:istart_test]
df_test = df[istart_test:]

df_clim_train = df_clim_mean[istart_train:istart_valid]
df_clim_valid = df_clim_mean[istart_valid:istart_test]
df_clim_test = df_clim_mean[istart_test:]

time_train = time_plot[istart_train:istart_valid]
time_valid = time_plot[istart_valid:istart_test]
time_test = time_plot[istart_test:]

#%%
"""# Preparing the LSTM samples
The LSTM that will be used will be fed the **last 120 days** of predicotr time series (e.g.  minimum and maximum air temperatures, water temperature itself, precipitation, etc.) to **predict the next 30 days** of water temperatures.


All series of 120 days of input variables will be extracted using a rolling window of 1-day.
In this example, a very simple loop is used to generate the dataset. This may be optimized in the future.

**This can take a few minutes.**

Note: the LSTM inputs (X) are three-dimensional, with the following dimensions:

1.   Samples. One sequence is one sample. A batch is comprised of one or more samples, typically a multiplicator of 8 or 16 (e.g. 32, 512, 1024).
2.   Time Steps. One time step is one point of observation in the sample.
3.   Features. One feature is one observation at a time step.
"""

# Prediction window length, in days
pred_len = 60

# Input window length, in days
input_len = 128

# Select variables to use:
predictor_vars = ['Avg. Ta_max',
                  'Avg. Ta_min',
                  'Tot. snowfall',
                  'NAO',
                  'Avg. Twater'
                  ]
# forecast_vars = ['Avg. Ta_mean',
#                 'Tot. snowfall',
#                 ]
forecast_vars = []
target_var = ['Avg. Twater']

# First, select the predictors and normalize them using the training values

# The data frames contain (in order):
# [the target variable] x 1 column
# [the past predictors] x n_predictors columns
# [the forecasts] x n_forecasts columns
df_train = df_train[target_var+predictor_vars+forecast_vars]
df_valid = df_valid[target_var+predictor_vars+forecast_vars]
df_test = df_test[target_var+predictor_vars+forecast_vars]

df_clim_train = df_clim_train[target_var+predictor_vars+forecast_vars]
df_clim_valid = df_clim_valid[target_var+predictor_vars+forecast_vars]
df_clim_test = df_clim_test[target_var+predictor_vars+forecast_vars]

# Normalize all predictors, forecasts, and targets using only the training data
def fit_scaler(df_train_in,norm_type='MinMax'):
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.preprocessing import StandardScaler

    if norm_type =='MinMax': scaler = MinMaxScaler()
    if norm_type =='Standard': scaler = StandardScaler()

    return scaler.fit(df_train_in)

def normalize_df(df_in,scaler):
    df = df_in.copy()
    scaled_values = scaler.transform(df_in.values)

    for i in range(len(df.columns)):
        df[df.columns[i]] = scaled_values[:,i]

    return df

# scaler = fit_scaler(df_train,norm_type='Standard')
scaler = fit_scaler(df_train,norm_type='MinMax')
df_train_scaled = normalize_df(df_train,scaler)
df_valid_scaled = normalize_df(df_valid,scaler)
df_test_scaled = normalize_df(df_test,scaler)



def create_dataset(df, df_clim, time_in,
                   n_forecasts,
                   window_size, forecast_size,
                   batch_size):
    """
    SEE WEB EXAMPLE:
    (https://www.angioi.com/time-series-encoder-decoder-tensorflow/)
    """

    # Total size of window is given by the number of steps to be considered
    # before prediction time + steps that we want to forecast
    total_size = window_size + forecast_size

    data = tf.data.Dataset.from_tensor_slices(df.values)
    data_clim = tf.data.Dataset.from_tensor_slices(df_clim.values)
    time = tf.data.Dataset.from_tensor_slices(time_in)

    # Selecting windows
    data = data.window(total_size, shift=1, drop_remainder=True)
    data = data.flat_map(lambda k: k.batch(total_size))

    data_clim = data_clim.window(total_size, shift=1, drop_remainder=True)
    data_clim = data_clim.flat_map(lambda k: k.batch(total_size))

    time = time.window(total_size, shift=1, drop_remainder=True)
    time = time.flat_map(lambda k: k.batch(total_size))

    # Shuffling data
    # !!!!! NOT SURE HOW TO DEAL WITH SHUFFLE AND KEEP TIME FOLLOWING THE SHUFFLED SAMPLES...
    # so we keep shuffle to False for now...
    shuffle = False
    if shuffle:
        shuffle_buffer_size = len(df) # This number can be changed
        data = data.shuffle(shuffle_buffer_size, seed=42)

    # Extracting (past features, forecasts, decoder initial recurrent input) + targets
    # NOTE : the initial decoder input is set as the last value of the target.
    if n_forecasts > 0:
        data = data.map(lambda k: ((k[:-forecast_size,1:-n_forecasts], # Past predictors samples
                                    k[-forecast_size:, -n_forecasts:], # Future forecasts samples
                                    k[-forecast_size-1:-forecast_size,0:1] # Decoder input: last time step of target before prediction time starts
                                    ),
                                  k[-forecast_size:, 0:1])) # Target samples during prediction time
    else:
        data = data.map(lambda k: ((k[:-forecast_size,1:],  # Past predictors samples
                                    k[-forecast_size-1:-forecast_size,0:1] # Decoder input: last time step of target before prediction time starts
                                    ),
                                  k[-forecast_size:, 0:1])) # Target samples during prediction time

    time = time.map(lambda k: (k[:-forecast_size], # Time for past predictors samples
                            k[-forecast_size:]))    # Time for prediction samples

    data_clim = data_clim.map(lambda k: (k[:-forecast_size,1:-n_forecasts], # Past predictor climatology samples
                                      k[-forecast_size:,0:1])) # Target climatology samples during prediction time


    return data.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE), data_clim.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE), time.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)

#%%
# SET MODEL AND TRAINING HYPER-PARAMETERS:

# Choose batch size
# batch_size = 32
# batch_size = 64
# batch_size = 128
# batch_size = 256
batch_size = 512
# batch_size = 1024
# batch_size = 2048
# batch_size = 4096
# batch_size = 8192

# OPTIMIZER:
# Choose learning rate
# lr = 0.001
# lr = 0.002
# lr = 0.004
# lr = 0.008
lr = 0.016
# lr = 0.032
# lr = 0.064
# lr = 0.128
# lr = 0.256

optimizer = keras.optimizers.Adam(learning_rate=lr)
# momentum = 0.9
# optimizer = keras.optimizers.SGD(learning_rate=lr,momentum=momentum)


# lr = 0.1
# optimizer = keras.optimizers.Adadelta(learning_rate=lr)


# Choose loss function:
loss = tf.keras.losses.MeanSquaredError()
# loss = tf.keras.losses.Huber()
# loss = tf.keras.losses.MeanAbsoluteError()


# Set max. number of epochs
n_epochs = 200

# Number of hidden neurons in Encoder and Decoder
latent_dim = 20

# Choose Dense layer activation function:
dense_act_func = 'sigmoid'
# dense_act_func = 'tanh'
# dense_act_func = None

# Choose Dropout rate:
inp_dropout = 0
rec_dropout = 0

#%%
# GET WINDOWED DATA SETS
# Now we get training, validation, and test as tf.data.Dataset objects
train_windowed, train_clim_windowed, time_train_windowed = create_dataset(df_train_scaled, df_clim_train, time_train,
                                                                          len(forecast_vars),
                                                                          input_len, pred_len,
                                                                          batch_size)

valid_windowed, valid_clim_windowed, time_valid_windowed  = create_dataset(df_valid_scaled, df_clim_valid, time_valid,
                                                                           len(forecast_vars),
                                                                           input_len, pred_len,
                                                                           batch_size)

test_windowed, test_clim_windowed, time_test_windowed  = create_dataset(df_test_scaled, df_clim_test, time_test,
                                                                        len(forecast_vars),
                                                                        input_len, pred_len,
                                                                        batch_size=1)
#%%
# BUILD MODEL ARCHITECTURE

# ENCODER:
past_inputs = keras.layers.Input(shape=(input_len, len(predictor_vars)), name='past_inputs')
encoder = keras.layers.LSTM(latent_dim, return_state=True,
                            dropout = inp_dropout,
                            recurrent_dropout = rec_dropout, name='encoder')
encoder_outputs, encoder_state_h, encoder_state_c = encoder(past_inputs)
# Discard encoder outputs and only keep the cell states and hidden states.
encoder_states = [encoder_state_h, encoder_state_c]


# DECODER: Process only one step at a time, reinjecting the output at step t as input to step t+1
decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True,
                                 return_state=True,
                                 dropout = inp_dropout,
                                 recurrent_dropout = rec_dropout,
                                 input_shape=[None,1,len(forecast_vars)+1], name='recursive_decoder')

# The output of the dense layer is fixed at one to return only the predicted water temperature.
# The sigmoid activation function ensures that the predicted values are positive and scaled between 0 and 1.
decoder_dense = keras.layers.Dense(1,activation=dense_act_func,name='Dense')

# Set the initial state of the decoder to be the ouput state of the encoder
states = encoder_states

# Initalize the recursive and forecast inputs
first_recursive_input = keras.layers.Input(shape=(1,1), name='first_recursive_input')

if len(forecast_vars) > 0 :
    it = -1
    future_inputs = keras.layers.Input(shape=(pred_len, len(forecast_vars)), name='future_inputs')
    first_future_input = tf.keras.layers.Cropping1D(cropping=(0,pred_len-1), name='future_input_'+str(it+1))(future_inputs)
    inputs = tf.keras.layers.Concatenate(name='concat_input_'+str(it+1))([first_future_input, first_recursive_input])
else:
    inputs = first_recursive_input

all_outputs = []

for it in range(pred_len):
    # Run the decoder on one timestep
    outputs, state_h, state_c = decoder_lstm(inputs,initial_state=states)
    outputs = decoder_dense(outputs)

    # Store the current prediction (we will concatenate all predictions later)
    all_outputs.append(outputs)

    # Reinject the outputs as inputs for the next loop iteration and concatenate
    # with the forecast for the next time step
    # + update the states
    if len(forecast_vars) > 0 :
        if it < pred_len - 1:
          inputs = tf.keras.layers.Concatenate(name='concat_input_'+str(it+1))([tf.keras.layers.Cropping1D(cropping=(it+1,pred_len-(it+2)),name='future_input_'+str(it+1))(future_inputs), outputs])
    else:
        inputs = outputs

    states = [state_h, state_c]

# Concatenate all predictions
decoder_outputs = Lambda(lambda x: K.concatenate(x, axis=1), name='concat_outputs')(all_outputs)

#%%
# DEFINE MODEL:
if len(forecast_vars) > 0 :
    model = tf.keras.models.Model(inputs=[past_inputs, future_inputs, first_recursive_input], outputs=decoder_outputs)
else:
    model = tf.keras.models.Model(inputs=[past_inputs, first_recursive_input], outputs=decoder_outputs)

# COMPILE MODEL:
model.compile(optimizer=optimizer, loss=loss,
                      metrics=["mae"]
                  )
# if tf.test.is_built_with_cuda():
#     with tf.device('/GPU:0'):
#         model.compile(optimizer=optimizer, loss=loss,
#                       metrics=["mae"]
#                   )
# else:
#     model.compile(optimizer=optimizer, loss=loss,
#                   metrics=["mae"]
#                   )

# DEFINE CALLBACKS FOR TRAINING:
early_stop = tf.keras.callbacks.EarlyStopping(
                                            monitor="val_loss",
                                            # patience=5,
                                            patience=8,
                                            min_delta=0,
                                            verbose=1
                                        )
lr_plateau =  tf.keras.callbacks.ReduceLROnPlateau(
                                                    monitor="val_loss",
                                                    factor=0.5,
                                                    patience=5,
                                                    verbose=1,
                                                    min_delta=0.0001
                                                )

# FIT/TRAIN MODEL:
# h = model.fit(train_windowed,
#                     epochs = n_epochs,
#                     validation_data = valid_windowed,
#                     verbose = 2,
#                     callbacks = [lr_plateau,early_stop])

if tf.test.is_built_with_cuda():
    with tf.device('/GPU:0'):
        h = model.fit(train_windowed,
                      epochs = n_epochs,
                      validation_data = valid_windowed,
                      verbose = 2,
                      callbacks = [lr_plateau,early_stop])
else:
    with tf.device('/CPU:0'):
        h = model.fit(train_windowed,
                      epochs = n_epochs,
                      validation_data = valid_windowed,
                      verbose = 2,
                      callbacks = [lr_plateau,early_stop])


"""## Results of the model training
Compare the training and validation loss to diagnose overfitting
"""

# Plot the training and validation loss (MSE)
fig, ax = plt.subplots(figsize=[12, 6])
ax.plot(h.history['loss'], 'o-')
ax.plot(h.history['val_loss'], 'o-')
plt.grid(True)
plt.legend(['Loss', 'Val loss'])
plt.xlabel('Number of epochs')
plt.ylabel('MSE')

# Show the LSTM model structure
# plot_model(
#     model,
#     to_file='model_plot.png',
#     # show_shapes=True,
#     show_layer_names=True
#     )

# Show the number of weights of the LSTM
# model.summary()


# Get predictions for all examples
y_pred_train_scaled = model.predict(train_windowed)
y_pred_valid_scaled = model.predict(valid_windowed)
y_pred_test_scaled  = model.predict(test_windowed)

# Get targets for all examples
y_train_scaled = np.concatenate([y for x, y in train_windowed], axis=0)
y_valid_scaled = np.concatenate([y for x, y in valid_windowed], axis=0)
y_test_scaled = np.concatenate([y for x, y in test_windowed], axis=0)

# Get predictors and forecasts as well because they will be needed to reverse the scaler
X_train = np.concatenate([x[0] for x, y in train_windowed], axis=0)
X_valid = np.concatenate([x[0] for x, y in valid_windowed], axis=0)
X_test = np.concatenate([x[0] for x, y in test_windowed], axis=0)

if len(forecast_vars) > 0:
    F_train = np.concatenate([x[1] for x, y in train_windowed], axis=0)
    F_valid = np.concatenate([x[1] for x, y in valid_windowed], axis=0)
    F_test = np.concatenate([x[1] for x, y in test_windowed], axis=0)

# All data must be retransformed back using the MinMaxScaler
y_pred_train = np.zeros(y_pred_train_scaled.shape)
y_pred_valid = np.zeros(y_pred_valid_scaled.shape)
y_pred_test = np.zeros(y_pred_test_scaled.shape)

y_train = np.zeros(y_train_scaled.shape)
y_valid = np.zeros(y_valid_scaled.shape)
y_test = np.zeros(y_test_scaled.shape)

for i in range(pred_len):
    if len(forecast_vars) > 0:
        y_pred_train[:,i,0] = scaler.inverse_transform(np.concatenate((y_pred_train_scaled[:,i,:], X_train[:,0,:], F_train[:,0,:]), axis=1))[:,0]
        y_pred_valid[:,i,0] = scaler.inverse_transform(np.concatenate((y_pred_valid_scaled[:,i,:], X_valid[:,0,:], F_valid[:,0,:]), axis=1))[:,0]
        y_pred_test[:,i,0] = scaler.inverse_transform(np.concatenate((y_pred_test_scaled[:,i,:], X_test[:,0,:], F_test[:,0,:]), axis=1))[:,0]

        y_train[:,i,0] = scaler.inverse_transform(np.concatenate((y_train_scaled[:,i,:], X_train[:,0,:], F_train[:,0,:]), axis=1))[:,0]
        y_valid[:,i,0] = scaler.inverse_transform(np.concatenate((y_valid_scaled[:,i,:], X_valid[:,0,:], F_valid[:,0,:]), axis=1))[:,0]
        y_test[:,i,0] = scaler.inverse_transform(np.concatenate((y_test_scaled[:,i,:], X_test[:,0,:], F_test[:,0,:]), axis=1))[:,0]
    else:
        y_pred_train[:,i,0] = scaler.inverse_transform(np.concatenate((y_pred_train_scaled[:,i,:], X_train[:,0,:]), axis=1))[:,0]
        y_pred_valid[:,i,0] = scaler.inverse_transform(np.concatenate((y_pred_valid_scaled[:,i,:], X_valid[:,0,:]), axis=1))[:,0]
        y_pred_test[:,i,0] = scaler.inverse_transform(np.concatenate((y_pred_test_scaled[:,i,:], X_test[:,0,:]), axis=1))[:,0]

        y_train[:,i,0] = scaler.inverse_transform(np.concatenate((y_train_scaled[:,i,:], X_train[:,0,:]), axis=1))[:,0]
        y_valid[:,i,0] = scaler.inverse_transform(np.concatenate((y_valid_scaled[:,i,:], X_valid[:,0,:]), axis=1))[:,0]
        y_test[:,i,0] = scaler.inverse_transform(np.concatenate((y_test_scaled[:,i,:], X_test[:,0,:]), axis=1))[:,0]



"""## Evaluate the model performance
The model performance is evaluated using typical regression metrics (i.e. MAE, RMSE, R$^2$)

"""
# Get the regression metrics:
y_train = np.squeeze(y_train)
y_valid = np.squeeze(y_valid)
y_test = np.squeeze(y_test)

y_pred_train = np.squeeze(y_pred_train)
y_pred_valid = np.squeeze(y_pred_valid)
y_pred_test = np.squeeze(y_pred_test)

rsqr_train, mae_train, rmse_train =  regression_metrics(y_train,y_pred_train)
rsqr_valid, mae_valid, rmse_valid =  regression_metrics(y_valid,y_pred_valid)
rsqr_test, mae_test, rmse_test =  regression_metrics(y_test,y_pred_test)

print('TRAINING ---')
print('Rsqr = '+ str(np.round(rsqr_train, 2)))
print('MAE = '+ str(np.round(mae_train, 2)))
print('RMSE = '+ str(np.round(rmse_train, 2)))
print(' ')
print('VALIDATION ---')
print('Rsqr = '+ str(np.round(rsqr_valid, 2)))
print('MAE = '+ str(np.round(mae_valid, 2)))
print('RMSE = '+ str(np.round(rmse_valid, 2)))
print(' ')
print('TEST ---')
print('Rsqr = '+ str(np.round(rsqr_test, 2)))
print('MAE = '+ str(np.round(mae_test, 2)))
print('RMSE = '+ str(np.round(rmse_test, 2)))
print(' ')
print('============================================')
print(' ')
print('TRAINING ---')
print('Rsqr = '+ str(np.round(np.mean(rsqr_train), 4)))
print('MAE = '+ str(np.round(np.mean(mae_train), 4)))
print('RMSE = '+ str(np.round(np.mean(rmse_train), 4)))
print(' ')
print('VALIDATION ---')
print('Rsqr = '+ str(np.round(np.mean(rsqr_valid), 4)))
print('MAE = '+ str(np.round(np.mean(mae_valid), 4)))
print('RMSE = '+ str(np.round(np.mean(rmse_valid), 4)))
print(' ')
print('TEST ---')
print('Rsqr = '+ str(np.round(np.mean(rsqr_test), 4)))
print('MAE = '+ str(np.round(np.mean(mae_test), 4)))
print('RMSE = '+ str(np.round(np.mean(rmse_test), 4)))



# Plot predictions - TRAINING
target_time_train = np.concatenate([y for x, y in time_train_windowed], axis=0)

plot_prediction_timeseries(y_pred_train,y_train,target_time_train, pred_type = 'training', lead=0, nyrs_plot= 2)
plot_prediction_timeseries(y_pred_train,y_train,target_time_train, pred_type = 'training', lead=50, nyrs_plot= 2)
# plot_prediction_timeseries(y_pred_train,y_train,target_time_train, pred_type = 'training', lead=5, nyrs_plot= 2)

plot_sample(y_pred_train,y_train,target_time_train,it=217, pred_type = 'training')
plot_sample(y_pred_train,y_train,target_time_train,it=227, pred_type = 'training')


# Plot predictions - TESTING
target_time_test = np.concatenate([y for x, y in time_test_windowed], axis=0)

plot_prediction_timeseries(y_pred_test,y_test,target_time_test,pred_type='testing', lead=0, nyrs_plot= 2)
# plot_prediction_timeseries(y_pred_test,y_test,target_time_test,pred_type='testing', lead=50, nyrs_plot= 2)
# plot_prediction_timeseries(y_pred_test,y_test,target_time_test,pred_type='testing', lead=5, nyrs_plot= 2)

# plot_sample(y_pred_test,y_test,target_time_test,it=217,pred_type='testing')
# plot_sample(y_pred_test,y_test,target_time_test,it=227,pred_type='testing')
