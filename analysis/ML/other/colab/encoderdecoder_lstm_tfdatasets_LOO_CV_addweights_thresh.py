# -*- coding: utf-8 -*-
"""EncoderDecoder_LSTM_TFDatasets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Dc3S0Zp_3ZazEEGBBPmOuVy48HqPPsqp
"""
#%%
local_path = '/storage/amelie/'
# local_path = '/Volumes/SeagateUSB/McGill/Postdoc/'

use_GPU = False

#%%
import tensorflow as tf

print("Num CPUs Available: ", len(tf.config.list_physical_devices('CPU')))
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

if use_GPU:
    if len(tf.config.list_physical_devices('GPU')) > 0:
        device_name = '/GPU:0'
    else:
        print('No GPU available... Defaulting to using CPU.')
        device_name = '/CPU:0'
else:
    device_name = '/CPU:0'

#%%
import sys
import os
FCT_DIR = os.path.dirname(os.path.abspath(local_path +'slice/prog/'+'/prog/'))
if not FCT_DIR in sys.path:
    sys.path.append(FCT_DIR)

import tensorflow.keras.backend as K
from tensorflow import keras
from tensorflow.keras.utils import plot_model
from keras.layers import Lambda
import sklearn.metrics as metrics

import numpy as np
import pandas as pd
import datetime as dt
import time as timer

from matplotlib import pyplot as plt
from matplotlib import dates as mdates
import cmocean

from functions import rolling_climo
from functions_ML import regression_metrics, plot_sample, plot_prediction_timeseries

#%%
# To see on which device the operation are done, uncomment below:
# tf.debugging.set_log_device_placement(True)

# Set random seed:
fixed_seed = True
fixed_seed = False
seed = 422
if fixed_seed:
    tf.keras.utils.set_random_seed(seed)

#%%

from functions_encoderdecoder import fit_scaler, normalize_df, get_predictor_clim, replace_nan_with_clim, create_dataset, reconstruct_ysamples, encoder_decoder_recursive, execute_fold

#%%
# CHOOSE VALIDATION SCHEME:

start_time = timer.time()

# If valid_scheme == 'LOOk', need to specify how many folds to use for validation:
valid_scheme = 'LOOk'
nfolds = 5

# If valid_scheme == 'standard', need to specify bounds for train/valid/test sets:
# valid_scheme = 'standard'
train_yr_start = 1992 # Training dataset: 1992 - 2010
valid_yr_start = 2011 # Validation dataset: 2011 - 2015
test_yr_start = 2016 # Testing dataset: 2016 - 2021



# SET MODEL AND TRAINING HYPER-PARAMETERS:

# Choose batch size:
# batch_size = 32
# batch_size = 64
# batch_size = 128
# batch_size = 256
batch_size = 512 # <--- This is the max size for running on bjerknes' GPU
# batch_size = 1024
# batch_size = 2048
# batch_size = 4096
# batch_size = 8192

# Choose learning rate: (Note: Optimizer is Adam)
optimizer_name = 'Adam'
# lr = 0.001
# lr = 0.002
# lr = 0.004
# lr = 0.008
lr = 0.016
# lr = 0.032
# lr = 0.064
# lr = 0.128
# lr = 0.256


# Choose loss function:
# loss = tf.keras.losses.MeanSquaredError()
# loss_name = 'MSETw'
loss = 'custom_loss'
# loss_name = 'MSETw_with_weights1_on_thresh0_75'
loss_name = 'MSETw_MSEdTwdt_with_weights1_on_thresh0_75'

# Set max. number of epochs
n_epochs = 50
# n_epochs = 100

# Number of hidden neurons in Encoder and Decoder
latent_dim = 25
# latent_dim = 50
nb_layers = 1

# Choose Dense layer activation function and data normalization type:
# dense_act_func = 'sigmoid'
# norm_type='MinMax'

dense_act_func = None
norm_type='Standard'


# Choose Dropout rate:
inp_dropout = 0
rec_dropout = 0

# Prediction window length, in days
pred_len = 60

# Input window length, in days
input_len = 128

# Select variables to use:
predictor_vars = ['Avg. Ta_max',
                  'Avg. Ta_min',
                  'Tot. snowfall',
                  'NAO',
                  'Avg. Twater'
                  ]
# predictor_vars = ['Avg. Ta_max']

forecast_vars = ['Avg. Ta_mean',
                # 'Tot. snowfall',
#                 'Avg. cloud cover',
#                 'Avg. SLP'
                ]
# forecast_vars = []

target_var = ['Avg. Twater']

# Choose if using anomaly timeseries:
# anomaly_target = True
# anomaly_past = True
# anomaly_frcst = True
anomaly_target = False
anomaly_past = False
anomaly_frcst = False

#%% RUN MODEL
with tf.device(device_name):

    # LOADING THE DATA AND PRE-PROCESSING
    """
    The dataset is composed of the following variables:

    *   Dates (in days since 1900-01-01)
    *   Daily water temperature (in Â°C - from the Longueuil water filtration plant)
    *   ERA5 weather daily variables (*see belo*w)
    *   Daily discharge (in m$^3$/s) and level (in m) for the St-Lawrence River (at Lasalle and Pointe-Claire, respectively)
    *   Daily level (in m) for the Ottawa River (at Ste-Anne-de-Bellevue)
    *   Daily time series of climate indices (*see below*)
    *   Daily time series of monthly and seasonal forecasts from CanSIPS (*see below*)

    """

    # The dataset has been copied into an Excel spreadsheet for this example
    filepath = 'slice/data/colab/'
    df = pd.read_excel(local_path+filepath+'predictor_data_daily_timeseries.xlsx')

    # The dates column is not needed
    time = df['Days since 1900-01-01'].values
    df.drop(columns='Days since 1900-01-01', inplace=True)

    # Keep only data for 1992-2020.
    yr_start = 1992
    yr_end = 2020
    date_ref = dt.date(1900,1,1)
    it_start = np.where(time == (dt.date(yr_start,1,1)-date_ref).days)[0][0]
    it_end = np.where(time == (dt.date(yr_end+1,1,1)-date_ref).days)[0][0]

    df = df.iloc[it_start:it_end,:]
    time = time[it_start:it_end]

    # There is a missing data in the water temperature time series...
    # This gap occurs during the winter (in 2018), so we will fill it with zeros.
    df['Avg. Twater'][9886:9946] = 0

    # We also cap all negative water temperature to zero degrees.
    df['Avg. Twater'][df['Avg. Twater'] < 0] = 0

    # And replace nan values in FDD and TDD by zeros
    df['Tot. FDD'][np.isnan(df['Tot. FDD'])] = 0
    df['Tot. TDD'][np.isnan(df['Tot. TDD'])] = 0

    # Other nan values due to missing values are replaced by the
    # training climatology below...

    # Create the time vector for plotting purposes:
    first_day = (date_ref+dt.timedelta(days=int(time[0]))).toordinal()
    last_day = (date_ref+dt.timedelta(days=int(time[-1]))).toordinal()
    time_plot = np.arange(first_day, last_day + 1)

    plot_target = False
    if plot_target:
        # Plot the observed water temperature time series, which will be the target variable
        fig, ax = plt.subplots(figsize=[12, 6])
        ax.plot(time_plot[:], df['Avg. Twater'][:], color='C0', label='T$_w$')
        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))
        ax.set_xlabel('Time')
        ax.set_ylabel('Water temperature $[^{\circ}C]$')
        ax.legend(loc='best')
        ax.grid(True)


    # PREDICTOR SELECTION
    """
    The data frame will contain (in order):
      [the target variable] x 1 column
      [the past predictors] x n_predictors columns
      [the forecasts] x n_forecasts columns
    """
    df = df[target_var+predictor_vars+forecast_vars]


    # SPLITTING THE DATASET + FILLING NANS + DATA SCALING + CREATE SAMPLES
    """
    The data is first separated in train-valid-test sets according to the
    chosen CV scheme.

    Then a daily climatology is computed for each predictor by using a moving
    average window and using only values from the training set.
    Nan values in the predictors and the target time series are replaced by
    their daily climatological values.

    The data is then normalized (MinMaxScaler or StandardScaler) to help
    the learning process. The scaler uses only the training dataset for
    calibration.

    Finally, the samples are prepared using a rolling window with
    tf.data.Dataset objects that are batched and pre-fetched.
    """

    #---------------------------
    if valid_scheme == 'LOOk':

        from sklearn.model_selection import KFold

        valid_years = np.nan
        test_years = np.arange(yr_start,yr_end)

        # y_test_all = np.zeros((len(test_years),365*28,pred_len))*np.nan
        # y_train_all = np.zeros((len(test_years),365*28,pred_len))*np.nan

        # y_pred_test_all = np.zeros((len(test_years),365*28,pred_len))*np.nan
        # y_pred_train_all = np.zeros((len(test_years),365*28,pred_len))*np.nan

        # y_clim_test_all = np.zeros((len(test_years),365*28,pred_len))*np.nan
        # y_clim_train_all = np.zeros((len(test_years),365*28,pred_len))*np.nan

        # target_time_test_all = np.zeros((len(test_years),365*28,pred_len))*np.nan
        # target_time_train_all = np.zeros((len(test_years),365*28,pred_len))*np.nan
        
        # history_test_all = np.zeros((len(test_years)),dtyp/e='object')


        kf = KFold(n_splits=nfolds)

        for iyr_test,yr_test in enumerate(test_years):
        # for iyr_test,yr_test in enumerate(test_years[10:12]):
            df_tmp = df.copy()
            istart_yr_test = np.where(time_plot == dt.date(yr_test, 4, 1).toordinal())[0][0]
            iend_yr_test = np.where(time_plot == dt.date(yr_test+1, 4, 1).toordinal())[0][0]

            is_test = np.zeros(df.shape[0], dtype=bool)
            is_test[istart_yr_test:iend_yr_test] = True

            # Mask test year from data set:
            df_tmp[is_test] = np.nan

            df_kfold = df[~is_test].copy()
            ind_kfold = df[~is_test].index

            df_test_fold = df[is_test].copy()
            ind_test_fold = df[is_test].index  # NOTE: THESE ARE THE SAME INDICES AS WHERE is_test IS TRUE
            test_dates = [dt.date.fromordinal(d) for d in time_plot[ind_test_fold]]
            time_test = time[ind_test_fold]
            time_test_plot = time_plot[ind_test_fold]

            y_valid_all = np.zeros((nfolds,365*28,pred_len))*np.nan
            y_pred_valid_all = np.zeros((nfolds,365*28,pred_len))*np.nan
            y_clim_valid_all = np.zeros((nfolds,365*28,pred_len))*np.nan
            target_time_valid_all = np.zeros((nfolds,365*28,pred_len))*np.nan
            history_valid_all = np.zeros((nfolds),dtype='object')

            # Get cross-validation folds (determined using all years other than test year)
            for ifold,[train_index, valid_index] in enumerate(kf.split(df_kfold.index)):
                # if (ifold == 2) | (ifold == 3): # REMOVE THIS AFTER!!!!!!!!!!!1
    
                # GET TRAINING AND VALIDATION DATA SETS FOR THIS FOLD:
                df_train_fold, df_valid_fold = df_kfold.iloc[train_index], df_kfold.iloc[valid_index]
                ind_train_fold, ind_valid_fold = ind_kfold[train_index], ind_kfold[valid_index]

                train_dates = [dt.date.fromordinal(d) for d in time_plot[ind_train_fold]]
                valid_dates = [dt.date.fromordinal(d) for d in time_plot[ind_valid_fold]]

                time_train = time[ind_train_fold]
                time_valid = time[ind_valid_fold]
                time_train_plot = time_plot[ind_train_fold]
                time_valid_plot = time_plot[ind_valid_fold]

                train_years = np.unique([dt.date.fromordinal(d).year for d in time_plot[ind_train_fold]])
                valid_years = np.unique([dt.date.fromordinal(d).year for d in time_plot[ind_valid_fold]])

                df_train = df_train_fold
                df_valid = df_valid_fold
                df_test = df_test_fold

                ind_train = ind_train_fold
                ind_valid = ind_valid_fold
                ind_test = ind_test_fold

                [model_out,history_valid_all[ifold],
                _,target_time_valid,_,
                _,y_pred_valid,_,
                _,y_valid,_,
                _,y_clim_valid,_] = execute_fold(df_train, df_valid, df_test,
                                                                        ind_train, ind_valid, ind_test,
                                                                        train_years,time,
                                                                        time_train,time_valid,time_test,
                                                                        time_train_plot,time_valid_plot,time_test_plot,
                                                                        latent_dim,inp_dropout,rec_dropout,dense_act_func,
                                                                        len(predictor_vars),len(forecast_vars),
                                                                        input_len,pred_len,
                                                                        norm_type,anomaly_target,anomaly_past,anomaly_frcst,
                                                                        batch_size,lr,loss,n_epochs,
                                                                        plot_loss=False,
                                                                        plot_predictions=False,
                                                                        plot_targets=False,
                                                                        show_modelgraph=False,
                                                                        show_weights=False)

                # SAVE VALIDATION PREDICTIONS AND METRICS
                # FOR ALL FOLDS OF THAT TEST YEAR.
                y_valid_all[ifold,0:y_valid.shape[0],:] = y_valid
                y_pred_valid_all[ifold,0:y_valid.shape[0],:] = y_pred_valid
                y_clim_valid_all[ifold,0:y_valid.shape[0],:] = y_clim_valid
                target_time_valid_all[ifold,0:y_valid.shape[0],:] = target_time_valid

                print('=====================================================')
                print(str(yr_test) + ' --- FOLD ' + str(ifold) + ' OF ' + str(nfolds)+ ' COMPLETE')
                print('=====================================================')
                del target_time_valid,y_pred_valid,y_valid,y_clim_valid,model_out
            
            # Remove other variables that are no longer needed
            del df_kfold, df_tmp, df_test_fold
            
            # Then, train with all other years to predict test year:
            df_train = df[~is_test].copy()
            df_valid = df[is_test].copy() # This is the same as test data, just as a placeholder
            df_test = df[is_test].copy()

            ind_train = df[~is_test].index
            ind_valid = df[is_test].index
            ind_test = df[is_test].index

            test_dates = [dt.date.fromordinal(d) for d in time_plot[ind_test]]

            time_train = time[ind_train]
            time_valid = time[ind_valid]
            time_test = time[ind_test]
            time_train_plot = time_plot[ind_train]
            time_valid_plot = time_plot[ind_valid]
            time_test_plot = time_plot[ind_test]

            train_years = np.unique([dt.date.fromordinal(d).year for d in time_plot[ind_train]])
            valid_years = np.unique([dt.date.fromordinal(d).year for d in time_plot[ind_valid]])

            [model_out_all,history_test_all,
            target_time_train,_,target_time_test,
            y_pred_train,_,y_pred_test,
            y_train,_,y_test,
            y_clim_train,_,y_clim_test] = execute_fold(df_train, df_valid, df_test,
                                                                    ind_train, ind_valid, ind_test,
                                                                    train_years,time,
                                                                    time_train,time_valid,time_test,
                                                                    time_train_plot,time_valid_plot,time_test_plot,
                                                                    latent_dim,inp_dropout,rec_dropout,dense_act_func,
                                                                    len(predictor_vars),len(forecast_vars),
                                                                    input_len,pred_len,
                                                                    norm_type,anomaly_target,anomaly_past,anomaly_frcst,
                                                                    batch_size,lr,loss,n_epochs,
                                                                    plot_loss=False,
                                                                    plot_predictions=False,
                                                                    plot_targets=False,
                                                                    show_modelgraph=False,
                                                                    show_weights=False)

            # SAVE TRAINING & TEST PREDICTIONS AND METRICS
            # FOR THAT TEST YEAR.
            # y_train_all[iyr_test,0:y_train.shape[0],:] = y_train
            # y_pred_train_all[iyr_test,0:y_pred_train.shape[0],:] = y_pred_train
            # y_clim_train_all[iyr_test,0:y_clim_train.shape[0],:] = y_clim_train
            # target_time_train_all[iyr_test,0:target_time_train.shape[0],:] = target_time_train

            # y_test_all[iyr_test,0:y_test.shape[0],:] = y_test
            # y_pred_test_all[iyr_test,0:y_pred_test.shape[0],:] = y_pred_test
            # y_clim_test_all[iyr_test,0:y_clim_test.shape[0],:] = y_clim_test
            # target_time_test_all[iyr_test,0:target_time_test.shape[0],:] = target_time_test
            
            
            save_model_outputs = True
            suffix = '_perfectexp'
            if save_model_outputs:
                if dense_act_func is None: dense_act_func_name = 'None'
                np.savez('./output/encoderdecoder_horizon'+str(pred_len)+'_context'+str(input_len)+'_nneurons'+str(latent_dim)+'_nepochs'+str(n_epochs)+'_'+norm_type+'_'+dense_act_func_name+'_'+loss_name+anomaly_target*'_anomaly_target'+suffix+'_'+str(yr_test),
                        target_time_train=target_time_train,
                        target_time_valid=target_time_valid_all,
                        target_time_test=target_time_test,
                        y_pred_train_all=y_pred_train,
                        y_pred_valid=y_pred_valid_all,
                        y_pred_test=y_pred_test,
                        y_train=y_train,
                        y_valid=y_valid_all,
                        y_test=y_test,
                        y_clim_train=y_clim_train,
                        y_clim_valid=y_clim_valid_all,
                        y_clim_test=y_clim_test,
                        predictor_vars = predictor_vars,
                        forecast_vars = forecast_vars,
                        target_var = target_var,
                        input_len = input_len,
                        pred_len = pred_len,
                        n_epochs = n_epochs,
                        date_ref = date_ref,
                        anomaly_target = anomaly_target,
                        anomaly_past =  anomaly_past,
                        anomaly_frcst = anomaly_frcst,
                        train_yr_start = train_yr_start,
                        valid_yr_start = valid_yr_start,
                        test_yr_start = test_yr_start,
                        valid_scheme = valid_scheme,
                        nfolds = nfolds,
                        norm_type = norm_type,
                        dense_act_func = dense_act_func,
                        latent_dim = latent_dim,
                        nb_layers = nb_layers,
                        batch_size = batch_size,
                        learning_rate = lr,
                        seed = seed,
                        fixed_seed = fixed_seed,
                        inp_dropout = inp_dropout,
                        rec_dropout = rec_dropout, 
                        test_history = history_test_all,
                        valid_history = history_valid_all,
                        optimizer_name = optimizer_name,
                        loss_name = loss_name,
                        )



            
            
            print('=====================================================')
            print(str(yr_test) + ' --- END')
            print('=====================================================')
            del df_train, df_test, df_valid
            del target_time_train,target_time_valid_all,target_time_test
            del y_pred_train,y_pred_valid_all,y_pred_test,
            del y_train,y_valid_all,y_test,
            del y_clim_train,y_clim_valid_all,y_clim_test
            del history_test_all, history_valid_all



    #-----------------------------
    if valid_scheme == 'standard':

        # GET TRAINING AND VALIDATION DATA SETS
        train_years = np.arange(train_yr_start,valid_yr_start)
        valid_years = np.arange(valid_yr_start,test_yr_start)
        test_years = np.arange(test_yr_start,yr_end+1)

        istart_train = np.where(time_plot == dt.date(train_yr_start, 4, 1).toordinal())[0][0]
        istart_valid = np.where(time_plot == dt.date(valid_yr_start, 4, 1).toordinal())[0][0]
        istart_test = np.where(time_plot == dt.date(test_yr_start, 4, 1).toordinal())[0][0]
        ind_train = np.arange(istart_train,istart_valid)
        ind_valid = np.arange(istart_valid,istart_test)
        ind_test = np.arange(istart_test,len(time))

        time_train = time[ind_train]
        time_valid = time[ind_valid]
        time_test = time[ind_test]
        time_train_plot = time_plot[ind_train]
        time_valid_plot = time_plot[ind_valid]
        time_test_plot = time_plot[ind_test]

        df_train = df.iloc[ind_train]
        df_valid = df.iloc[ind_valid]
        df_test = df.iloc[ind_test]


        [model_out,history_test_all,
        target_time_train_all,target_time_valid_all,target_time_test_all,
        y_pred_train_all,y_pred_valid_all,y_pred_test_all,
        y_train_all,y_valid_all,y_test_all,
        y_clim_train_all,y_clim_valid_all,y_clim_test_all] = execute_fold(df_train, df_valid, df_test,
                                                                                     ind_train, ind_valid, ind_test,
                                                                                     train_years,time,
                                                                                     time_train,time_valid,time_test,
                                                                                     time_train_plot,time_valid_plot,time_test_plot,
                                                                                     latent_dim,inp_dropout,rec_dropout,dense_act_func,
                                                                                     len(predictor_vars),len(forecast_vars),
                                                                                     input_len,pred_len,
                                                                                     norm_type,anomaly_target,anomaly_past,anomaly_frcst,
                                                                                     batch_size,lr,loss,n_epochs,
                                                                                     plot_loss=True,
                                                                                     plot_predictions=False,
                                                                                     plot_targets=False,
                                                                                     show_modelgraph=False,
                                                                                     show_weights=False)
        history_valid_all = history_test_all


end_time = timer.time()
print('=====================================================')
print('END!')
print('Total time: ' + str(end_time - start_time) + ' seconds')




#%% PLOT PREDICTION TIME SERIES
# if valid_scheme == 'standard':
#     # plot_prediction_timeseries(y_pred_test_all,y_test_all,y_clim_test_all,target_time_test_all,pred_type='testing', lead=0, nyrs_plot= 28)
#     # plot_prediction_timeseries(y_pred_train_all,y_train_all,y_clim_train_all,target_time_train_all,pred_type='training', lead=0, nyrs_plot= 28)
#     plot_prediction_timeseries(y_pred_valid_all,y_valid_all,y_clim_valid_all,target_time_valid_all,pred_type='valid', lead=0, nyrs_plot= 28)
#     plot_prediction_timeseries(y_pred_valid_all,y_valid_all,y_clim_valid_all,target_time_valid_all,pred_type='valid', lead=50, nyrs_plot= 28)

# if valid_scheme == 'LOOk':
#     for iyr in range(len(test_years)):
#         y_pred_in = y_pred_test_all[iyr,:,:]
#         y_pred_in = y_pred_in[np.where(np.all(~np.isnan(y_pred_in),axis=1))[0]]
#         y_in = y_test_all[iyr,:,:]
#         y_in = y_in[np.where(np.all(~np.isnan(y_in),axis=1))[0]]
#         y_clim_in = y_clim_test_all[iyr,:,:]
#         y_clim_in = y_clim_in[np.where(np.all(~np.isnan(y_clim_in),axis=1))[0]]
#         time_in = target_time_test_all[iyr,:,:]
#         time_in = time_in[np.where(np.all(~np.isnan(time_in),axis=1))[0]]
#         if y_pred_in.shape[0]>0: plot_prediction_timeseries(y_pred_in,y_in,y_clim_in,time_in,pred_type='testing', lead=0, nyrs_plot= 28)
    
#     # for iyr in range(len(test_years)):
#     #     y_pred_in = y_pred_train_all[iyr,:,:]
#     #     y_pred_in = y_pred_in[np.where(np.all(~np.isnan(y_pred_in),axis=1))[0]]
#     #     y_in = y_train_all[iyr,:,:]
#     #     y_in = y_in[np.where(np.all(~np.isnan(y_in),axis=1))[0]]
#     #     y_clim_in = y_clim_train_all[iyr,:,:]
#     #     y_clim_in = y_clim_in[np.where(np.all(~np.isnan(y_clim_in),axis=1))[0]]
#     #     time_in = target_time_train_all[iyr,:,:]
#     #     time_in = time_in[np.where(np.all(~np.isnan(time_in),axis=1))[0]]
#     #     if y_pred_in.shape[0]>0: plot_prediction_timeseries(y_pred_in,y_in,y_clim_in,time_in,pred_type='training', lead=0, nyrs_plot= 56)
        
#     # for iyr in range(len(test_years)):
#     #     for ifold in range(nfolds):
#     #         y_pred_in = y_pred_valid_all[iyr,ifold,:,:]
#     #         y_pred_in = y_pred_in[np.where(np.all(~np.isnan(y_pred_in),axis=1))[0]]
#     #         y_in = y_valid_all[iyr,ifold,:,:]
#     #         y_in = y_in[np.where(np.all(~np.isnan(y_in),axis=1))[0]]
#     #         y_clim_in = y_clim_valid_all[iyr,ifold,:,:]
#     #         y_clim_in = y_clim_in[np.where(np.all(~np.isnan(y_clim_in),axis=1))[0]]
#     #         time_in = target_time_valid_all[iyr,ifold,:,:]
#     #         time_in = time_in[np.where(np.all(~np.isnan(time_in),axis=1))[0]]
#     #         if y_pred_in.shape[0]>0: plot_prediction_timeseries(y_pred_in,y_in,y_clim_in,time_in,pred_type='valid', lead=0, nyrs_plot= 6)


# # plot_sample(y_pred_test,y_test,y_clim_test,target_time_test,it=210, pred_type = 'testing')
# # plot_sample(y_pred_test,y_test,y_clim_test,target_time_test,it=220, pred_type = 'testing')
# # plot_sample(y_pred_test,y_test,y_clim_test,target_time_test,it=240, pred_type = 'testing')

# # plot_sample(y_pred_test,y_test,y_clim_test,target_time_test,it=200+365, pred_type = 'testing')
# # plot_sample(y_pred_test,y_test,y_clim_test,target_time_test,it=210+365, pred_type = 'testing')
# # plot_sample(y_pred_test,y_test,y_clim_test,target_time_test,it=220+365, pred_type = 'testing')

#%% EVALUATE TWATER FORECASTS
    
# def plot_Tw_metric(plot_Tw_metric,plot_Tw_clim_metric,metric_name,mname,vmin=0,vmax=2,vmin_diff=-0.95,vmax_diff=0.95):
#     # cmap = cmocean.cm.tempo
#     # cmap = cmocean.cm.dense
#     # cmap = cmocean.cm.deep
#     # cmap = cmocean.cm.thermal
#     # cmap = plt.get_cmap('cividis')
#     # cmap = plt.get_cmap('viridis')
#     cmap = plt.get_cmap('magma')

#     fig, axs = plt.subplots(1, 1, figsize=(6,4))
#     mappable = axs.pcolormesh(np.flipud(plot_Tw_metric), cmap=cmap, vmin=vmin, vmax=vmax)
#     axs.set_title('$T_{w}$ '+metric_name+' (deg. C) - ' + mname)
#     fig.colorbar(mappable, ax=[axs], location='left')
#     for imonth in range(12):
#         axs.text(62,11.35-imonth,str(np.nanmean(plot_Tw_metric[imonth,:])))

#     fig_clim, axs_clim = plt.subplots(1, 1, figsize=(6,4))
#     mappable = axs_clim.pcolormesh(np.flipud(plot_Tw_clim_metric), cmap=cmap, vmin=vmin, vmax=vmax)
#     axs_clim.set_title('$T_{w}$ '+metric_name+' (deg. C) - Climatology')
#     fig_clim.colorbar(mappable, ax=[axs_clim], location='left')

#     cmap = cmocean.cm.balance
#     fig_diff, axs_diff = plt.subplots(1, 1, figsize=(6,4))
#     mappable = axs_diff.pcolormesh(np.flipud(plot_Tw_metric)-np.flipud(plot_Tw_clim_metric), cmap=cmap, vmin=vmin_diff, vmax=vmax_diff)
#     axs_diff.set_title('$T_{w}$ '+metric_name+' diff. (deg. C)\n ' + mname+' - Climatology')
#     fig_diff.colorbar(mappable, ax=[axs_diff], location='left')

# def evaluate_Tw_forecasts(y_pred_in,y_in,yclim_in,time_in,mname,metric_name = 'MAE', plot = False):
#     rsqr_Tw = np.zeros((12,pred_len))*np.nan; rsqr_Tw_clim = np.zeros((12,pred_len))*np.nan
#     mae_Tw = np.zeros((12,pred_len))*np.nan; mae_Tw_clim = np.zeros((12,pred_len))*np.nan
#     rmse_Tw = np.zeros((12,pred_len))*np.nan; rmse_Tw_clim = np.zeros((12,pred_len))*np.nan

#     # for h in range(pred_len):
#     for imonth in range(12):
#         month = imonth+1
#         samples_Tw = y_pred_in[np.where(np.array([(date_ref+dt.timedelta(days=int(time_in[s,0]))).month for s in range(time_in.shape[0])]) == month )[0]]
#         targets_Tw = y_in[np.where(np.array([(date_ref+dt.timedelta(days=int(time_in[s,0]))).month for s in range(time_in.shape[0])]) == month )[0]]
#         clim_Tw = yclim_in[np.where(np.array([(date_ref+dt.timedelta(days=int(time_in[s,0]))).month for s in range(time_in.shape[0])]) == month )[0]]
#         if samples_Tw.shape[0] > 1:
#             rsqr_Tw[imonth,:],mae_Tw[imonth,:],rmse_Tw[imonth,:] = regression_metrics(targets_Tw,samples_Tw)
#             rsqr_Tw_clim[imonth,:], mae_Tw_clim[imonth,:], rmse_Tw_clim[imonth,:] =  regression_metrics(targets_Tw,clim_Tw)

#             if month == 11:
#                 rsqr_nov,mae_nov,rmse_nov = regression_metrics(targets_Tw,samples_Tw,output_opt='uniform_average')
#                 print('Nov. Tw. MAE (days): '+ str(mae_nov) )
#                 # print(np.nanmean(mae_Tw[imonth,:]))
#             if month == 12:
#                 rsqr_dec,mae_dec,rmse_dec = regression_metrics(targets_Tw,samples_Tw,output_opt='uniform_average')
#                 print('Dec. Tw. MAE (days): '+ str(mae_dec) )
#                 # print(np.nanmean(mae_Tw[imonth,:]))
                
#     # Plot Tw forecast metrics
#     if metric_name == 'Rsqr':
#         if plot: plot_Tw_metric(rsqr_Tw,rsqr_Tw_clim,'R$^2$',mname)
#         return rsqr_Tw,rsqr_Tw_clim
#     if metric_name == 'MAE':
#         if plot: plot_Tw_metric(mae_Tw,mae_Tw_clim,'MAE',mname)
#         return mae_Tw,mae_Tw_clim
#     if metric_name == 'RMSE':
#         if plot: plot_Tw_metric(rmse_Tw,rmse_Tw_clim,'RMSE',mname)
#         return rmse_Tw,rmse_Tw_clim
    
    


# if valid_scheme == 'LOOk':
#     # model_name = 'Encoder-Decoder LSTM - TEST'
#     # Tw_MAE_test_yr = np.zeros((12,pred_len,len(test_years)))*np.nan
#     # Tw_clim_MAE_test_yr = np.zeros((12,pred_len,len(test_years)))*np.nan
#     # for iyr in range(len(test_years)):
#     #     y_pred_in_tmp = y_pred_test_all[iyr,:,:]
#     #     y_pred_in = y_pred_in_tmp[np.where(np.all(~np.isnan(y_pred_in_tmp),axis=1))[0]]
#     #     y_in = y_test_all[iyr,:,:]
#     #     y_in = y_in[np.where(np.all(~np.isnan(y_pred_in_tmp),axis=1))[0]]
#     #     y_clim_in = y_clim_test_all[iyr,:,:]
#     #     y_clim_in = y_clim_in[np.where(np.all(~np.isnan(y_pred_in_tmp),axis=1))[0]]
#     #     time_in = target_time_test_all[iyr,:,:]
#     #     time_in = time_in[np.where(np.all(~np.isnan(y_pred_in_tmp),axis=1))[0]]
#     #     if y_pred_in.shape[0]>0: Tw_MAE_test_yr[:,:,iyr], Tw_clim_MAE_test_yr[:,:,iyr] = evaluate_Tw_forecasts(y_pred_in,y_in,y_clim_in,time_in,model_name)
#     # Tw_MAE_test = np.nanmean(Tw_MAE_test_yr,axis=2)
#     # Tw_clim_MAE_test = np.nanmean(Tw_clim_MAE_test_yr,axis=2)      
#     # Tw_MAE_test_std = np.nanstd(Tw_MAE_test_yr,axis=2)
#     # Tw_clim_MAE_test_std = np.nanstd(Tw_clim_MAE_test_yr,axis=2)     
#     # plot_Tw_metric(Tw_MAE_test,Tw_clim_MAE_test,'MAE',model_name)
#     # plot_Tw_metric(Tw_MAE_test_std,Tw_clim_MAE_test_std,'MAE',model_name,vmin=0,vmax=0.5)
    
#     # model_name = 'Encoder-Decoder LSTM - TRAINING'
#     # Tw_MAE_train_yr = np.zeros((12,pred_len,len(test_years)))*np.nan
#     # Tw_clim_MAE_train_yr = np.zeros((12,pred_len,len(test_years)))*np.nan
#     # for iyr in range(len(test_years)):
#     #     y_pred_in_tmp = y_pred_train_all[iyr,:,:]
#     #     y_pred_in = y_pred_in_tmp[np.where(np.all(~np.isnan(y_pred_in_tmp),axis=1))[0]]
#     #     y_in = y_train_all[iyr,:,:]
#     #     y_in = y_in[np.where(np.all(~np.isnan(y_pred_in_tmp),axis=1))[0]]
#     #     y_clim_in = y_clim_train_all[iyr,:,:]
#     #     y_clim_in = y_clim_in[np.where(np.all(~np.isnan(y_pred_in_tmp),axis=1))[0]]
#     #     time_in = target_time_train_all[iyr,:,:]
#     #     time_in = time_in[np.where(np.all(~np.isnan(y_pred_in_tmp),axis=1))[0]]
#     #     if y_pred_in.shape[0]>0: Tw_MAE_train_yr[:,:,iyr], Tw_clim_MAE_train_yr[:,:,iyr] = evaluate_Tw_forecasts(y_pred_in,y_in,y_clim_in,time_in,model_name)
#     # Tw_MAE_train = np.nanmean(Tw_MAE_train_yr,axis=2)
#     # Tw_clim_MAE_train = np.nanmean(Tw_clim_MAE_train_yr,axis=2)      
#     # Tw_MAE_train_std = np.nanstd(Tw_MAE_train_yr,axis=2)
#     # Tw_clim_MAE_train_std = np.nanstd(Tw_clim_MAE_train_yr,axis=2)     
#     # plot_Tw_metric(Tw_MAE_train,Tw_clim_MAE_train,'MAE_mean',model_name)
#     # plot_Tw_metric(Tw_MAE_train_std,Tw_clim_MAE_train_std,'MAE_std',model_name,vmin=0,vmax=0.5)
    

#     model_name = 'Encoder-Decoder LSTM - VALID'
#     show_all_folds = False
#     Tw_MAE_valid_fold = np.zeros((12,pred_len,len(test_years),nfolds))*np.nan
#     Tw_clim_MAE_valid_fold = np.zeros((12,pred_len,len(test_years),nfolds))*np.nan
#     for iyr in range(len(test_years)):
#         for ifold in range(nfolds):
#             y_pred_in_tmp = y_pred_valid_all[iyr,ifold,:,:]
#             y_pred_in = y_pred_in_tmp[np.where(np.all(~np.isnan(y_pred_in_tmp),axis=1))[0]]
#             y_in = y_valid_all[iyr,ifold,:,:]
#             y_in = y_in[np.where(np.all(~np.isnan(y_pred_in_tmp),axis=1))[0]]
#             y_clim_in = y_clim_valid_all[iyr,ifold,:,:]
#             y_clim_in = y_clim_in[np.where(np.all(~np.isnan(y_pred_in_tmp),axis=1))[0]]
#             time_in = target_time_valid_all[iyr,ifold,:,:]
#             time_in = time_in[np.where(np.all(~np.isnan(y_pred_in_tmp),axis=1))[0]]
#             if y_pred_in.shape[0]>0: Tw_MAE_valid_fold[:,:,iyr,ifold], Tw_clim_MAE_valid_fold[:,:,iyr,ifold] = evaluate_Tw_forecasts(y_pred_in,y_in,y_clim_in,time_in,model_name)
#         if show_all_folds:
#             if ~np.all(np.isnan(Tw_MAE_valid_fold[:,:,iyr,:])):
#                 Tw_MAE_valid_yr = np.nanmean(Tw_MAE_valid_fold,axis=3)
#                 Tw_clim_MAE_valid_yr = np.nanmean(Tw_clim_MAE_valid_fold,axis=3)      
#                 Tw_MAE_valid_std_yr = np.nanstd(Tw_MAE_valid_fold,axis=3)
#                 Tw_clim_MAE_valid_std_yr = np.nanstd(Tw_clim_MAE_valid_fold,axis=3)          
#                 plot_Tw_metric(Tw_MAE_valid_yr[:,:,iyr],Tw_clim_MAE_valid_yr[:,:,iyr],'MAE_mean',model_name)
#                 # plot_Tw_metric(Tw_MAE_valid_std_yr[:,:,iyr],Tw_clim_MAE_valid_std_yr[:,:,iyr],'MAE_std',model_name,vmin=0,vmax=0.5)
#     Tw_MAE_valid = np.nanmean(Tw_MAE_valid_fold,axis=(2,3))
#     Tw_clim_MAE_valid = np.nanmean(Tw_clim_MAE_valid_fold,axis=(2,3))      
#     Tw_MAE_valid_std = np.nanstd(Tw_MAE_valid_fold,axis=(2,3))
#     Tw_clim_MAE_valid_std = np.nanstd(Tw_clim_MAE_valid_fold,axis=(2,3))          
#     plot_Tw_metric(Tw_MAE_valid,Tw_clim_MAE_valid,'MAE_mean',model_name)
#     # plot_Tw_metric(Tw_MAE_valid_std,Tw_clim_MAE_valid_std,'MAE_std',model_name,vmin=0,vmax=0.5)
    
# else:
#     # model_name = 'Encoder-Decoder LSTM - TEST'
#     # y_clim_test_all = np.squeeze(y_clim_test_all)
#     # Tw_MAE_test, Tw_clim_MAE_test = evaluate_Tw_forecasts(y_pred_test_all,y_test_all,y_clim_test_all,target_time_test_all,model_name)

#     model_name = 'Encoder-Decoder LSTM - VALID'
#     y_clim_valid_all = np.squeeze(y_clim_valid_all)
#     Tw_MAE_valid, Tw_clim_MAE_valid = evaluate_Tw_forecasts(y_pred_valid_all,y_valid_all,y_clim_valid_all,target_time_valid_all,model_name)
#     plot_Tw_metric(Tw_MAE_valid,Tw_clim_MAE_valid,'MAE_mean',model_name)


#     # model_name = 'Encoder-Decoder LSTM - TRAINING'
#     # y_clim_train_all = np.squeeze(y_clim_train_all)
#     # Tw_MAE_train, Tw_clim_MAE_train = evaluate_Tw_forecasts(y_pred_train_all,y_train_all,y_clim_train_all,target_time_train_all,model_name)




#%% EVALUATE FUD FORECASTS

# def detect_FUD_from_Tw_samples(y_true,y_pred,y_clim,time_y,years,freezeup_opt,start_doy_arr,istart_label,day_istart,plot_samples,date_ref = dt.date(1900,1,1) ):

#     from functions import running_nanmean
#     import scipy
#     import scipy.ndimage
#     import calendar
    
    
#     def find_freezeup_Tw_threshold(def_opt,Twater_in,time,thresh_T=2.0,ndays=7,date_ref=dt.date(1900,1,1)):

#         def record_event(istart,time,year,Twater_in,freezeup_date,mask_freezeup,date_ref):
#             # Temperature has been lower than thresh_T
#             # for more than (or equal to) ndays.
#             # Define freezeup date as first date of group

#             date_start = date_ref+dt.timedelta(days=int(time[istart]))
#             doy_start = (date_start - dt.date(int(date_start.year),1,1)).days+1

#             if ((date_start.year > 1992) | ((date_start.year == 1992) & (date_start.month > 10)) ):
#                 if ( (date_start.year == year) & (doy_start > 319) ) | ((date_start.year == year+1) & (doy_start < 46)):
#                         freezeup_date[0] = date_start.year
#                         freezeup_date[1] = date_start.month
#                         freezeup_date[2] = date_start.day
#                         freezeup_Tw = Twater_in[istart]
#                         mask_freezeup[istart] = True
#                 else:
#                     freezeup_date[0] = np.nan
#                     freezeup_date[1] = np.nan
#                     freezeup_date[2] = np.nan
#                     freezeup_Tw = np.nan
#                     mask_freezeup[istart] = False
#             else:
#             # I think this condition exists because the Tw time series
#             # starts in January 1992, so it is already frozen, but we
#             # do not want to detect this as freezeup for 1992, so we
#             # have to wait until at least October 1992 before recording
#             # any FUD events.
#                 freezeup_date[0] = np.nan
#                 freezeup_date[1] = np.nan
#                 freezeup_date[2] = np.nan
#                 freezeup_Tw = np.nan
#                 mask_freezeup[istart] = False

#             return freezeup_date, freezeup_Tw, mask_freezeup

#         date_start = dt.timedelta(days=int(time[0])) + date_ref
#         if date_start.month < 3:
#             year = date_start.year-1
#         else:
#             year = date_start.year

#         mask_tmp = Twater_in <= thresh_T
#         mask_freezeup = mask_tmp.copy()
#         mask_freezeup[:] = False

#         freezeup_Tw = np.nan
#         freezeup_date=np.zeros((3))*np.nan

#         # Loop on sample time steps
#         for im in range(mask_freezeup.size):

#             if (im == 0): # First time step cannot be detected as freeze-up
#                 sum_m = 0
#                 istart = -1 # This ensures that a freeze-up is not detected if the time series started already below the freezing temp.
#             else:
#                 if (np.sum(mask_freezeup) == 0): # Only continue while no prior freeze-up was detected for the sequence
#                     if (~mask_tmp[im-1]):
#                         sum_m = 0
#                         if ~mask_tmp[im]:
#                             sum_m = 0
#                         else:
#                             # start new group
#                             sum_m +=1
#                             istart = im
#                             # Below will only occur if ndays is set to 1, e.g. first day of freezing temp.
#                             if (sum_m >= ndays):
#                                 freezeup_date,freezeup_Tw,mask_freezeup = record_event(istart,time,year,Twater_in,freezeup_date,mask_freezeup,date_ref)
#                     else:
#                         if (mask_tmp[im]) & (istart > 0):
#                             sum_m += 1
#                             if (sum_m >= ndays):
#                                 freezeup_date,freezeup_Tw,mask_freezeup = record_event(istart,time,year,Twater_in,freezeup_date,mask_freezeup,date_ref)

#         return freezeup_date, mask_freezeup


#     if freezeup_opt == 1:
#         def_opt = 1
#         smooth_T =False; N_smooth = 3; mean_type='centered'
#         round_T = False; round_type= 'half_unit'
#         Gauss_filter = False
#         sig_dog = 3.5
#         T_thresh = 0.75
#         dTdt_thresh = 0.25
#         d2Tdt2_thresh = 0.25
#         nd = 1
#         no_negTw = False
    
#     # FIND FREEZE-UP FROM SIMULATED SAMPLES
#     nsamples_per_doy = np.zeros(len(start_doy_arr))*np.nan
    
#     for istart in range(len(start_doy_arr)):
#         month = month_istart[istart]
#         day = day_istart[istart]
#         month_ind = np.where((np.array([(date_ref+dt.timedelta(days=int(time_y[s,0]))).month for s in range(time_y.shape[0])]) == month))[0]
#         day_ind = np.where((np.array([(date_ref+dt.timedelta(days=int(time_y[s,0]))).day for s in range(time_y.shape[0])]) == day))[0]
#         istart_ind = np.sort(list( set(month_ind).intersection(day_ind) ))
#         nsamples_per_doy[istart] = len(istart_ind)
    
#     freezeup_dates_sample = np.zeros((len(start_doy_arr),int(np.nanmax(nsamples_per_doy)),4))*np.nan
#     freezeup_dates_sample_doy = np.zeros((len(start_doy_arr),int(np.nanmax(nsamples_per_doy))))*np.nan
    
#     freezeup_dates_target = np.zeros((len(start_doy_arr),int(np.nanmax(nsamples_per_doy)),4))*np.nan
#     freezeup_dates_target_doy = np.zeros((len(start_doy_arr),int(np.nanmax(nsamples_per_doy))))*np.nan
    
#     freezeup_dates_clim_target = np.zeros((len(start_doy_arr),int(np.nanmax(nsamples_per_doy)),4))*np.nan
#     freezeup_dates_clim_target_doy = np.zeros((len(start_doy_arr),int(np.nanmax(nsamples_per_doy))))*np.nan
    
#     for istart,start_doy in enumerate(start_doy_arr):
    
#         if nsamples_per_doy[istart] > 0:
#             # Find sample starting on the day & month of start date:
#             month = month_istart[istart]
#             day = day_istart[istart]
#             month_ind = np.where((np.array([(date_ref+dt.timedelta(days=int(time_y[s,0]))).month for s in range(time_y.shape[0])]) == month))[0]
#             day_ind = np.where((np.array([(date_ref+dt.timedelta(days=int(time_y[s,0]))).day for s in range(time_y.shape[0])]) == day))[0]
#             istart_ind = np.sort(list( set(month_ind).intersection(day_ind) ))
        
#             samples = y_pred[istart_ind]
#             targets = y_true[istart_ind]
#             clim_targets = y_clim[istart_ind]
#             time_st = time_y[istart_ind]
        
#             for s in range(samples.shape[0]):
        
#                 time = time_st[s,:]
        
#                 # FIND DTDt, D2Tdt2,etc. - SAMPLE
#                 Twater_sample = samples[s]
        
#                 Twater_tmp = Twater_sample.copy()
#                 Twater_dTdt_sample = np.zeros(Twater_sample.shape)*np.nan
#                 Twater_d2Tdt2_sample = np.zeros(Twater_sample.shape)*np.nan
#                 Twater_DoG1_sample = np.zeros(Twater_sample.shape)*np.nan
#                 Twater_DoG2_sample = np.zeros(Twater_sample.shape)*np.nan
        
#                 if round_T:
#                     if round_type == 'unit':
#                         Twater_tmp = np.round(Twater_tmp.copy())
#                     if round_type == 'half_unit':
#                         Twater_tmp = np.round(Twater_tmp.copy()* 2) / 2.
#                 if smooth_T:
#                     Twater_tmp = running_nanmean(Twater_tmp.copy(),N_smooth,mean_type=mean_type)
        
#                 dTdt_tmp = np.zeros((Twater_tmp.shape[0],3))*np.nan
        
#                 dTdt_tmp[0:-1,0]= Twater_tmp[1:]- Twater_tmp[0:-1] # Forwards
#                 dTdt_tmp[1:,1] = Twater_tmp[1:] - Twater_tmp[0:-1] # Backwards
#                 dTdt_tmp[0:-1,2]= Twater_tmp[0:-1]-Twater_tmp[1:]  # -1*Forwards
        
#                 Twater_dTdt_sample= np.nanmean(dTdt_tmp[:,0:2],axis=1)
#                 Twater_d2Tdt2_sample = -1*np.nanmean(dTdt_tmp[:,1:3],axis=1)
        
#                 if Gauss_filter:
#                     Twater_dTdt_sample = scipy.ndimage.gaussian_filter1d(Twater_tmp.copy(),sigma=sig_dog,order=1)
#                     Twater_d2Tdt2_sample = scipy.ndimage.gaussian_filter1d(Twater_tmp.copy(),sigma=sig_dog,order=2)
        
#                 # FIND DTDt, D2Tdt2,etc. - TARGET
#                 Twater_target = targets[s]
#                 Twater_dTdt_target = np.zeros(Twater_target.shape)*np.nan
#                 Twater_d2Tdt2_target = np.zeros(Twater_target.shape)*np.nan
#                 Twater_DoG1_target = np.zeros(Twater_target.shape)*np.nan
#                 Twater_DoG2_target = np.zeros(Twater_target.shape)*np.nan
        
#                 Twater_tmp_target = Twater_target.copy()
#                 if round_T:
#                     if round_type == 'unit':
#                         Twater_tmp_target = np.round(Twater_tmp_target.copy())
#                     if round_type == 'half_unit':
#                         Twater_tmp_target = np.round(Twater_tmp_target.copy()* 2) / 2.
#                 if smooth_T:
#                     Twater_tmp_target = running_nanmean(Twater_tmp_target.copy(),N_smooth,mean_type=mean_type)
        
#                 dTdt_tmp = np.zeros((Twater_tmp_target.shape[0],3))*np.nan
        
#                 dTdt_tmp[0:-1,0]= Twater_tmp_target[1:]- Twater_tmp_target[0:-1] # Forwards
#                 dTdt_tmp[1:,1] = Twater_tmp_target[1:] - Twater_tmp_target[0:-1] # Backwards
#                 dTdt_tmp[0:-1,2]= Twater_tmp_target[0:-1]-Twater_tmp_target[1:]  # -1*Forwards
        
#                 Twater_dTdt_target= np.nanmean(dTdt_tmp[:,0:2],axis=1)
#                 Twater_d2Tdt2_target = -1*np.nanmean(dTdt_tmp[:,1:3],axis=1)
        
#                 if Gauss_filter:
#                     Twater_dTdt_target = scipy.ndimage.gaussian_filter1d(Twater_tmp_target.copy(),sigma=sig_dog,order=1)
#                     Twater_d2Tdt2_target = scipy.ndimage.gaussian_filter1d(Twater_tmp_target.copy(),sigma=sig_dog,order=2)
        
#                 # FIND DTDt, D2Tdt2,etc. - CLIM TARGET
#                 Twater_clim_target = clim_targets[s]
#                 Twater_dTdt_clim_target = np.zeros(Twater_clim_target.shape)*np.nan
#                 Twater_d2Tdt2_clim_target = np.zeros(Twater_clim_target.shape)*np.nan
#                 Twater_DoG1_clim_target = np.zeros(Twater_clim_target.shape)*np.nan
#                 Twater_DoG2_clim_target = np.zeros(Twater_clim_target.shape)*np.nan
        
#                 Twater_tmp_clim_target = Twater_clim_target.copy()
#                 if round_T:
#                     if round_type == 'unit':
#                         Twater_tmp_clim_target = np.round(Twater_tmp_clim_target.copy())
#                     if round_type == 'half_unit':
#                         Twater_tmp_clim_target = np.round(Twater_tmp_clim_target.copy()* 2) / 2.
#                 if smooth_T:
#                     Twater_tmp_clim_target = running_nanmean(Twater_tmp_clim_target.copy(),N_smooth,mean_type=mean_type)
        
#                 dTdt_tmp = np.zeros((Twater_tmp_clim_target.shape[0],3))*np.nan
        
#                 dTdt_tmp[0:-1,0]= Twater_tmp_clim_target[1:]- Twater_tmp_clim_target[0:-1] # Forwards
#                 dTdt_tmp[1:,1] = Twater_tmp_clim_target[1:] - Twater_tmp_clim_target[0:-1] # Backwards
#                 dTdt_tmp[0:-1,2]= Twater_tmp_clim_target[0:-1]-Twater_tmp_clim_target[1:]  # -1*Forwards
        
#                 Twater_dTdt_clim_target= np.nanmean(dTdt_tmp[:,0:2],axis=1)
#                 Twater_d2Tdt2_clim_target = -1*np.nanmean(dTdt_tmp[:,1:3],axis=1)
        
#                 if Gauss_filter:
#                     Twater_dTdt_clim_target = scipy.ndimage.gaussian_filter1d(Twater_tmp_clim_target.copy(),sigma=sig_dog,order=1)
#                     Twater_d2Tdt2_clim_target = scipy.ndimage.gaussian_filter1d(Twater_tmp_clim_target.copy(),sigma=sig_dog,order=2)
        
        
#                 # if (istart == 4):
#                 if plot_samples:
#                     plt.figure()
#                     plt.title(istart_label[istart]+', '+str((dt.timedelta(days=int(time[0])) + date_ref).year))
#                     plt.plot(Twater_target, color='black',label='Observed')
#                     plt.plot(Twater_clim_target,label='Climatology')
#                     plt.plot(Twater_sample,label='Forecast')
#                     plt.legend()
    
        
#                 # FIND FREEZE-UP FOR BOTH SAMPLE AND TARGET
#                 date_start = dt.timedelta(days=int(time[0])) + date_ref
#                 year = date_start.year
        
#                 if year >= years[0]:
#                     iyr = np.where(years == year)[0][0]
#                     fd_sample, mask_freeze_sample = find_freezeup_Tw_threshold(def_opt,Twater_tmp,time,thresh_T = T_thresh,ndays = nd)
#                     fd_target, mask_freeze_target = find_freezeup_Tw_threshold(def_opt,Twater_tmp_target,time,thresh_T = T_thresh,ndays = nd)
#                     fd_clim_target, mask_freeze_clim_target = find_freezeup_Tw_threshold(def_opt,Twater_tmp_clim_target,time,thresh_T = T_thresh,ndays = nd)
        
#                     if (np.sum(mask_freeze_sample) > 0): # A freeze-up was detected in sample
#                         if fd_sample[0] == year:
#                             if calendar.isleap(years[iyr]):
#                                 freezeup_dates_sample[istart,s,0] = iyr
#                                 freezeup_dates_sample[istart,s,1:4] = fd_sample
#                                 freezeup_dates_sample_doy[istart,s]= (dt.date(int(fd_sample[0]),int(fd_sample[1]),int(fd_sample[2]))-dt.date(int(fd_sample[0]),1,1)).days
#                             else:
#                                 freezeup_dates_sample[istart,s,0] = iyr
#                                 freezeup_dates_sample[istart,s,1:4] = fd_sample
#                                 freezeup_dates_sample_doy[istart,s]= (dt.date(int(fd_sample[0]),int(fd_sample[1]),int(fd_sample[2]))-dt.date(int(fd_sample[0]),1,1)).days+1
#                         else:
#                             if calendar.isleap(years[iyr]):
#                                 freezeup_dates_sample[istart,s,0] = iyr
#                                 freezeup_dates_sample[istart,s,1:4] = fd_sample
#                                 freezeup_dates_sample_doy[istart,s]= fd_sample[2]+365
#                             else:
#                                 freezeup_dates_sample[istart,s,0] = iyr
#                                 freezeup_dates_sample[istart,s,1:4] = fd_sample
#                                 freezeup_dates_sample_doy[istart,s]= fd_sample[2]+365
        
        
#                     if (np.sum(mask_freeze_target) > 0): # A freeze-up was detected in target
#                         if fd_target[0] == year:
#                             if calendar.isleap(years[iyr]):
#                                 freezeup_dates_target[istart,s,0] = iyr
#                                 freezeup_dates_target[istart,s,1:4] = fd_target
#                                 freezeup_dates_target_doy[istart,s]= (dt.date(int(fd_target[0]),int(fd_target[1]),int(fd_target[2]))-dt.date(int(fd_target[0]),1,1)).days
#                             else:
#                                 freezeup_dates_target[istart,s,0] = iyr
#                                 freezeup_dates_target[istart,s,1:4] = fd_target
#                                 freezeup_dates_target_doy[istart,s]= (dt.date(int(fd_target[0]),int(fd_target[1]),int(fd_target[2]))-dt.date(int(fd_target[0]),1,1)).days+1
#                         else:
#                             if calendar.isleap(years[iyr]):
#                                 freezeup_dates_target[istart,s,0] = iyr
#                                 freezeup_dates_target[istart,s,1:4] = fd_target
#                                 freezeup_dates_target_doy[istart,s]= fd_target[2]+365
#                             else:
#                                 freezeup_dates_target[istart,s,0] = iyr
#                                 freezeup_dates_target[istart,s,1:4] = fd_target
#                                 freezeup_dates_target_doy[istart,s]= fd_target[2]+365
        
#                     if (np.sum(mask_freeze_clim_target) > 0): # A freeze-up was detected in climatology
#                         if fd_clim_target[0] == year:
#                             freezeup_dates_clim_target[istart,s,0] = iyr
#                             if calendar.isleap(years[iyr]):
#                                 freezeup_dates_clim_target[istart,s,1] = fd_clim_target[0]
#                                 freezeup_dates_clim_target[istart,s,2] = fd_clim_target[1]
#                                 freezeup_dates_clim_target[istart,s,3] = fd_clim_target[2]+1
#                                 freezeup_dates_clim_target_doy[istart,s]= (dt.date(int(fd_clim_target[0]),int(fd_clim_target[1]),int(fd_clim_target[2]))-dt.date(int(fd_clim_target[0]),1,1)).days+1
#                             else:
#                                 freezeup_dates_clim_target[istart,s,1:4] = fd_clim_target
#                                 freezeup_dates_clim_target_doy[istart,s]= (dt.date(int(fd_clim_target[0]),int(fd_clim_target[1]),int(fd_clim_target[2]))-dt.date(int(fd_clim_target[0]),1,1)).days+1
        
        
#     # FIND MEAN FUD FROM Tw CLIM:
#     mean_clim_FUD = np.nanmean(freezeup_dates_clim_target_doy)
#     # if recalibrate:
#     #     if offset_type == 'mean_clim':
#     #         offset_forecasts = mean_clim_FUD
#     #     freezeup_dates_sample_doy = freezeup_dates_sample_doy - offset_forecasts + mean_FUD_Longueuil_train
    
    
#     return freezeup_dates_sample,freezeup_dates_sample_doy,freezeup_dates_target,freezeup_dates_target_doy,freezeup_dates_clim_target,freezeup_dates_clim_target_doy,mean_clim_FUD
   
# def evaluate_FUD_forecasts(freezeup_dates_sample,freezeup_dates_sample_doy,freezeup_dates_target,freezeup_dates_target_doy,mean_clim_FUD,years,start_doy_arr,istart_label,plot_FUD_ts,plot_metrics,verbose=True):
    
#     import statsmodels.api as sm
    
#     # EVALUATE FREEZE-UP FORECAST ACCORDING TO SELECTED METRIC:
#     fu_rmse = np.zeros((len(start_doy_arr),len(years)))*np.nan
#     fu_acc = np.zeros((len(start_doy_arr),len(years)))*np.nan
#     fu_mae = np.zeros((len(start_doy_arr),len(years)))*np.nan
    
#     MAE_arr = np.zeros((len(start_doy_arr)))*np.nan
#     RMSE_arr = np.zeros((len(start_doy_arr)))*np.nan
#     Rsqr_arr = np.zeros((len(start_doy_arr)))*np.nan
#     Rsqradj_arr = np.zeros((len(start_doy_arr)))*np.nan
#     Acc_arr = np.zeros((len(start_doy_arr)))*np.nan
    
#     for istart in range(len(start_doy_arr)):
#         n = 0
    
#         # The FUD is not detectabble because the forecast
#         # length doesn't reach the FUD (or because there
#         # were no samples for that period, e.g. because
#         # of the presence of nans in the predictors time
#         # series)
        
#         # years_impossible =[]
#         # tmp_arr = years.copy().astype('float')
#         # for iyr in range(len(years)):
#         #     if len(np.where(freezeup_dates_target[istart,:,0]==iyr)[0])>0:
#         #         i_s = np.where(freezeup_dates_target[istart,:,0]==iyr)[0]
#         #         if ~np.isnan(freezeup_dates_target[istart,i_s,0]):
#         #             tmp_arr[iyr] = np.nan
#         # years_impossible = years_impossible + tmp_arr[~np.isnan(tmp_arr)].tolist()
    
#         # print(years_impossible)
#         # years_impossible = np.array(years_impossible).astype('int')
#         sample_freezeup_doy = freezeup_dates_sample_doy[istart,:]
#         target_freezeup_doy = freezeup_dates_target_doy[istart,:]
#         ts_doy = np.zeros(len(years))*np.nan
#         ts_doy_obs = np.zeros(len(years))*np.nan
    
#         # Evaluate the performance only on detectable FUDs
#         if np.sum(~np.isnan(target_freezeup_doy)) > 0:
    
#             for iyr in range(len(years)):
#                 if len(np.where(freezeup_dates_target[istart,:,0]==iyr)[0])>0:
#                     i_st = np.where(freezeup_dates_target[istart,:,0]==iyr)[0]
#                     fo_doy = target_freezeup_doy[i_st]
#                     ts_doy_obs[iyr] = fo_doy
                    
#                     if (~np.isnan(target_freezeup_doy[i_st])):
#                         n += 1
#                         if len(np.where(freezeup_dates_sample[istart,:,0]==iyr)[0])>0:
#                             i_ss = np.where(freezeup_dates_sample[istart,:,0]==iyr)[0][0]
#                             fs_doy = sample_freezeup_doy[i_ss]
#                             ts_doy[iyr] = fs_doy
#                             fc_doy = mean_clim_FUD
        
#                             # obs_cat = Longueuil_FUD_period_cat[iyr]
#                             # if ~np.isnan(fs_doy):
#                             #     if fs_doy <= tercile1_FUD_Longueuil_train:
#                             #         sample_cat = -1
#                             #     elif fs_doy > tercile2_FUD_Longueuil_train:
#                             #         sample_cat = 1
#                             #     else:
#                             #         sample_cat = 0
#                             #     if (sample_cat == obs_cat):
#                             #         fu_acc[istart,iyr] = 1
#                             #     else:
#                             #         fu_acc[istart,iyr] = 0
#                             # else:
#                             #     fu_acc[istart,iyr] = np.nan
        
#                             fu_rmse[istart,iyr] = (fs_doy-fo_doy)**2.
#                             fu_mae[istart,iyr] = np.abs(fs_doy-fo_doy)
#                             # print(istart,iyr,n,fo_doy,fs_doy,fc_doy,obs_cat,sample_cat,fu_acc[istart,iyr])
        
#                         else:
#                             ts_doy[iyr] = np.nan
#                             fu_rmse[istart,iyr] = np.nan
#                             fu_mae[istart,iyr] = np.nan
#                             # fu_acc[istart,iyr] = np.nan
        
#                             # print(istart,iyr,n,Longueuil_FUD_period[iyr],np.nan,mean_clim_FUD,Longueuil_FUD_period_cat[iyr],np.nan,fu_acc[istart,iyr])
    
#         # Here we don't use nanmean, but we nansum and divide by "n", where
#         # "n" is the number of detectable FUDs for the given start date.
#         # If an FUD was not forecasted/detected but it could have been,
#         # then n will be larger than the available number of ML forecasts so the
#         # performance is penalized.
#         # print(istart,n)
#         if np.all(np.isnan(fu_mae[istart,:])):
#             MAE_arr[istart] = np.nan
#         else:
#             MAE_arr[istart] = np.nansum(fu_mae[istart,:])/n
    
#         if np.all(np.isnan(fu_rmse[istart,:])):
#             RMSE_arr[istart] = np.nan
#         else:
#             RMSE_arr[istart] = np.sqrt(np.nansum(fu_rmse[istart,:])/n)
    
#         # if  np.all(np.isnan(fu_acc[istart,:])):
#         #     Acc_arr[istart] = np.nan
#         # else:
#         #     Acc_arr[istart] = np.nansum(fu_acc[istart,:])/n
    
#         # if (np.all(np.isnan(ts_doy))) | (np.all(np.isnan(Longueuil_FUD_period))) :
#         #     Rsqr_arr[istart,il,iw,ih] = np.nan
#         #     Rsqradj_arr[istart,il,iw,ih] = np.nan
#         # else:
#         #     model = sm.OLS(Longueuil_FUD_period, sm.add_constant(ts_doy,has_constant='skip'), missing='drop').fit()
#         #     Rsqr_arr[istart,il,iw,ih] = model.rsquared
#         #     Rsqradj_arr[istart,il,iw,ih] = model.rsquared_adj
    
    
#     # PLOT FUD TIME SERIES
#     if plot_FUD_ts:
#         fig, ax = plt.subplots()
#         ax.plot(years,np.ones(len(years))*(mean_clim_FUD),color=plt.get_cmap('tab20c')(2))
#         # ax.plot(years,np.ones(len(years))*(mean_FUD_Longueuil_train),color=[0.7,0.7,0.7])
#         ax.plot(years,ts_doy_obs,'o-',color='black')
        
#         for ic,istart in enumerate([0,1,2,3,4]):
    
#             figi, axi = plt.subplots()
#             axi.plot(years,np.ones(len(years))*(mean_clim_FUD),color=plt.get_cmap('tab20c')(2))
#             axi.plot(years,ts_doy_obs,'o-',color='black')
            
#             fd_ML_forcast = np.zeros((len(years)))*np.nan
#             for iyr in range(len(years)):
#                 select_yr_fud = freezeup_dates_sample[istart,np.where(freezeup_dates_sample[istart,:,0] == iyr)[0]]
#                 if select_yr_fud.shape[0] > 0:
#                     fd_ML_forcast[iyr] = freezeup_dates_sample_doy[istart,np.where(freezeup_dates_sample[istart,:,0] == iyr)[0]]
#             # print(fd_ML_forcast)
#             ax.plot(years,fd_ML_forcast,'o:', color=plt.get_cmap('tab20c')(7-ic), label= model_name + ' - '+istart_label[istart])
#             axi.plot(years,fd_ML_forcast,'o:', color=plt.get_cmap('tab20c')(7-ic), label= model_name + ' - '+istart_label[istart])
            
#             if ~np.all(np.isnan(fd_ML_forcast)):
#                 model = sm.OLS(ts_doy_obs, sm.add_constant(fd_ML_forcast,has_constant='skip'), missing='drop').fit()
#                 if verbose:
#                     print('-----------------------------')
#                     print('START DATE: ' + istart_label[istart])
#                     print('Rsqr: ',model.rsquared, model.rsquared_adj)
#                     print('MAE: ', MAE_arr[istart])
#                     print('RMSE: ',RMSE_arr[istart])
#                     print('Acc.: ',Acc_arr[istart])
        
#             # if recalibrate:
#             #     if offset_type == 'mean_clim':
#             #         plt.title('Recalibrated Forecasts - Mean clim\n'+'nlayers:'+str(nlayers)+', input window: '+str(inpw))
#             # else:
#             #     plt.title('Raw Forecasts\n'+'nlayers:'+str(nlayers)+', input window: '+str(inpw))
#             plt.title('Raw Forecasts')
    
#         ax.legend()
    
    
#     # PLOT FUD EVALUATION METRICS
#     # if plot_metrics:
#         # istart_plot = [0,1,2,3,4]
#         # fig_mae,ax_mae = plt.subplots(nrows = 1, ncols = len(istart_plot),figsize=(14,3))
#         # letter = ['a)','b)','c)','d)','e)','f)']
#         # for i,istart in enumerate(istart_plot):
#         #     p_mae=ax_mae[i].imshow(np.expand_dims(np.expand_dims(MAE_arr[istart],axis=0),axis=0),vmin=3,vmax=9)
#         #     # p_mae=ax_mae[i].imshow(np.squeeze(MAE_arr[istart]),vmin=3,vmax=9)
#         #     # ax_mae[0].set_ylabel('Nb. Layers')
#         #     # ax_mae[1].set_xlabel('Context Window (days)')
#         #     # ax_mae[i].set_yticks(np.arange(len(nb_layers_list)))
#         #     # ax_mae[i].set_yticklabels([str(nb_layers_list[k]) for k in range(len(nb_layers_list))])
#         #     # ax_mae[i].set_xticks(np.arange(len(input_window_list)))
#         #     # ax_mae[i].set_xticklabels([str(input_window_list[k]) for k in range(len(input_window_list))])
#         #     ax_mae[i].set_title(letter[i]+' '+istart_label[istart])
        
#         # fig_mae.subplots_adjust(left=0.2)
#         # cbar_mae_ax = fig_mae.add_axes([0.07, 0.16, 0.02, 0.62])
#         # fig_mae.colorbar(p_mae, cax=cbar_mae_ax)
#         # cbar_mae_ax.text(-0.95,4.75,'MAE (days)',fontsize=14, rotation = 90)


#     return MAE_arr, RMSE_arr#, Rsqr_arr, Rsqradj_arr, Acc_arr
   
    
    
    
    
    
    
    
    

# #-----------------
# freezeup_opt = 1
# start_doy_arr =[307,        314,         321,         328,         335        ]
# istart_label = ['Nov. 3rd', 'Nov. 10th', 'Nov. 17th', 'Nov. 24th', 'Dec. 1st' ] 
# month_istart = [11,11,11,11,12]
# day_istart   = [ 3,10,17,24, 1]
# date_ref = dt.date(1900,1,1)
# years_eval = np.arange(1992,2020)

# plot_samples = False
# # plot_samples = True

# #-----------------

# if valid_scheme == 'standard':
#     # y_true = y_test_all
#     # y_pred = y_pred_test_all
#     # y_clim = y_clim_test_all
#     # time_y = target_time_test_all
#     # model_name = 'Encoder-Decoder LSTM - TEST'

#     # y_true = y_train_all
#     # y_pred = y_pred_train_all
#     # y_clim = y_clim_train_all
#     # time_y = target_time_train_all
#     # model_name = 'Encoder-Decoder LSTM - TRAINING'

#     y_true = y_valid_all
#     y_pred = y_pred_valid_all
#     y_clim = y_clim_valid_all
#     time_y = target_time_valid_all
#     model_name = 'Encoder-Decoder LSTM - VALID'
    
    
#     [freezeup_dates_sample,freezeup_dates_sample_doy,
#       freezeup_dates_target,freezeup_dates_target_doy,
#       freezeup_dates_clim_target,freezeup_dates_clim_target_doy,
#       mean_clim_FUD]  = detect_FUD_from_Tw_samples(y_true,y_pred,y_clim,time_y,years_eval,freezeup_opt,start_doy_arr,istart_label,day_istart,plot_samples)
   
#     plot_FUD_ts = True
#     plot_metrics = False
#     MAE_arr, RMSE_arr = evaluate_FUD_forecasts(freezeup_dates_sample,freezeup_dates_sample_doy,freezeup_dates_target,freezeup_dates_target_doy,mean_clim_FUD,years_eval,start_doy_arr,istart_label,plot_FUD_ts,plot_metrics,verbose=True)
#     plot_FUD_ts = False
#     plot_metrics = False
#     MAE_arr_clim, RMSE_arr_clim = evaluate_FUD_forecasts(freezeup_dates_clim_target,freezeup_dates_clim_target_doy,freezeup_dates_target,freezeup_dates_target_doy,mean_clim_FUD,years_eval,start_doy_arr,istart_label,plot_FUD_ts,plot_metrics,verbose=True)
    
#     istart_plot = [0,1,2,3,4]    
#     fig_mae_ts,ax_mae_ts = plt.subplots(nrows = 1, ncols = 1,figsize=(4,3))
#     ax_mae_ts.plot(np.arange(len(istart_plot)),MAE_arr,'o-',label='Model')
#     ax_mae_ts.plot(np.arange(len(istart_plot)),MAE_arr_clim,'x-',label='Climatology')
#     ax_mae_ts.set_ylabel('MAE (days)')
#     ax_mae_ts.set_xlabel('Forecast start date')
#     ax_mae_ts.set_xticks(np.arange(len(istart_plot)))
#     ax_mae_ts.set_xticklabels([istart_label[k] for k in range(len(istart_plot))])
#     ax_mae_ts.legend()
    
#     fig_mae_ss,ax_mae_ss = plt.subplots(nrows = 1, ncols = 1,figsize=(4,3))
#     ax_mae_ss.plot(np.arange(len(istart_plot)),1-(MAE_arr/MAE_arr_clim),'o-')
#     ax_mae_ss.set_ylabel('Skill Score (MAE)')
#     ax_mae_ss.set_xlabel('Forecast start date')
#     ax_mae_ss.set_xticks(np.arange(len(istart_plot)))
#     ax_mae_ss.set_xticklabels([istart_label[k] for k in range(len(istart_plot))])

    

# if valid_scheme == 'LOOk':
    
#     # # TEST SET
#     # for iyr in range(len(test_years)):
#     #     y_pred_tmp = y_pred_test_all[iyr,:,:]
#     #     y_true_yr = y_test_all[iyr,:,:]
#     #     y_clim_yr = y_clim_test_all[iyr,:,:]
#     #     time_y_yr = target_time_test_all[iyr,:,:]
#     #     model_name = 'Encoder-Decoder LSTM - TEST'
        
#     #     y_pred_yr = y_pred_tmp[np.where(np.all(~np.isnan(y_pred_tmp),axis=1))[0]]
#     #     y_true_yr = y_true_yr[np.where(np.all(~np.isnan(y_pred_tmp),axis=1))[0]]
#     #     y_clim_yr = y_clim_yr[np.where(np.all(~np.isnan(y_pred_tmp),axis=1))[0]]
#     #     time_y_yr = time_y_yr[np.where(np.all(~np.isnan(y_pred_tmp),axis=1))[0]]
    
#     #     [freezeup_dates_sample_yr,freezeup_dates_sample_doy_yr,
#     #      freezeup_dates_target_yr,freezeup_dates_target_doy_yr,
#     #      freezeup_dates_clim_target_yr,freezeup_dates_clim_target_doy_yr,
#     #      mean_clim_FUD_yr]  = detect_FUD_from_Tw_samples(y_true_yr,y_pred_yr,y_clim_yr,time_y_yr,years_eval,freezeup_opt,start_doy_arr,istart_label,day_istart,plot_samples)
    
#     #     if iyr == 0:
#     #         [freezeup_dates_sample,freezeup_dates_sample_doy,
#     #          freezeup_dates_target,freezeup_dates_target_doy,
#     #          freezeup_dates_clim_target,freezeup_dates_clim_target_doy,
#     #          mean_clim_FUD]  = [freezeup_dates_sample_yr,freezeup_dates_sample_doy_yr,
#     #                               freezeup_dates_target_yr,freezeup_dates_target_doy_yr,
#     #                               freezeup_dates_clim_target_yr,freezeup_dates_clim_target_doy_yr,
#     #                               mean_clim_FUD_yr]
#     #     else:
#     #         freezeup_dates_sample = np.concatenate((freezeup_dates_sample,freezeup_dates_sample_yr),axis=1)
#     #         freezeup_dates_sample_doy = np.concatenate((freezeup_dates_sample_doy,freezeup_dates_sample_doy_yr),axis=1)
#     #         freezeup_dates_target = np.concatenate((freezeup_dates_target,freezeup_dates_target_yr),axis=1)
#     #         freezeup_dates_target_doy = np.concatenate((freezeup_dates_target_doy,freezeup_dates_target_doy_yr),axis=1)
#     #         freezeup_dates_clim_target = np.concatenate((freezeup_dates_clim_target,freezeup_dates_clim_target_yr),axis=1)
#     #         freezeup_dates_clim_target_doy = np.concatenate((freezeup_dates_clim_target_doy,freezeup_dates_clim_target_doy_yr),axis=1)
#     #         mean_clim_FUD = np.nanmean((mean_clim_FUD,mean_clim_FUD_yr))
             
            
#     # if freezeup_dates_sample.shape[1] > 0:
#     #     plot_FUD_ts = True
#     #     plot_metrics = True
#     #     MAE_arr, RMSE_arr = evaluate_FUD_forecasts(freezeup_dates_sample,freezeup_dates_sample_doy,freezeup_dates_target,freezeup_dates_target_doy,mean_clim_FUD,years_eval,start_doy_arr,istart_label,plot_FUD_ts,plot_metrics,verbose=True)
#     #     plot_FUD_ts = False
#     #     plot_metrics = False
#     #     MAE_arr_clim, RMSE_arr_clim = evaluate_FUD_forecasts(freezeup_dates_clim_target,freezeup_dates_clim_target_doy,freezeup_dates_target,freezeup_dates_target_doy,mean_clim_FUD,years_eval,start_doy_arr,istart_label,plot_FUD_ts,plot_metrics,verbose=True)

#     # istart_plot = [0,1,2,3,4]    
#     # fig_mae_ts,ax_mae_ts = plt.subplots(nrows = 1, ncols = 1,figsize=(4,3))
#     # ax_mae_ts.plot(np.arange(len(istart_plot)),MAE_arr,'o-',label='Model')
#     # ax_mae_ts.plot(np.arange(len(istart_plot)),MAE_arr_clim,'x-',label='Climatology')
#     # ax_mae_ts.set_ylabel('MAE (days)')
#     # ax_mae_ts.set_xlabel('Forecast start date')
#     # ax_mae_ts.set_xticks(np.arange(len(istart_plot)))
#     # ax_mae_ts.set_xticklabels([istart_label[k] for k in range(len(istart_plot))])
#     # ax_mae_ts.legend()
    
#     # fig_mae_ss,ax_mae_ss = plt.subplots(nrows = 1, ncols = 1,figsize=(4,3))
#     # ax_mae_ss.plot(np.arange(len(istart_plot)),1-(MAE_arr/MAE_arr_clim),'o-')
#     # ax_mae_ss.set_ylabel('Skill Score (MAE)')
#     # ax_mae_ss.set_xlabel('Forecast start date')
#     # ax_mae_ss.set_xticks(np.arange(len(istart_plot)))
#     # ax_mae_ss.set_xticklabels([istart_label[k] for k in range(len(istart_plot))])



#     # VALIDATION SET
#     SS_MAE_arr_valid = np.zeros((len(start_doy_arr),len(test_years),nfolds))*np.nan
#     MAE_arr_valid = np.zeros((len(start_doy_arr),len(test_years),nfolds))*np.nan
#     RMSE_arr_valid = np.zeros((len(start_doy_arr),len(test_years),nfolds))*np.nan
#     MAE_arr_clim_valid = np.zeros((len(start_doy_arr),len(test_years),nfolds))*np.nan
#     RMSE_arr_clim_valid = np.zeros((len(start_doy_arr),len(test_years),nfolds))*np.nan
#     for iyr in range(len(test_years)):
#         for ifold in range(nfolds):
#             y_pred_tmp = y_pred_valid_all[iyr,ifold,:,:]
#             y_true_fold = y_valid_all[iyr,ifold,:,:]
#             y_clim_fold = y_clim_valid_all[iyr,ifold,:,:]
#             time_y_fold = target_time_valid_all[iyr,ifold,:,:]
#             model_name = 'Encoder-Decoder LSTM - VALID'
            
#             y_pred_fold = y_pred_tmp[np.where(np.all(~np.isnan(y_pred_tmp),axis=1))[0]]
#             y_true_fold = y_true_fold[np.where(np.all(~np.isnan(y_pred_tmp),axis=1))[0]]
#             y_clim_fold = y_clim_fold[np.where(np.all(~np.isnan(y_pred_tmp),axis=1))[0]]
#             time_y_fold = time_y_fold[np.where(np.all(~np.isnan(y_pred_tmp),axis=1))[0]]
        
#             [freezeup_dates_sample_fold,freezeup_dates_sample_doy_fold,
#              freezeup_dates_target_fold,freezeup_dates_target_doy_fold,
#              freezeup_dates_clim_target_fold,freezeup_dates_clim_target_doy_fold,
#              mean_clim_FUD]  = detect_FUD_from_Tw_samples(y_true_fold,y_pred_fold,y_clim_fold,time_y_fold,years_eval,freezeup_opt,start_doy_arr,istart_label,day_istart,plot_samples)
            
#             if freezeup_dates_sample_fold.shape[1] > 0:
#                 plot_FUD_ts = False
#                 plot_metrics = False
#                 MAE_arr_valid[:,iyr,ifold], RMSE_arr_valid[:,iyr,ifold] = evaluate_FUD_forecasts(freezeup_dates_sample_fold,freezeup_dates_sample_doy_fold,freezeup_dates_target_fold,freezeup_dates_target_doy_fold,mean_clim_FUD,years_eval,start_doy_arr,istart_label,plot_FUD_ts,plot_metrics,verbose=True)
#                 plot_FUD_ts = False
#                 plot_metrics = False
#                 MAE_arr_clim_valid[:,iyr,ifold], RMSE_arr_clim_valid[:,iyr,ifold] = evaluate_FUD_forecasts(freezeup_dates_clim_target_fold,freezeup_dates_clim_target_doy_fold,freezeup_dates_target_fold,freezeup_dates_target_doy_fold,mean_clim_FUD,years_eval,start_doy_arr,istart_label,plot_FUD_ts,plot_metrics,verbose=True)
#                 SS_MAE_arr_valid[:,iyr,ifold] = 1-(MAE_arr_valid[:,iyr,ifold]/MAE_arr_clim_valid[:,iyr,ifold])
                
                
     
#     istart_plot = [0,1,2,3,4]    
#     fig_mae_CV,ax_mae_CV = plt.subplots(nrows = 1, ncols = len(istart_plot),figsize=(12,3),sharex=True,sharey=True)
#     fig_mae_SS,ax_mae_SS = plt.subplots(nrows = 1, ncols = len(istart_plot),figsize=(12,3),sharex=True,sharey=True)
#     letter = ['a)','b)','c)','d)','e)','f)']
#     for i,istart in enumerate(istart_plot):
#         for iyr in range(len(test_years)):
#             ax_mae_CV[i].plot(test_years[iyr],np.nanmean(MAE_arr_valid[istart,iyr,:]),'x',color=plt.get_cmap('tab20')(1))
#             ax_mae_CV[i].plot([test_years[iyr],test_years[iyr]],[np.nanmin(MAE_arr_valid[istart,iyr,:]),np.nanmax(MAE_arr_valid[istart,iyr,:])],'-',linewidth=0.5,color=plt.get_cmap('tab20')(1))
#         ax_mae_CV[i].plot(test_years,np.ones(len(test_years))*np.nanmean(MAE_arr_valid[istart,:,:],axis=(0,1)),'-',color=plt.get_cmap('tab20')(0))
#         # !!!! ADD OPTION HERE TO ADD TEST RESULTS AS WELL FOR EACH START DATE.
#         ax_mae_CV[i].set_xlabel('Years')
#         ax_mae_CV[i].set_xticks(test_years)
#         ax_mae_CV[i].set_xticklabels(['' for k in range(len(test_years))])
#         # ax_mae_CV[i].set_xticklabels([str(test_years[k]) for k in range(len(test_years))])
#         # ax_mae_CV[i].legend()
#         ax_mae_CV[i].set_title(letter[i]+' '+istart_label[istart])
        
        
#         for iyr in range(len(test_years)):
#             ax_mae_SS[i].plot(test_years[iyr],np.nanmean(SS_MAE_arr_valid[istart,iyr,:]),'x',color=plt.get_cmap('tab20')(1))
#             ax_mae_SS[i].plot([test_years[iyr],test_years[iyr]],[np.nanmin(SS_MAE_arr_valid[istart,iyr,:]),np.nanmax(SS_MAE_arr_valid[istart,iyr,:])],'-',linewidth=0.5,color=plt.get_cmap('tab20')(1))
#         ax_mae_SS[i].plot(test_years,np.ones(len(test_years))*np.nanmean(SS_MAE_arr_valid[istart,:,:],axis=(0,1)),'-',color=plt.get_cmap('tab20')(0))
#         # !!!! ADD OPTION HERE TO ADD TEST RESULTS AS WELL FOR EACH START DATE.
#         ax_mae_SS[i].set_xlabel('Years')
#         ax_mae_SS[i].set_xticks(test_years)
#         ax_mae_SS[i].set_xticklabels(['' for k in range(len(test_years))])
#         # ax_mae_SS[i].set_xticklabels([str(test_years[k]) for k in range(len(test_years))])
#         # ax_mae_SS[i].legend()
#         ax_mae_SS[i].set_title(letter[i]+' '+istart_label[istart])
        
    
#     fig_mae_CV.suptitle('VALID')
#     fig_mae_CV.subplots_adjust(top=0.8,bottom=0.15,left=0.05,right=0.95)
#     ax_mae_CV[0].set_ylabel('MAE (days)')      
    
#     fig_mae_SS.suptitle('VALID')
#     fig_mae_SS.subplots_adjust(top=0.8,bottom=0.15,left=0.05,right=0.95)
#     ax_mae_SS[0].set_ylabel(r'Skill Score (1- $\frac{MAE_{model}}{MAE_{clim}}$)')







#%% BELOW IS FOR WORKING ON TRANSFERRING FUD DETECTION TO TF.TENSORS

# # y_pred = np.expand_dims(y_pred_test[200:200+512,:],axis=-1)
# # y_true = np.expand_dims(y_test[200:200+512,:],axis=-1)
# y_pred = np.expand_dims(y_pred_train_all[1200:1200+512,:],axis=-1)
# y_true = np.expand_dims(y_train_all[1200:1200+512,:],axis=-1)

# def_opt = 1
# Twater_in = y_true
# time = np.squeeze(target_time_test_all[200+365,:])
# thresh_T=0.75
# ndays=1
# date_ref=dt.date(1900,1,1)

# mask_threshold = tf.math.less_equal(y_true, thresh_T)
# mask_threshold_pred = tf.math.less_equal(y_pred, thresh_T)

# n_same = tf.equal(mask_threshold,mask_threshold_pred)

# n_thresh = tf.reduce_sum(tf.cast(mask_threshold,'int32'),axis=1)
# n_thresh_pred = tf.reduce_sum(tf.cast(mask_threshold_pred,'int32'),axis=1)
#%%
# tf.where(tf.equal(mask_threshold[0],mask_threshold_pred[0]),
#           y_true[0],tf.constant(100,shape=tf.shape(y_true[0]))
#           )

# tf.equal(mask_threshold[0],mask_threshold_pred[0])

# metrics.confusion_matrix(y_true[0],y_pred[0])
# thresh_T=0.75
# mask_threshold = tf.cast(tf.math.less_equal(y_true, thresh_T), 'float')
# mask_threshold_pred = tf.cast(tf.math.less_equal(y_pred, thresh_T), 'float')



# tp = tf.reduce_sum(tf.cast(tf.math.multiply(mask_threshold,mask_threshold_pred), 'float'), axis=1)
# tn = tf.reduce_sum(tf.cast(tf.math.multiply(1-mask_threshold,1-mask_threshold_pred), 'float'), axis=1)
# fp = tf.reduce_sum(tf.cast(tf.math.multiply(1-mask_threshold,mask_threshold_pred), 'float'), axis=1)
# fn = tf.reduce_sum(tf.cast(tf.math.multiply(mask_threshold,1-mask_threshold_pred), 'float'), axis=1)

# precision = tp / (tp + fp + K.epsilon())
# recall = tp / (tp + fn + K.epsilon())

# f1 = 2*precision*recall / (precision+recall+K.epsilon())
# f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)
#%%
# [batchsize,pred_len,1]
# y_pred = np.expand_dims(y_pred_train_all[0,1200:1200+512,:],axis=-1)
# y_true = np.expand_dims(y_train_all[0,1200:1200+512,:],axis=-1)

# mse = tf.keras.losses.MeanSquaredError()
# dy_true = y_true[:,1:,:]-y_true[:,:-1,:]
# dy_pred = y_pred[:,1:,:]-y_pred[:,:-1,:]

# # # tmp = tf.concat(dy_true,tf.constant(1.0,shape=[tf.shape(y_true)[0],1,1]))

# # l = 0.8*mse(y_true,y_pred)+0.2*mse(dy_true,dy_pred)

# # wtmp = (tf.cast(tf.math.less_equal(y_true, 0.75),'float'))

# w = (tf.cast(tf.reduce_any(tf.math.less_equal(y_true, 3.0),axis=2),'float'))
  



#%%
# def record_event_tf(istart,time,year,Twater_in,freezeup_date,mask_freezeup,date_ref):
#     # Temperature has been lower than thresh_T
#     # for more than (or equal to) ndays.
#     # Define freezeup date as first date of group

#     date_start = date_ref+dt.timedelta(days=int(time[istart]))
#     doy_start = (date_start - dt.date(int(date_start.year),1,1)).days+1

#     if ((date_start.year > 1992) | ((date_start.year == 1992) & (date_start.month > 10)) ):
#         if ( (date_start.year == year) & (doy_start > 319) ) | ((date_start.year == year+1) & (doy_start < 46)):
#                 freezeup_date[0] = date_start.year
#                 freezeup_date[1] = date_start.month
#                 freezeup_date[2] = date_start.day
#                 freezeup_Tw = Twater_in[istart]
#                 mask_freezeup[istart] = True
#         else:
#             freezeup_date[0] = np.nan
#             freezeup_date[1] = np.nan
#             freezeup_date[2] = np.nan
#             freezeup_Tw = np.nan
#             mask_freezeup[istart] = False
#     else:
#     # I think this condition exists because the Tw time series
#     # starts in January 1992, so it is already frozen, but we
#     # do not want to detect this as freezeup for 1992, so we
#     # have to wait until at least October 1992 before recording
#     # any FUD events.
#         freezeup_date[0] = np.nan
#         freezeup_date[1] = np.nan
#         freezeup_date[2] = np.nan
#         freezeup_Tw = np.nan
#         mask_freezeup[istart] = False

#     return freezeup_date, freezeup_Tw, mask_freezeup


# date_start = dt.timedelta(days=int(time[0])) + date_ref
# if date_start.month < 3:
#     year = date_start.year-1
# else:
#     year = date_start.year


# mask_threshold = tf.math.less_equal(Twater_in, thresh_T)
# mask_freezeup = tf.constant(False, dtype=bool,shape=tf.shape(mask_threshold))

# # tf.where(mask_threshold)

# #%%

# freezeup_date=np.zeros((3))*np.nan
# isample = 0
# # Loop on sample time steps
# for im in range(mask_freezeup.shape[1]):

#     if (im == 0): # First time step cannot be detected as freeze-up
#         sum_m = 0
#         istart = -1 # This ensures that a freeze-up is not detected if the time series started already below the freezing temp.
#     else:
#         if (tf.equal(tf.reduce_sum(tf.cast(mask_freezeup,'int32'), axis=1)[isample],0)):
#         # if (np.sum(mask_freezeup) == 0): # Only continue while no prior freeze-up was detected for the sequence
#             if (~mask_threshold[im-1]):
#                 sum_m = 0
#                 if ~mask_threshold[im]:
#                     sum_m = 0
#                 else:
#                     # start new group
#                     sum_m +=1
#                     istart = im
#                     # Below will only occur if ndays is set to 1, e.g. first day of freezing temp.
#                     if (sum_m >= ndays):
#                         freezeup_date,freezeup_Tw,mask_freezeup = record_event_tf(istart,time,year,Twater_in,freezeup_date,mask_freezeup,date_ref)
#             else:
#                 if (mask_threshold[im]) & (istart > 0):
#                     sum_m += 1
#                     if (sum_m >= ndays):
#                         freezeup_date,freezeup_Tw,mask_freezeup = record_event_tf(istart,time,year,Twater_in,freezeup_date,mask_freezeup,date_ref)










#%%
# # BELOW IS JUST A SPACE TO TEST CUSTOM LOSS IMPLEMENTATION
# mse = tf.keras.losses.MeanSquaredError()
# # mse2 = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)

# # [batchsize,pred_len,1]
# y_pred = np.expand_dims(y_pred_test[0:512,:],axis=-1)
# y_true = np.expand_dims(y_test[0:512,:],axis=-1)

# # penalize_FN = True
# penalize_FN = False

# # use_exp_decay_loss = True
# use_exp_decay_loss = False

# if penalize_FN:
#     # Check for each sample in targets and predictions if Tw reaches below the freeze-up threshold
#     obs_FU_batch = tf.cast(tf.reduce_any(tf.math.less(y_true,0.75),axis=1),y_true.dtype)
#     pred_FU_batch = tf.cast(tf.reduce_any(tf.math.less(y_pred,0.75),axis=1),y_true.dtype)

#     # One weight per sample, depending if sample is a false positive,
#     # false negative, true positive or true negative for detected freeze-up
#     w_batch = tf.constant(1,y_true.dtype,shape=obs_FU_batch.shape) + 2*tf.math.squared_difference(obs_FU_batch,pred_FU_batch) + tf.math.subtract(obs_FU_batch,pred_FU_batch)
#     w_batch = tf.squeeze(w_batch)

#     if use_exp_decay_loss:
#         # Forcing an exponential weight decay per time step in
#         # forecast horizon, i.e. put more weight on the error
#         # of the first forecast steps.
#         obs_FU_all_steps = tf.cast(tf.reduce_any(tf.math.less(y_true,0.75),axis=-1),y_true.dtype)
#         pred_FU_all_steps = tf.cast(tf.reduce_any(tf.math.less(y_pred,0.75),axis=-1),y_true.dtype)

#         wtmp = tf.cast(tf.fill(tf.shape(obs_FU_all_steps),1.0),y_true.dtype)
#         exp_tmp = 1/tf.exp(tf.range(1,pred_len+1,dtype=y_true.dtype))
#         exp_w = tf.multiply(wtmp,exp_tmp)

#         w = tf.multiply(exp_w,tf.expand_dims(w_batch,axis=-1))
#     else:
#         w = w_batch

# else:
#     if use_exp_decay_loss:
#         # Forcing an exponential weight decay per time step in
#         # forecast horizon, i.e. put more weight on the error
#         # of the first forecast steps.
#         obs_FU_all_steps = tf.cast(tf.reduce_any(tf.math.less(y_true,0.75),axis=-1),y_true.dtype)
#         pred_FU_all_steps = tf.cast(tf.reduce_any(tf.math.less(y_pred,0.75),axis=-1),y_true.dtype)

#         wtmp = tf.cast(tf.fill(tf.shape(tf.squeeze(y_true)),1.0),y_true.dtype)
#         exp_tmp = 1/tf.exp(tf.range(1,pred_len+1,dtype=y_true.dtype))
#         exp_w = tf.multiply(wtmp,exp_tmp)

#         w = exp_w
#     else:
#         w = 1

# out = mse(y_true,y_pred,sample_weight=w)


# exp_tmp = 1/tf.exp(tf.range(1,pred_len+1,dtype=y_true.dtype))
# tmp = tf.reduce_sum(tf.math.log(tf.reduce_mean(tf.math.square(y_true-y_pred),axis=0)),axis=0)

# tmp = tf.math.log(tf.reduce_mean(tf.math.square(y_true-y_pred),axis=0))



# #%%

# # Epoch 1/50
# # 14/14 - 589s - loss: 5.7569 - mae: 0.4655 - val_loss: 1.3554 - val_mae: 0.3147 - lr: 0.0160 - 589s/epoch - 42s/step


# # TRAINING ---
# # Rsqr = 0.9857
# # MAE = 0.6915
# # RMSE = 0.9985

# # VALIDATION ---
# # Rsqr = 0.9852
# # MAE = 0.7562
# # RMSE = 1.0602

# # TEST ---
# # Rsqr = 0.9824
# # MAE = 0.8597
# # RMSE = 1.1815